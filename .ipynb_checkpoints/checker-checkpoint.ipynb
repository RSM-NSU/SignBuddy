{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43448ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Open the default camera (0 usuammlly refers to the first camera)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "    \n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Camera', frame)\n",
    "    \n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7192a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'Nothing', 'O', 'P', 'Q', 'R', 'S', 'Space', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae3c21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "import tflite_runtime.interpreter as tflite\n",
    "\n",
    "class_names = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'Nothing', 'O', 'P', 'Q', 'R', 'S', 'Space', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "TARGET_SIZE = 224\n",
    "MODEL_PATH = \"/home/rafayahmadraza/SignBuddy/Models/asl_model_1.tflite\"   # your trained model file\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Resize the cropped hand to 224Ã—224 while keeping aspect\n",
    "# -------------------------------------------------------\n",
    "def resize_with_padding(img, target=TARGET_SIZE):\n",
    "    h, w = img.shape[:2]\n",
    "    aspect = w / h\n",
    "\n",
    "    if aspect > 1:  # wide\n",
    "        new_w = target\n",
    "        new_h = int(target / aspect)\n",
    "    else:  # tall\n",
    "        new_h = target\n",
    "        new_w = int(target * aspect)\n",
    "\n",
    "    resized = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Add black padding\n",
    "    top = (target - new_h) // 2\n",
    "    bottom = target - new_h - top\n",
    "    left = (target - new_w) // 2\n",
    "    right = target - new_w - left\n",
    "\n",
    "    padded = cv2.copyMakeBorder(\n",
    "        resized, top, bottom, left, right,\n",
    "        cv2.BORDER_CONSTANT, value=(0, 0, 0)\n",
    "    )\n",
    "    return padded\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Load TFLite Model\n",
    "# -------------------------------------------------------\n",
    "interpreter = tflite.Interpreter(model_path=MODEL_PATH)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"Model input:\", input_details[0][\"shape\"])\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Initialize MediaPipe Hands\n",
    "# -------------------------------------------------------\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.6,\n",
    "    min_tracking_confidence=0.6\n",
    ")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Main loop\n",
    "# -------------------------------------------------------\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(rgb)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "\n",
    "            # Extract bounding box\n",
    "            xs = [lm.x for lm in hand_landmarks.landmark]\n",
    "            ys = [lm.y for lm in hand_landmarks.landmark]\n",
    "\n",
    "            xmin = int(min(xs) * w)\n",
    "            xmax = int(max(xs) * w)\n",
    "            ymin = int(min(ys) * h)\n",
    "            ymax = int(max(ys) * h)\n",
    "\n",
    "            pad = 30\n",
    "            xmin = max(0, xmin - pad)\n",
    "            ymin = max(0, ymin - pad)\n",
    "            xmax = min(w, xmax + pad)\n",
    "            ymax = min(h, ymax + pad)\n",
    "\n",
    "            # Draw box\n",
    "            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "\n",
    "            # Crop\n",
    "            crop = frame[ymin:ymax, xmin:xmax]\n",
    "\n",
    "            if crop.size > 0:\n",
    "                crop_96 = resize_with_padding(crop)\n",
    "\n",
    "                # Show resized hand\n",
    "                cv2.imshow(\"Hand 96x96\", crop_96)\n",
    "\n",
    "                # Prepare input for model\n",
    "                inp = cv2.cvtColor(crop_96, cv2.COLOR_BGR2RGB)\n",
    "                inp = inp.astype(np.float32) / 255.0\n",
    "                inp = np.expand_dims(inp, axis=0)\n",
    "\n",
    "                # Run inference\n",
    "                interpreter.set_tensor(input_details[0][\"index\"], inp)\n",
    "                interpreter.invoke()\n",
    "                prediction = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "\n",
    "                # Get predicted class\n",
    "                cls = np.argmax(prediction)\n",
    "                conf = np.max(prediction)\n",
    "\n",
    "                # Display prediction\n",
    "                text = f\"Class: {class_names[np.argmax(prediction)]}  ({conf:.2f})\"\n",
    "                cv2.putText(frame, text, (xmin, ymin - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "                # Draw landmarks\n",
    "                mp_draw.draw_landmarks(\n",
    "                    frame, hand_landmarks, mp_hands.HAND_CONNECTIONS\n",
    "                )\n",
    "\n",
    "    cv2.imshow(\"Live\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffe922f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f05cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# SETTINGS\n",
    "# -------------------------------------------------------\n",
    "class_names = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N',\n",
    "               'Nothing', 'O', 'P', 'Q', 'R', 'S', 'Space', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "\n",
    "TARGET_SIZE = 96\n",
    "MODEL_PATH = \"/home/rafayahmadraza/SignBuddy/Models/asl_tiny_model_3.keras\"   # ðŸ‘ˆ now .h5 file\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Resize the cropped hand to 224Ã—224 while keeping aspect\n",
    "# -------------------------------------------------------\n",
    "def resize_with_padding(img, target=TARGET_SIZE):\n",
    "    h, w = img.shape[:2]\n",
    "    aspect = w / h\n",
    "\n",
    "    if aspect > 1:  # wide\n",
    "        new_w = target\n",
    "        new_h = int(target / aspect)\n",
    "    else:  # tall\n",
    "        new_h = target\n",
    "        new_w = int(target * aspect)\n",
    "\n",
    "    resized = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Add black padding\n",
    "    top = (target - new_h) // 2\n",
    "    bottom = target - new_h - top\n",
    "    left = (target - new_w) // 2\n",
    "    right = target - new_w - left\n",
    "\n",
    "    padded = cv2.copyMakeBorder(\n",
    "        resized, top, bottom, left, right,\n",
    "        cv2.BORDER_CONSTANT, value=(0, 0, 0)\n",
    "    )\n",
    "    return padded\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Load Keras Model\n",
    "# -------------------------------------------------------\n",
    "print(\"Loading Keras model...\")\n",
    "model = load_model(MODEL_PATH)\n",
    "print(\"Model loaded!\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Initialize MediaPipe Hands\n",
    "# -------------------------------------------------------\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.6,\n",
    "    min_tracking_confidence=0.6\n",
    ")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Main loop\n",
    "# -------------------------------------------------------\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(rgb)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "\n",
    "            # Extract bounding box\n",
    "            xs = [lm.x for lm in hand_landmarks.landmark]\n",
    "            ys = [lm.y for lm in hand_landmarks.landmark]\n",
    "\n",
    "            xmin = int(min(xs) * w)\n",
    "            xmax = int(max(xs) * w)\n",
    "            ymin = int(min(ys) * h)\n",
    "            ymax = int(max(ys) * h)\n",
    "\n",
    "            pad = 30\n",
    "            xmin = max(0, xmin - pad)\n",
    "            ymin = max(0, ymin - pad)\n",
    "            xmax = min(w, xmax + pad)\n",
    "            ymax = min(h, ymax + pad)\n",
    "\n",
    "            # Draw box\n",
    "            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "\n",
    "            # Crop\n",
    "            crop = frame[ymin:ymax, xmin:xmax]\n",
    "\n",
    "            if crop.size > 0:\n",
    "                crop_96 = resize_with_padding(crop)\n",
    "\n",
    "                # Show resized hand\n",
    "                cv2.imshow(\"Hand 224x224\", crop_96)\n",
    "\n",
    "                # Prepare input for model\n",
    "                inp = cv2.cvtColor(crop_96, cv2.COLOR_BGR2RGB)\n",
    "                inp = inp.astype(np.float32) / 255.0\n",
    "                inp = np.expand_dims(inp, axis=0)\n",
    "\n",
    "                # Predict\n",
    "                prediction = model.predict(inp)[0]\n",
    "\n",
    "                # Get predicted class\n",
    "                cls = np.argmax(prediction)\n",
    "                conf = prediction[cls]\n",
    "\n",
    "                # Display prediction\n",
    "                text = f\"Class: {class_names[cls]}  ({conf:.2f})\"\n",
    "                cv2.putText(frame, text, (xmin, ymin - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "                # Draw landmarks\n",
    "                mp_draw.draw_landmarks(\n",
    "                    frame, hand_landmarks, mp_hands.HAND_CONNECTIONS\n",
    "                )\n",
    "            \n",
    "\n",
    "    cv2.imshow(\"Live\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926109f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
