{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":399170,"sourceType":"datasetVersion","datasetId":177084},{"sourceId":2184214,"sourceType":"datasetVersion","datasetId":1311225},{"sourceId":6094993,"sourceType":"datasetVersion","datasetId":3400451},{"sourceId":12361288,"sourceType":"datasetVersion","datasetId":7793547},{"sourceId":13506823,"sourceType":"datasetVersion","datasetId":8575690}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install mediapipe\n!pip install --upgrade scikit-learn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport os\nfrom matplotlib import pyplot as plt\nimport time\nimport mediapipe as mp","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nmp_holistic = mp.solutions.holistic # Holistic model\nmp_drawing = mp.solutions.drawing_utils # Drawing utilities","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def mediapipe_detection(image, model):\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n    image.flags.writeable = False                  # Image is no longer writeable\n    results = model.process(image)                 # Make prediction\n    image.flags.writeable = True                   # Image is now writeable \n    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n    return image, results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def draw_styled_landmarks(image, results):\n    # Draw face connections\n    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS, \n                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n                             ) \n    # Draw pose connections\n    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n                             ) \n    # Draw left hand connections\n    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n                             ) \n    # Draw right hand connections  \n    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n                             ) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_keypoints(results):\n    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n    return np.concatenate([pose, face, lh, rh])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DATA_PATH = os.path.join('/kaggle/input/mpdata/MP_DATA')\n\nactions = np.array(['a','b','c','d','e','f','g',\n                    'h','i','j','k','l','m','n',\n                    'o','p','q','r','s','t','u',\n                    'v','w','x','y','z'])\n\n#no of videos\nno_sequnces = 30\n\n#number of frames\nsequnce_length = 30 \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DATA_PATH = os.path.join('/kaggle/input/wlpsl/WLPSL')\naction = []\nfor subdir,dirs,files in os.walk(DATA_PATH +\"/Videos\"):\n    for dirc in dirs:\n        action.append(dirc)\n\nactions = np.array(action)\n\n#no of videos\nno_sequnces = 30\n\n#number of frames\nsequnce_length = 30 \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"actions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(actions)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_map = {label:num for num,label in enumerate(actions)}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_map","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sequences,labels = [],[]\nfor action in actions:\n    for sequence in range(no_sequnces):\n        window = []\n        for frame_num in range(sequnce_length):\n            res = np.load(os.path.join(DATA_PATH,action,str(sequence),\"{}.npy\".format(frame_num)))\n            window.append(res)\n        sequences.append(window)\n        labels.append(label_map[action])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.unique(labels)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x = np.array(sequences)\ny = to_categorical(labels).astype(int)\nX_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.05)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(x.shape)   # should be (num_samples, 30, 1662) or similar\nprint(y.shape)   # should be (num_samples,)\nprint(np.unique(np.argmax(y, axis=1)))  # should show 26 unique numbers (0–25)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import joblib","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\nX_train_scaled = scaler.fit_transform(X_train_reshaped)\nX_train = X_train_scaled.reshape(X_train.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nX_test_reshaped = x_test.reshape(-1, x_test.shape[-1])\nX_test_scaled = scaler.transform(X_test_reshaped)\nx_test = X_test_scaled.reshape(x_test.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(700)\nX_train_reduced = pca.fit_transform(X_train.reshape(-1, X_train.shape[-1]))\nX_train_reduced = X_train_reduced.reshape(X_train.shape[0], X_train.shape[1], 700)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test_reduced = pca.transform(x_test.reshape(-1, x_test.shape[-1]))\nX_test_reduced = X_test_reduced.reshape(x_test.shape[0], x_test.shape[1], 700)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import joblib\njoblib.dump(scaler,\"scalerV2.pkl\")\njoblib.dump(pca,\"pcaV2_700.pkl\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport random\n\ndef augment_sequence(sequence, noise_std=0.001, dropout_prob=0.1):\n    \"\"\"\n    sequence: np.array of shape (30, 1662)\n    \"\"\"\n    seq = sequence.copy()\n\n    # 1. Add small Gaussian noise\n    seq += np.random.normal(0, noise_std, seq.shape)\n\n    # 2. Random feature scaling\n    seq *= np.random.uniform(0.95, 1.02)\n\n    # 3. Random frame dropout\n    if random.random() < dropout_prob:\n        drop_idx = np.random.choice(seq.shape[0], size=1, replace=False)\n        seq = np.delete(seq, drop_idx, axis=0)\n        # Pad back to 30 frames (duplicate last frame)\n        seq = np.pad(seq, ((0, 1), (0, 0)), mode='edge')\n\n    return seq\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_augmented = []\ny_augmented = []\n\nfor x, y in zip(X_train_reduced, y_train):\n    X_augmented.append(x)\n    y_augmented.append(y)\n\n    # Add one augmented version of each sequence\n    X_augmented.append(augment_sequence(x))\n    y_augmented.append(y)\n\nX_augmented = np.array(X_augmented)\ny_augmented = np.array(y_augmented)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_augmented.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_augmented.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM,Dense,Dropout,BatchNormalization,Bidirectional\nfrom tensorflow.keras.callbacks import TensorBoard\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.regularizers import l2\n\n\nlog_dir = os.path.join('logs')\n\ntb_callback = TensorBoard(log_dir=log_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"early_stop = EarlyStopping(\n    monitor='val_loss',\n    patience=40,              # stop if val_loss doesn't improve for 20 epochs\n    restore_best_weights=True\n)\n\ncheckpoint = ModelCheckpoint(\n    'best_model.keras',\n    monitor='val_loss',\n    save_best_only=True,\n    verbose=1\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ReduceLROnPlateau\n\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5,\n    patience=15,\n    min_lr=1e-6,\n    verbose=1\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_reduced.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Bidirectional(LSTM(64,return_sequences=True,activation=\"tanh\",input_shape=(30,700))))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\n\n\nmodel.add(Bidirectional(LSTM(64,return_sequences=False,activation=\"tanh\")))\nmodel.add(Dropout(0.2))\n\n\nmodel.add(Dense(128,activation='relu',kernel_regularizer=l2(5e-6)))\nmodel.add(Dropout(0.15))\n\nmodel.add(Dense(64,activation='relu',))\nmodel.add(Dropout(0.15))\n\nmodel.add(Dense(32,activation='relu',))\nmodel.add(Dropout(0.1))\n\nmodel.add(Dense(actions.shape[0],activation='softmax'))\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(optimizer=\"Adam\",loss=\"categorical_crossentropy\",metrics=['accuracy'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.fit(X_augmented,y_augmented,validation_split=0.2,epochs=2000,callbacks=[tb_callback,early_stop,checkpoint,reduce_lr])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import load_model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loss,acc = model.evaluate(X_test_reduced,y_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(acc)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save(\"model_12.keras\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = load_model(\"model_11.keras\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Base CNN Model","metadata":{}},{"cell_type":"code","source":"import cv2,os","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DATA_PATH = os.path.join('/kaggle/input/asl-dataset/asl_dataset/asl_dataset')\noutput_path = \"/kaggle/working/outputs\"\nFRAMES_PER_VIDEO = 30\nFRAME_SIZE = (224,224)\nOUTPUT_PATH = \"/kaggle/working/outputs\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.listdir(DATA_PATH)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2, os\n\ndef extract_frames(video_path, output_folder, frame_count=60):  \n    os.makedirs(output_folder, exist_ok=True)\n\n    cap = cv2.VideoCapture(video_path)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    if total_frames == 0:\n        print(f\"Skipping (no frames): {video_path}\")\n        return \n    \n    # FIXED evenly spaced frames\n    frame_indices = [int(i * total_frames / frame_count) for i in range(frame_count)]\n    frame_indices = sorted(set(frame_indices))\n\n    saved = 0\n    count = 0\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        if count in frame_indices:\n            frame = cv2.resize(frame, FRAME_SIZE)\n            frame_name = f\"frame_{saved}.jpg\"\n            cv2.imwrite(os.path.join(output_folder, frame_name), frame)\n            saved += 1\n        \n        count += 1\n\n        if saved >= len(frame_indices):\n            break\n\n    cap.release()\n    print(f\"Extracted {saved} frames from {video_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_all_frames():\n    for class_name in os.listdir(DATA_PATH):\n        class_path = os.path.join(DATA_PATH, class_name)\n\n        if not os.path.isdir(class_path):\n            continue\n\n        print(f\"\\nProcessing Class: {class_name}\")\n\n        output_dir = os.path.join(output_path, class_name)\n        os.makedirs(output_dir, exist_ok=True)\n\n        for video_file in os.listdir(class_path):\n            if video_file.lower().endswith((\".mp4\", \".avi\", \".mov\", \".mkv\")):\n\n                video_path = os.path.join(class_path, video_file)\n\n                video_output_folder = os.path.join(\n                    output_dir,\n                    video_file.split('.')[0]\n                )\n                os.makedirs(video_output_folder, exist_ok=True)\n\n                print(f\"Extracting ALL frames from: {video_file}\")\n                extract_frames(video_path, video_output_folder)\n\n    print(\"\\nAll Frames Extracted Successfully\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"extract_all_frames()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_frame(frame_path,dataset_dir,frame):\n    os.makedirs(dataset_dir,exist_ok=True)\n    \n    img = cv2.imread(frame_path)\n    resized_img = cv2.resize(img,(224,224),interpolation=cv2.INTER_AREA)\n    rgb_img = cv2.cvtColor(resized_img,cv2.COLOR_BGR2RGB)\n    \n    norm_img = cv2.normalize(\n    rgb_img, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n     \n    final_img = norm_img\n    bgr_final = cv2.cvtColor(final_img, cv2.COLOR_RGB2BGR)\n    folder_path = os.path.join(dataset_dir)\n    cv2.imwrite(os.path.join(folder_path,frame),bgr_final)\n\n    \n","metadata":{"trusted":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preprocess_output_path = \"/kaggle/working/new_dataset_asl\"\nalphabetinput = '/kaggle/input/dataset-pakistani-sign-language/dataset'\nos.makedirs(preprocess_output_path,exist_ok=True)\n\ndef process_all_frames():\n    for class_name in os.listdir(DATA_PATH):\n        class_path = os.path.join(DATA_PATH,class_name)\n        \n        if not os.path.isdir(class_path):\n            continue\n    \n        for video_num in os.listdir(class_path):\n            frame_dir = os.path.join(class_path,video_num)\n    \n            for frame in os.listdir(frame_dir):\n                \n                frame_path = os.path.join(frame_dir,frame)\n                \n                dataset_path = os.path.join(preprocess_output_path,class_name)\n                dataset_dir = os.path.join(dataset_path,video_num)\n                print(dataset_dir)\n                print(f\"Preprocessing {frame} of {class_name}\")\n                preprocess_frame(frame_path,dataset_dir,frame)\n                \n                \n    \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#for alphabets\ndef process_all_frames():\n    for class_name in os.listdir(DATA_PATH):\n        class_path = os.path.join(DATA_PATH, class_name)\n\n        if not os.path.isdir(class_path):\n            continue\n\n        # output folder for this class\n        output_class_dir = os.path.join(preprocess_output_path, class_name)\n        os.makedirs(output_class_dir, exist_ok=True)\n\n        # iterate all images\n        for frame in os.listdir(class_path):\n            frame_path = os.path.join(class_path, frame)\n\n            if not frame.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n                continue  # skip non-images\n            \n            print(f\"Preprocessing {frame} of {class_name}\")\n            preprocess_frame(frame_path, output_class_dir, frame)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"process_all_frames()\n","metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T05:46:20.212576Z","iopub.execute_input":"2025-12-01T05:46:20.212884Z","iopub.status.idle":"2025-12-01T05:46:20.218378Z","shell.execute_reply.started":"2025-12-01T05:46:20.212860Z","shell.execute_reply":"2025-12-01T05:46:20.216798Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"DATASET_PATH = \"/kaggle/working/new_dataset_asl\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.listdir(DATASET_PATH)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 32\nimg_size= (224,224)\n\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    DATASET_PATH,\n    validation_split=0.2,\n    subset=\"training\",\n    seed=42,\n    image_size=img_size,\n    batch_size=batch_size\n)\n\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    DATASET_PATH,\n    validation_split=0.2,\n    subset=\"validation\",\n    seed=42,\n    image_size=img_size,\n    batch_size=batch_size\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BIGGER_ASL_DATASET = \"/kaggle/input/american-sign-language/ASL_Dataset\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = os.path.join(BIGGER_ASL_DATASET,\"Train\")\ntest = os.path.join(BIGGER_ASL_DATASET,\"Test\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nbatch_size = 32\nimg_size= (224,224)\n\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    train,\n    validation_split=0.2,\n    subset=\"training\",\n    seed=42,\n    image_size=img_size,\n    batch_size=batch_size\n)\n\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    train,\n    validation_split=0.2,\n    subset=\"validation\",\n    seed=42,\n    image_size=img_size,\n    batch_size=batch_size\n)\n\ntest_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    test,\n    seed=42,\n    image_size=img_size,\n    batch_size=batch_size\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_names = train_ds.class_names\nprint(class_names)\n\nfor images,labels in train_ds.take(1):\n    print(images.shape)\n    print(labels.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(class_names)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"AUTOTUNE = tf.data.AUTOTUNE\n\nnormalization_layer = tf.keras.layers.Rescaling(1./255)\n\ntrain_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\nval_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\ntest_ds = test_ds.map(lambda x, y: (normalization_layer(x), y))\n\n\n\ntrain_ds = train_ds.prefetch(AUTOTUNE)\nval_ds = val_ds.prefetch(AUTOTUNE)\ntest_ds = test_ds.prefetch(AUTOTUNE)\n\nfor images, labels in train_ds.take(1):\n    print(f\"After normalization - Min: {images.numpy().min():.3f}, Max: {images.numpy().max():.3f}, Mean: {images.numpy().mean():.3f}\")\n    # Should now print: Min: 0.0, Max: 1.0 (approximately)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout,Rescaling\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_classes=len(class_names)\n\nmodel = Sequential([\n    \n    Conv2D(32,(3,3),activation='relu',input_shape=(224,224,3)),\n    MaxPooling2D(2,2),\n    Dropout(0.2),\n    \n    Conv2D(64,(3,3),activation='relu'),\n    MaxPooling2D(2,2),\n    Dropout(0.3),\n\n    Conv2D(128,(3,3),activation='relu'),\n    MaxPooling2D(2,2),\n    Dropout(0.3),\n\n    Flatten(),\n\n    Dense(128,activation='relu'),\n    Dropout(0.5),\n    Dense(64,activation='relu'),\n    Dropout(0.5),\n    Dense(num_classes,activation='softmax')\n    \n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nearly_stopping = EarlyStopping(\n    monitor='val_loss',           # Monitor validation loss\n    patience=3,                    # Stop if no improvement for 5 epochs\n    restore_best_weights=True,     # Restore weights from best epoch\n    verbose=1                      # Print messages\n)\n\n# Optional: Save the best model during training\nmodel_checkpoint = ModelCheckpoint(\n    'best_model.keras',           # or 'best_model.h5'\n    monitor='val_loss',\n    save_best_only=True,\n    verbose=1\n)\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\nmodel.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=50,\n    batch_size=32,\n    callbacks=[early_stopping, model_checkpoint]\n\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 2, figsize=(14, 5))\n\n# Accuracy\nax[0].plot(history.history['accuracy'], label='Train Acc')\nax[0].plot(history.history['val_accuracy'], label='Val Acc')\nax[0].set_title(\"Accuracy\")\nax[0].set_xlabel(\"Epoch\")\nax[0].set_ylabel(\"Accuracy\")\nax[0].legend()\nax[0].grid(True)\n\n# Loss\nax[1].plot(history.history['loss'], label='Train Loss')\nax[1].plot(history.history['val_loss'], label='Val Loss')\nax[1].set_title(\"Loss\")\nax[1].set_xlabel(\"Epoch\")\nax[1].set_ylabel(\"Loss\")\nax[1].legend()\nax[1].grid(True)\n\nplt.show()\nplt.savefig(\"signbuddy_cnn.png\",dpi=300,bbox_inches='tight')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import classification_report, confusion_matrix\n\ny_true = []\ny_pred = []\n\nfor batch_x, batch_y in test_ds:\n    # predict on this batch\n    batch_preds = model.predict(batch_x, verbose=0)\n    batch_pred_classes = np.argmax(batch_preds, axis=1)\n\n    # store\n    y_true.extend(batch_y.numpy())\n    y_pred.extend(batch_pred_classes)\n\ny_true = np.array(y_true)\ny_pred = np.array(y_pred)\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_true, y_pred, target_names=class_names))\n\nprint(\"\\nConfusion Matrix:\")\ncm = confusion_matrix(y_true, y_pred)\nprint(cm)\n\ntest_loss, test_accuracy = model.evaluate(test_ds)\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Test Loss: {test_loss:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Get one batch for visualization\nfor images, labels in test_ds.take(1):\n    preds = model.predict(images)\n    pred_classes = np.argmax(preds, axis=1)\n    \n    plt.figure(figsize=(15, 10))\n    for i in range(min(9, len(images))):\n        plt.subplot(3, 3, i+1)\n        plt.imshow(images[i].numpy())\n        plt.title(f\"True: {class_names[labels[i]]}\\nPred: {class_names[pred_classes[i]]}\")\n        plt.axis('off')\n    plt.tight_layout()\n\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save(\"signbuddy_cnn_asl_model_4.h5\")\nmodel.save(\"signbuddy_cnn_asl_model_4.keras\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import load_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T05:45:21.690043Z","iopub.execute_input":"2025-12-01T05:45:21.690648Z","iopub.status.idle":"2025-12-01T05:45:40.194920Z","shell.execute_reply.started":"2025-12-01T05:45:21.690615Z","shell.execute_reply":"2025-12-01T05:45:40.193601Z"}},"outputs":[{"name":"stderr","text":"2025-12-01 05:45:23.576441: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764567923.819990      38 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764567923.889228      38 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"model = load_model('/kaggle/working/signbuddy_cnn_asl_model_4.keras')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T05:45:40.196657Z","iopub.execute_input":"2025-12-01T05:45:40.197271Z","iopub.status.idle":"2025-12-01T05:45:41.030965Z","shell.execute_reply.started":"2025-12-01T05:45:40.197234Z","shell.execute_reply":"2025-12-01T05:45:41.029793Z"}},"outputs":[{"name":"stderr","text":"2025-12-01 05:45:40.214198: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 12 variables whereas the saved optimizer has 22 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Evaluate on the SAME test set that gave you 100% accuracy\ntest_loss, test_acc = model.evaluate(test_ds)\nprint(f\"Test Accuracy: {test_acc}\")\n\n# Get predictions for the entire test set\npredictions = []\ntrue_labels = []\n\nfor images, labels in test_ds:\n    preds = model.predict(images, verbose=0)\n    predictions.extend(preds.argmax(axis=1))\n    true_labels.extend(labels.numpy())\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(true_labels, predictions, target_names=class_names))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T05:49:35.199221Z","iopub.execute_input":"2025-12-01T05:49:35.199512Z","iopub.status.idle":"2025-12-01T05:49:44.123556Z","shell.execute_reply.started":"2025-12-01T05:49:35.199491Z","shell.execute_reply":"2025-12-01T05:49:44.122174Z"}},"outputs":[{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1764568175.824789      89 service.cc:148] XLA service 0x7c89c4008870 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1764568175.825651      89 service.cc:156]   StreamExecutor device (0): Host, Default Version\nI0000 00:00:1764568176.317569      89 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 755ms/step - accuracy: 1.0000 - loss: 0.0035\nTest Accuracy: 1.0\n              precision    recall  f1-score   support\n\n           A       1.00      1.00      1.00         4\n           B       1.00      1.00      1.00         4\n           C       1.00      1.00      1.00         4\n           D       1.00      1.00      1.00         4\n           E       1.00      1.00      1.00         4\n           F       1.00      1.00      1.00         4\n           G       1.00      1.00      1.00         4\n           H       1.00      1.00      1.00         4\n           I       1.00      1.00      1.00         4\n           J       1.00      1.00      1.00         4\n           K       1.00      1.00      1.00         4\n           L       1.00      1.00      1.00         4\n           M       1.00      1.00      1.00         4\n           N       1.00      1.00      1.00         4\n     Nothing       1.00      1.00      1.00         4\n           O       1.00      1.00      1.00         4\n           P       1.00      1.00      1.00         4\n           Q       1.00      1.00      1.00         4\n           R       1.00      1.00      1.00         4\n           S       1.00      1.00      1.00         4\n       Space       1.00      1.00      1.00         4\n           T       1.00      1.00      1.00         4\n           U       1.00      1.00      1.00         4\n           V       1.00      1.00      1.00         4\n           W       1.00      1.00      1.00         4\n           X       1.00      1.00      1.00         4\n           Y       1.00      1.00      1.00         4\n           Z       1.00      1.00      1.00         4\n\n    accuracy                           1.00       112\n   macro avg       1.00      1.00      1.00       112\nweighted avg       1.00      1.00      1.00       112\n\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import tensorflow as tf\n\n\n\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\ntflite_model = converter.convert()\n\nwith open(\"asl_alphabert_model.tflite\", \"wb\") as f:\n    f.write(tflite_model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T05:50:15.163789Z","iopub.execute_input":"2025-12-01T05:50:15.164085Z","iopub.status.idle":"2025-12-01T05:50:19.307760Z","shell.execute_reply.started":"2025-12-01T05:50:15.164065Z","shell.execute_reply":"2025-12-01T05:50:19.306602Z"}},"outputs":[{"name":"stdout","text":"Saved artifact at '/tmp/tmptqv7gko8'. The following endpoints are available:\n\n* Endpoint 'serve'\n  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_layer_15')\nOutput Type:\n  TensorSpec(shape=(None, 28), dtype=tf.float32, name=None)\nCaptures:\n  136932680043984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  136932680046288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  136932680045712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  136932680044752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  136932680048400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  136932680049168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  136932680049360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  136932680050896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  136932680051088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  136932680052816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1764568217.049429      38 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\nW0000 00:00:1764568217.049472      38 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}