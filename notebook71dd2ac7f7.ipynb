{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":29550,"sourceType":"datasetVersion","datasetId":23079},{"sourceId":399170,"sourceType":"datasetVersion","datasetId":177084},{"sourceId":2184214,"sourceType":"datasetVersion","datasetId":1311225},{"sourceId":2702383,"sourceType":"datasetVersion","datasetId":1646010},{"sourceId":6094993,"sourceType":"datasetVersion","datasetId":3400451},{"sourceId":12361288,"sourceType":"datasetVersion","datasetId":7793547},{"sourceId":13506823,"sourceType":"datasetVersion","datasetId":8575690},{"sourceId":14026916,"sourceType":"datasetVersion","datasetId":8932655},{"sourceId":14152799,"sourceType":"datasetVersion","datasetId":9020402}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install mediapipe\n!pip install --upgrade scikit-learn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport os\nfrom matplotlib import pyplot as plt\nimport time\nimport mediapipe as mp","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nmp_holistic = mp.solutions.holistic # Holistic model\nmp_drawing = mp.solutions.drawing_utils # Drawing utilities","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def mediapipe_detection(image, model):\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n    image.flags.writeable = False                  # Image is no longer writeable\n    results = model.process(image)                 # Make prediction\n    image.flags.writeable = True                   # Image is now writeable \n    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n    return image, results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def draw_styled_landmarks(image, results):\n    # Draw face connections\n    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS, \n                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n                             ) \n    # Draw pose connections\n    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n                             ) \n    # Draw left hand connections\n    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n                             ) \n    # Draw right hand connections  \n    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n                             ) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_keypoints(results):\n    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n    return np.concatenate([pose, face, lh, rh])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DATA_PATH = os.path.join('/kaggle/input/mpdata/MP_DATA')\n\nactions = np.array(['a','b','c','d','e','f','g',\n                    'h','i','j','k','l','m','n',\n                    'o','p','q','r','s','t','u',\n                    'v','w','x','y','z'])\n\n#no of videos\nno_sequnces = 30\n\n#number of frames\nsequnce_length = 30 \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DATA_PATH = os.path.join('/kaggle/input/wlpsl/WLPSL')\naction = []\nfor subdir,dirs,files in os.walk(DATA_PATH +\"/Videos\"):\n    for dirc in dirs:\n        action.append(dirc)\n\nactions = np.array(action)\n\n#no of videos\nno_sequnces = 30\n\n#number of frames\nsequnce_length = 30 \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"actions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(actions)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_map = {label:num for num,label in enumerate(actions)}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_map","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sequences,labels = [],[]\nfor action in actions:\n    for sequence in range(no_sequnces):\n        window = []\n        for frame_num in range(sequnce_length):\n            res = np.load(os.path.join(DATA_PATH,action,str(sequence),\"{}.npy\".format(frame_num)))\n            window.append(res)\n        sequences.append(window)\n        labels.append(label_map[action])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.unique(labels)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x = np.array(sequences)\ny = to_categorical(labels).astype(int)\nX_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.05)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(x.shape)   # should be (num_samples, 30, 1662) or similar\nprint(y.shape)   # should be (num_samples,)\nprint(np.unique(np.argmax(y, axis=1)))  # should show 26 unique numbers (0â€“25)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import joblib","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\nX_train_scaled = scaler.fit_transform(X_train_reshaped)\nX_train = X_train_scaled.reshape(X_train.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nX_test_reshaped = x_test.reshape(-1, x_test.shape[-1])\nX_test_scaled = scaler.transform(X_test_reshaped)\nx_test = X_test_scaled.reshape(x_test.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(700)\nX_train_reduced = pca.fit_transform(X_train.reshape(-1, X_train.shape[-1]))\nX_train_reduced = X_train_reduced.reshape(X_train.shape[0], X_train.shape[1], 700)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test_reduced = pca.transform(x_test.reshape(-1, x_test.shape[-1]))\nX_test_reduced = X_test_reduced.reshape(x_test.shape[0], x_test.shape[1], 700)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import joblib\njoblib.dump(scaler,\"scalerV2.pkl\")\njoblib.dump(pca,\"pcaV2_700.pkl\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport random\n\ndef augment_sequence(sequence, noise_std=0.001, dropout_prob=0.1):\n    \"\"\"\n    sequence: np.array of shape (30, 1662)\n    \"\"\"\n    seq = sequence.copy()\n\n    # 1. Add small Gaussian noise\n    seq += np.random.normal(0, noise_std, seq.shape)\n\n    # 2. Random feature scaling\n    seq *= np.random.uniform(0.95, 1.02)\n\n    # 3. Random frame dropout\n    if random.random() < dropout_prob:\n        drop_idx = np.random.choice(seq.shape[0], size=1, replace=False)\n        seq = np.delete(seq, drop_idx, axis=0)\n        # Pad back to 30 frames (duplicate last frame)\n        seq = np.pad(seq, ((0, 1), (0, 0)), mode='edge')\n\n    return seq\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_augmented = []\ny_augmented = []\n\nfor x, y in zip(X_train_reduced, y_train):\n    X_augmented.append(x)\n    y_augmented.append(y)\n\n    # Add one augmented version of each sequence\n    X_augmented.append(augment_sequence(x))\n    y_augmented.append(y)\n\nX_augmented = np.array(X_augmented)\ny_augmented = np.array(y_augmented)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_augmented.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_augmented.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM,Dense,Dropout,BatchNormalization,Bidirectional\nfrom tensorflow.keras.callbacks import TensorBoard\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.regularizers import l2\n\n\nlog_dir = os.path.join('logs')\n\ntb_callback = TensorBoard(log_dir=log_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"early_stop = EarlyStopping(\n    monitor='val_loss',\n    patience=40,              # stop if val_loss doesn't improve for 20 epochs\n    restore_best_weights=True\n)\n\ncheckpoint = ModelCheckpoint(\n    'best_model.keras',\n    monitor='val_loss',\n    save_best_only=True,\n    verbose=1\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ReduceLROnPlateau\n\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5,\n    patience=15,\n    min_lr=1e-6,\n    verbose=1\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_reduced.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Bidirectional(LSTM(64,return_sequences=True,activation=\"tanh\",input_shape=(30,700))))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\n\n\nmodel.add(Bidirectional(LSTM(64,return_sequences=False,activation=\"tanh\")))\nmodel.add(Dropout(0.2))\n\n\nmodel.add(Dense(128,activation='relu',kernel_regularizer=l2(5e-6)))\nmodel.add(Dropout(0.15))\n\nmodel.add(Dense(64,activation='relu',))\nmodel.add(Dropout(0.15))\n\nmodel.add(Dense(32,activation='relu',))\nmodel.add(Dropout(0.1))\n\nmodel.add(Dense(actions.shape[0],activation='softmax'))\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(optimizer=\"Adam\",loss=\"categorical_crossentropy\",metrics=['accuracy'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.fit(X_augmented,y_augmented,validation_split=0.2,epochs=2000,callbacks=[tb_callback,early_stop,checkpoint,reduce_lr])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import load_model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loss,acc = model.evaluate(X_test_reduced,y_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(acc)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save(\"model_12.keras\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = load_model(\"model_11.keras\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Base CNN Model","metadata":{}},{"cell_type":"code","source":"import cv2,os","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DATA_PATH = os.path.join('/kaggle/input/asl-dataset/asl_dataset/asl_dataset')\noutput_path = \"/kaggle/working/outputs\"\nFRAMES_PER_VIDEO = 30\nFRAME_SIZE = (224,224)\nOUTPUT_PATH = \"/kaggle/working/outputs\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.listdir(DATA_PATH)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2, os\n\ndef extract_frames(video_path, output_folder, frame_count=60):  \n    os.makedirs(output_folder, exist_ok=True)\n\n    cap = cv2.VideoCapture(video_path)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    if total_frames == 0:\n        print(f\"Skipping (no frames): {video_path}\")\n        return \n    \n    # FIXED evenly spaced frames\n    frame_indices = [int(i * total_frames / frame_count) for i in range(frame_count)]\n    frame_indices = sorted(set(frame_indices))\n\n    saved = 0\n    count = 0\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        if count in frame_indices:\n            frame = cv2.resize(frame, FRAME_SIZE)\n            frame_name = f\"frame_{saved}.jpg\"\n            cv2.imwrite(os.path.join(output_folder, frame_name), frame)\n            saved += 1\n        \n        count += 1\n\n        if saved >= len(frame_indices):\n            break\n\n    cap.release()\n    print(f\"Extracted {saved} frames from {video_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_all_frames():\n    for class_name in os.listdir(DATA_PATH):\n        class_path = os.path.join(DATA_PATH, class_name)\n\n        if not os.path.isdir(class_path):\n            continue\n\n        print(f\"\\nProcessing Class: {class_name}\")\n\n        output_dir = os.path.join(output_path, class_name)\n        os.makedirs(output_dir, exist_ok=True)\n\n        for video_file in os.listdir(class_path):\n            if video_file.lower().endswith((\".mp4\", \".avi\", \".mov\", \".mkv\")):\n\n                video_path = os.path.join(class_path, video_file)\n\n                video_output_folder = os.path.join(\n                    output_dir,\n                    video_file.split('.')[0]\n                )\n                os.makedirs(video_output_folder, exist_ok=True)\n\n                print(f\"Extracting ALL frames from: {video_file}\")\n                extract_frames(video_path, video_output_folder)\n\n    print(\"\\nAll Frames Extracted Successfully\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"extract_all_frames()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_frame(frame_path,dataset_dir,frame):\n    os.makedirs(dataset_dir,exist_ok=True)\n    \n    img = cv2.imread(frame_path)\n    resized_img = cv2.resize(img,(224,224),interpolation=cv2.INTER_AREA)\n    rgb_img = cv2.cvtColor(resized_img,cv2.COLOR_BGR2RGB)\n    \n    norm_img = cv2.normalize(\n    rgb_img, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n     \n    final_img = norm_img\n    bgr_final = cv2.cvtColor(final_img, cv2.COLOR_RGB2BGR)\n    folder_path = os.path.join(dataset_dir)\n    cv2.imwrite(os.path.join(folder_path,frame),bgr_final)\n\n    \n","metadata":{"trusted":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preprocess_output_path = \"/kaggle/working/new_dataset_asl\"\nalphabetinput = '/kaggle/input/dataset-pakistani-sign-language/dataset'\nos.makedirs(preprocess_output_path,exist_ok=True)\n\ndef process_all_frames():\n    for class_name in os.listdir(DATA_PATH):\n        class_path = os.path.join(DATA_PATH,class_name)\n        \n        if not os.path.isdir(class_path):\n            continue\n    \n        for video_num in os.listdir(class_path):\n            frame_dir = os.path.join(class_path,video_num)\n    \n            for frame in os.listdir(frame_dir):\n                \n                frame_path = os.path.join(frame_dir,frame)\n                \n                dataset_path = os.path.join(preprocess_output_path,class_name)\n                dataset_dir = os.path.join(dataset_path,video_num)\n                print(dataset_dir)\n                print(f\"Preprocessing {frame} of {class_name}\")\n                preprocess_frame(frame_path,dataset_dir,frame)\n                \n                \n    \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#for alphabets\ndef process_all_frames():\n    for class_name in os.listdir(DATA_PATH):\n        class_path = os.path.join(DATA_PATH, class_name)\n\n        if not os.path.isdir(class_path):\n            continue\n\n        # output folder for this class\n        output_class_dir = os.path.join(preprocess_output_path, class_name)\n        os.makedirs(output_class_dir, exist_ok=True)\n\n        # iterate all images\n        for frame in os.listdir(class_path):\n            frame_path = os.path.join(class_path, frame)\n\n            if not frame.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n                continue  # skip non-images\n            \n            print(f\"Preprocessing {frame} of {class_name}\")\n            preprocess_frame(frame_path, output_class_dir, frame)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"process_all_frames()\n","metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DATASET_PATH = \"/kaggle/working/new_dataset_asl\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.listdir(DATASET_PATH)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 32\nimg_size= (224,224)\n\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    DATASET_PATH,\n    validation_split=0.2,\n    subset=\"training\",\n    seed=42,\n    image_size=img_size,\n    batch_size=batch_size\n)\n\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    DATASET_PATH,\n    validation_split=0.2,\n    subset=\"validation\",\n    seed=42,\n    image_size=img_size,\n    batch_size=batch_size\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BIGGER_ASL_DATASET = \"/kaggle/input/american-sign-language/ASL_Dataset\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = \"/kaggle/input/asl-dataset-mine/archive/ASL_Alphabet_Dataset/asl_alphabet_train\"\ntest = \"/kaggle/input/asl-dataset-mine/archive/ASL_Alphabet_Dataset/asl_alphabet_test\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport numpy as np","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nbatch_size = 32\nimg_size= (224,224)\n\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    train,\n    validation_split=0.2,\n    subset=\"training\",\n    seed=42,\n    image_size=img_size,\n    batch_size=batch_size\n)\n\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    train,\n    validation_split=0.2,\n    subset=\"validation\",\n    seed=42,\n    image_size=img_size,\n    batch_size=batch_size\n)\n\ntest_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    test,\n    seed=42,\n    image_size=img_size,\n    batch_size=batch_size\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_names = train_ds.class_names\nprint(class_names)\n\nfor images,labels in train_ds.take(1):\n    print(images.shape)\n    print(labels.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(class_names)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_augmentation=tf.keras.Sequential([\n    tf.keras.layers.RandomRotation(0.15),\n    tf.keras.layers.RandomTranslation(0.1,0.1),\n    tf.keras.layers.RandomZoom(0.15),\n    # tf.keras.layers.RandomBrightness(0.2),\n])\nnormalization_layer = tf.keras.layers.Rescaling(1./255)\n\nbrightness_augmentation = tf.keras.Sequential([\n    tf.keras.layers.RandomBrightness(0.2,value_range=(0.0,1.0)),\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prepare_dataset(ds,augment=False):\n    if augment:\n        ds = ds.map(lambda x,y:(data_augmentation(x,training=True),y),num_parallel_calls=tf.data.AUTOTUNE)\n        \n    ds = ds.map(lambda x,y:(normalization_layer(x),y), num_parallel_calls=tf.data.AUTOTUNE)\n\n    if augment:\n        ds = ds.map(lambda x, y:(brightness_augmentation(x,training=True),y), num_parallel_calls=tf.data.AUTOTUNE)\n        # ds = ds.map(lambda x, y:(tf.clip_by_values(x,0.0,1.0),y), num_parallel_calls=tf.data.AUTOTUNE)\n        \n        \n\n    \n\n    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n    return ds\n    \n\ntrain_ds = prepare_dataset(train_ds,augment=True)\nval_ds = prepare_dataset(val_ds,augment=False)\ntest_ds = prepare_dataset(test_ds,augment=True)\n\n\n\nfor images, labels in train_ds.take(1):\n    print(f\"After normalization - Min: {images.numpy().min():.3f}, Max: {images.numpy().max():.3f}, Mean: {images.numpy().mean():.3f}\")\n    # Should now print: Min: 0.0, Max: 1.0 (approximately)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout,Rescaling\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_classes=len(class_names)\n\nmodel = Sequential([\n    \n    Conv2D(32,(3,3),activation='relu',input_shape=(224,224,3)),\n    MaxPooling2D(2,2),\n    Dropout(0.25),\n    \n    Conv2D(64,(3,3),activation='relu'),\n    MaxPooling2D(2,2),\n    Dropout(0.3),\n\n    Conv2D(128,(3,3),activation='relu'),\n    MaxPooling2D(2,2),\n    Dropout(0.3),\n\n    Flatten(),\n\n    Dense(128,activation='relu'),\n    Dropout(0.5),\n    Dense(64,activation='relu'),\n    Dropout(0.5),\n    Dense(num_classes,activation='softmax')\n    \n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nearly_stopping = EarlyStopping(\n    monitor='val_loss',           # Monitor validation loss\n    patience=3,                    # Stop if no improvement for 5 epochs\n    restore_best_weights=True,     # Restore weights from best epoch\n    verbose=1                      # Print messages\n)\n\n# Optional: Save the best model during training\nmodel_checkpoint = ModelCheckpoint(\n    'best_model.keras',           # or 'best_model.h5'\n    monitor='val_loss',\n    save_best_only=True,\n    verbose=1\n)\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\nmodel.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=35,\n    batch_size=32,\n    callbacks=[early_stopping, model_checkpoint]\n\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 2, figsize=(14, 5))\n\n# Accuracy\nax[0].plot(history.history['accuracy'], label='Train Acc')\nax[0].plot(history.history['val_accuracy'], label='Val Acc')\nax[0].set_title(\"Accuracy\")\nax[0].set_xlabel(\"Epoch\")\nax[0].set_ylabel(\"Accuracy\")\nax[0].legend()\nax[0].grid(True)\n\n# Loss\nax[1].plot(history.history['loss'], label='Train Loss')\nax[1].plot(history.history['val_loss'], label='Val Loss')\nax[1].set_title(\"Loss\")\nax[1].set_xlabel(\"Epoch\")\nax[1].set_ylabel(\"Loss\")\nax[1].legend()\nax[1].grid(True)\n\nplt.show()\nplt.savefig(\"signbuddy_cnn.png\",dpi=300,bbox_inches='tight')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import classification_report, confusion_matrix\n\ny_true = []\ny_pred = []\n\nfor batch_x, batch_y in test_ds:\n    # predict on this batch\n    batch_preds = model.predict(batch_x, verbose=0)\n    batch_pred_classes = np.argmax(batch_preds, axis=1)\n\n    # store\n    y_true.extend(batch_y.numpy())\n    y_pred.extend(batch_pred_classes)\n\ny_true = np.array(y_true)\ny_pred = np.array(y_pred)\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_true, y_pred, target_names=class_names))\n\nprint(\"\\nConfusion Matrix:\")\ncm = confusion_matrix(y_true, y_pred)\nprint(cm)\n\ntest_loss, test_accuracy = model.evaluate(test_ds)\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Test Loss: {test_loss:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Get one batch for visualization\nfor images, labels in test_ds.take(1):\n    preds = model.predict(images)\n    pred_classes = np.argmax(preds, axis=1)\n    \n    plt.figure(figsize=(15, 10))\n    for i in range(min(9, len(images))):\n        plt.subplot(3, 3, i+1)\n        plt.imshow(images[i].numpy())\n        plt.title(f\"True: {class_names[labels[i]]}\\nPred: {class_names[pred_classes[i]]}\")\n        plt.axis('off')\n    plt.tight_layout()\n\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save(\"signbuddy_cnn_asl_model_6.h5\")\nmodel.save(\"signbuddy_cnn_asl_model_6.keras\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import load_model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = load_model('/kaggle/working/signbuddy_cnn_asl_model_4.keras')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate on the SAME test set that gave you 100% accuracy\ntest_loss, test_acc = model.evaluate(test_ds)\nprint(f\"Test Accuracy: {test_acc}\")\n\n# Get predictions for the entire test set\npredictions = []\ntrue_labels = []\n\nfor images, labels in test_ds:\n    preds = model.predict(images, verbose=0)\n    predictions.extend(preds.argmax(axis=1))\n    true_labels.extend(labels.numpy())\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(true_labels, predictions, target_names=class_names))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\n\n\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\ntflite_model = converter.convert()\n\nwith open(\"asl_alphabert_model.tflite\", \"wb\") as f:\n    f.write(tflite_model)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import (\n    Conv2D, MaxPooling2D, Flatten, Dense, Dropout, \n    BatchNormalization, GlobalAveragePooling2D\n)\nfrom tensorflow.keras.callbacks import (\n    EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n)\nimport numpy as np\n\n# ============================================================================\n# LOAD DATA WITH AUGMENTATION + PREFETCHING\n# ============================================================================\n\n# TRAIN_PATH = \"/kaggle/input/american-sign-language/ASL_Dataset/Train\"\n# TEST_PATH = \"/kaggle/input/american-sign-language/ASL_Dataset/Test\"\n\nbatch_size = 32  # Reduced from 32 - uses half the memory!\nimg_size = (224, 224)\n\n# Load datasets using tf.keras.utils\ntrain_ds = tf.keras.utils.image_dataset_from_directory(\n    TRAIN_PATH,\n    validation_split=0.2,\n    subset=\"training\",\n    seed=42,\n    image_size=img_size,\n    batch_size=batch_size,\n    shuffle=True\n)\n\nval_ds = tf.keras.utils.image_dataset_from_directory(\n    TRAIN_PATH,\n    validation_split=0.2,\n    subset=\"validation\",\n    seed=42,\n    image_size=img_size,\n    batch_size=batch_size,\n    shuffle=False\n)\n\ntest_ds = tf.keras.utils.image_dataset_from_directory(\n    TEST_PATH,\n    seed=42,\n    image_size=img_size,\n    batch_size=batch_size,\n    shuffle=False\n)\n\nclass_names = train_ds.class_names\nnum_classes = len(class_names)\nprint(f\"Number of classes: {num_classes}\")\nprint(f\"Classes: {class_names}\")\n\n# ============================================================================\n# DATA AUGMENTATION LAYER - FIXED VERSION\n# ============================================================================\n\n# Option 1: Augment on raw images, then normalize (SAFEST)\ndata_augmentation_raw = tf.keras.Sequential([\n    tf.keras.layers.RandomRotation(0.15),\n    tf.keras.layers.RandomTranslation(0.1, 0.1, fill_mode='nearest'),\n    tf.keras.layers.RandomZoom(0.15, fill_mode='nearest'),\n    # Skip RandomBrightness here - will add after normalization\n])\n\n# Normalization layer\nnormalization_layer = tf.keras.layers.Rescaling(1./255)\n\n# Brightness adjustment (works better on normalized data)\nbrightness_augmentation = tf.keras.Sequential([\n    tf.keras.layers.RandomBrightness(0.2, value_range=(0.0, 1.0)),  # Specify range!\n])\n\n# Apply augmentation in correct sequence\ndef prepare_dataset(ds, augment=False):\n    if augment:\n        # Step 1: Geometric augmentations on raw images (0-255)\n        ds = ds.map(lambda x, y: (data_augmentation_raw(x, training=True), y),\n                    num_parallel_calls=tf.data.AUTOTUNE)\n    \n    # Step 2: Normalize to 0-1\n    ds = ds.map(lambda x, y: (normalization_layer(x), y), \n                num_parallel_calls=tf.data.AUTOTUNE)\n    \n    if augment:\n        # Step 3: Brightness on normalized images (0-1)\n        ds = ds.map(lambda x, y: (brightness_augmentation(x, training=True), y),\n                    num_parallel_calls=tf.data.AUTOTUNE)\n    \n    # Step 4: Cache and prefetch\n    # REMOVED CACHE to save GPU memory\n    # if not augment:\n    #     ds = ds.cache()  # Only cache val/test\n    \n    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n    \n    return ds\n\ntrain_ds = prepare_dataset(train_ds, augment=True)\nval_ds = prepare_dataset(val_ds, augment=False)\ntest_ds = prepare_dataset(test_ds, augment=False)\n\n# Verify normalization is working correctly\nprint(\"\\nðŸ” Verifying normalization on TRAINING data (with augmentation)...\")\nfor images, labels in train_ds.take(1):\n    min_val = tf.reduce_min(images).numpy()\n    max_val = tf.reduce_max(images).numpy()\n    mean_val = tf.reduce_mean(images).numpy()\n    print(f\"  Min:  {min_val:.3f}\")\n    print(f\"  Max:  {max_val:.3f}\")\n    print(f\"  Mean: {mean_val:.3f}\")\n    if min_val >= 0.0 and max_val <= 1.0:\n        print(\"  âœ… CORRECT: Values in [0, 1] range!\")\n    else:\n        print(\"  âŒ ERROR: Values outside [0, 1] range!\")\n\nprint(\"\\nðŸ” Verifying normalization on VALIDATION data (no augmentation)...\")\nfor images, labels in val_ds.take(1):\n    min_val = tf.reduce_min(images).numpy()\n    max_val = tf.reduce_max(images).numpy()\n    mean_val = tf.reduce_mean(images).numpy()\n    print(f\"  Min:  {min_val:.3f}\")\n    print(f\"  Max:  {max_val:.3f}\")\n    print(f\"  Mean: {mean_val:.3f}\")\n    if min_val >= 0.0 and max_val <= 1.0:\n        print(\"  âœ… CORRECT: Values in [0, 1] range!\")\n    else:\n        print(\"  âŒ ERROR: Values outside [0, 1] range!\")\n    \nprint(\"\\nâœ… Datasets prepared with caching and prefetching!\")\n\n# ============================================================================\n# IMPROVED CNN MODEL\n# ============================================================================\n\ndef create_improved_model(num_classes):\n    model = Sequential([\n        # Block 1\n        Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(224,224,3)),\n        BatchNormalization(),\n        Conv2D(32, (3,3), activation='relu', padding='same'),\n        BatchNormalization(),\n        MaxPooling2D(2,2),\n        Dropout(0.25),\n        \n        # Block 2\n        Conv2D(64, (3,3), activation='relu', padding='same'),\n        BatchNormalization(),\n        Conv2D(64, (3,3), activation='relu', padding='same'),\n        BatchNormalization(),\n        MaxPooling2D(2,2),\n        Dropout(0.25),\n        \n        # Block 3\n        Conv2D(128, (3,3), activation='relu', padding='same'),\n        BatchNormalization(),\n        Conv2D(128, (3,3), activation='relu', padding='same'),\n        BatchNormalization(),\n        MaxPooling2D(2,2),\n        Dropout(0.3),\n        \n        # Block 4\n        Conv2D(256, (3,3), activation='relu', padding='same'),\n        BatchNormalization(),\n        Conv2D(256, (3,3), activation='relu', padding='same'),\n        BatchNormalization(),\n        MaxPooling2D(2,2),\n        Dropout(0.3),\n        \n        # Classification head\n        GlobalAveragePooling2D(),  # Better than Flatten\n        \n        Dense(512, activation='relu'),\n        BatchNormalization(),\n        Dropout(0.5),\n        \n        Dense(256, activation='relu'),\n        BatchNormalization(),\n        Dropout(0.4),\n        \n        Dense(num_classes, activation='softmax')\n    ])\n    \n    return model\n\nmodel = create_improved_model(num_classes)\n\n# ============================================================================\n# ADVANCED CALLBACKS\n# ============================================================================\n\nearly_stopping = EarlyStopping(\n    monitor='val_loss',\n    patience=15,\n    restore_best_weights=True,\n    verbose=1\n)\n\nmodel_checkpoint = ModelCheckpoint(\n    'best_asl_model.keras',\n    monitor='val_accuracy',\n    save_best_only=True,\n    mode='max',\n    verbose=1\n)\n\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5,\n    patience=5,\n    min_lr=1e-7,\n    verbose=1\n)\n\n# ============================================================================\n# COMPILE WITH OPTIMIZED SETTINGS\n# ============================================================================\n\n# CRITICAL: Match loss function to label format!\n# sparse_categorical_crossentropy for integer labels (0, 1, 2, ...)\n# categorical_crossentropy for one-hot labels ([0,0,1,0,...])\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n    loss='sparse_categorical_crossentropy',  # For integer labels\n    metrics=['accuracy']\n)\n\nmodel.summary()\n\nprint(\"\\nâœ… Model compiled with sparse_categorical_crossentropy\")\n\n# ============================================================================\n# TRAINING\n# ============================================================================\n\nhistory = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=3,\n    callbacks=[early_stopping, model_checkpoint, reduce_lr],\n    verbose=1\n)\n\n# ============================================================================\n# EVALUATION\n# ============================================================================\n\ntest_loss, test_accuracy = model.evaluate(test_ds)\nprint(f\"\\n{'='*60}\")\nprint(f\"FINAL TEST ACCURACY: {test_accuracy*100:.2f}%\")\nprint(f\"FINAL TEST LOSS: {test_loss:.4f}\")\nprint(f\"{'='*60}\")\n\n# ============================================================================\n# DETAILED METRICS\n# ============================================================================\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Get predictions\ny_pred = []\ny_true = []\n\nfor batch_x, batch_y in test_ds:\n    preds = model.predict(batch_x, verbose=0)\n    y_pred.extend(np.argmax(preds, axis=1))\n    y_true.extend(batch_y.numpy())\n\ny_pred = np.array(y_pred)\ny_true = np.array(y_true)\n\n# Classification report\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_true, y_pred, target_names=class_names))\n\n# Confusion matrix\ncm = confusion_matrix(y_true, y_pred)\nplt.figure(figsize=(20, 16))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=class_names, yticklabels=class_names)\nplt.title('Confusion Matrix')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.tight_layout()\nplt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n# ============================================================================\n# TRAINING CURVES\n# ============================================================================\n\nfig, ax = plt.subplots(1, 2, figsize=(14, 5))\n\n# Accuracy\nax[0].plot(history.history['accuracy'], label='Train Acc', linewidth=2)\nax[0].plot(history.history['val_accuracy'], label='Val Acc', linewidth=2)\nax[0].set_title(\"Model Accuracy\", fontsize=14, fontweight='bold')\nax[0].set_xlabel(\"Epoch\", fontsize=12)\nax[0].set_ylabel(\"Accuracy\", fontsize=12)\nax[0].legend(fontsize=11)\nax[0].grid(True, alpha=0.3)\n\n# Loss\nax[1].plot(history.history['loss'], label='Train Loss', linewidth=2)\nax[1].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\nax[1].set_title(\"Model Loss\", fontsize=14, fontweight='bold')\nax[1].set_xlabel(\"Epoch\", fontsize=12)\nax[1].set_ylabel(\"Loss\", fontsize=12)\nax[1].legend(fontsize=11)\nax[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(\"training_curves.png\", dpi=300, bbox_inches='tight')\nplt.show()\n\n# ============================================================================\n# SAVE MODEL\n# ============================================================================\n\nmodel.save(\"asl_final_model.keras\")\nprint(\"\\nModel saved as 'asl_final_model.keras'\")\n\n# Convert to TFLite\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\ntflite_model = converter.convert()\n\nwith open(\"asl_model_1.tflite\", \"wb\") as f:\n    f.write(tflite_model)\n    \nprint(\"TFLite model saved as 'asl_model0.tflite'\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-15T16:01:07.309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import (\n    Conv2D, MaxPooling2D, Flatten, Dense, Dropout, \n    BatchNormalization, GlobalAveragePooling2D, Input\n)\nfrom tensorflow.keras.applications import MobileNetV2, EfficientNetB0\nfrom tensorflow.keras.callbacks import (\n    EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n)\nfrom tensorflow.keras.regularizers import l2\nimport numpy as np\n\n# ============================================================================\n# MEMORY OPTIMIZATION SETTINGS\n# ============================================================================\n\n# Clear any previous models from memory\ntf.keras.backend.clear_session()\n\n# Enable memory growth for GPU (prevents allocation of all GPU memory at once)\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"âœ… GPU memory growth enabled for {len(gpus)} GPU(s)\")\n    except RuntimeError as e:\n        print(f\"âš ï¸  {e}\")\n\n# Set TensorFlow to use less memory\nimport os\nos.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"MEMORY-EFFICIENT CONFIGURATION\")\nprint(\"=\"*60)\n\n# ============================================================================\n# ANTI-OVERFITTING CONFIGURATION\n# ============================================================================\n\nTRAIN_PATH = \"/kaggle/input/american-sign-language/ASL_Dataset/Train\"\nTEST_PATH = \"/kaggle/input/american-sign-language/ASL_Dataset/Test\"\n\nbatch_size = 32  # REDUCED from 32 - uses half the memory!\nimg_size = (128, 128)  # REDUCED from 128 - 30% less memory per image!\n\n# ============================================================================\n# AGGRESSIVE DATA AUGMENTATION (Critical for Generalization)\n# ============================================================================\n\ndata_augmentation = tf.keras.Sequential([\n    # Geometric transformations\n    tf.keras.layers.RandomRotation(0.2),  # Â±20% rotation\n    tf.keras.layers.RandomTranslation(0.15, 0.15),  # Shift hands\n    tf.keras.layers.RandomZoom(0.2),  # Zoom in/out\n    tf.keras.layers.RandomFlip(\"horizontal\"),  # Mirror hands\n    \n    # Color/lighting variations (critical for generalization!)\n    tf.keras.layers.RandomBrightness(0.3),  # Lighting changes\n    tf.keras.layers.RandomContrast(0.3),  # Contrast variations\n    \n    # Optional: Add noise to prevent memorization\n    # tf.keras.layers.GaussianNoise(0.01),\n])\n\n# normalization = tf.keras.layers.Rescaling(1./255)\n\n# ============================================================================\n# LOAD DATA - MEMORY EFFICIENT VERSION\n# ============================================================================\n\n# CRITICAL: Don't load all data at once!\n# Use generators to load batches on-the-fly\n\ntrain_ds = tf.keras.utils.image_dataset_from_directory(\n    TRAIN_PATH,\n    validation_split=0.2,\n    subset=\"training\",\n    seed=42,\n    image_size=img_size,\n    batch_size=batch_size,\n    shuffle=True,\n    label_mode='int'\n)\n\nval_ds = tf.keras.utils.image_dataset_from_directory(\n    TRAIN_PATH,\n    validation_split=0.2,\n    subset=\"validation\",\n    seed=42,\n    image_size=img_size,\n    batch_size=batch_size,\n    shuffle=False,\n    label_mode='int'\n)\n\ntest_ds = tf.keras.utils.image_dataset_from_directory(\n    TEST_PATH,\n    seed=42,\n    image_size=img_size,\n    batch_size=batch_size,\n    shuffle=False\n)\n\nclass_names = train_ds.class_names\nnum_classes = len(class_names)\nprint(f\"Number of classes: {num_classes}\")\nprint(f\"Classes: {class_names}\")\n\n# Apply augmentation - NO CACHING (saves memory!)\ndef prepare_dataset(ds, augment=False):\n\n    \n    # Augment if training\n    if augment:\n        ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y),\n                    num_parallel_calls=tf.data.AUTOTUNE)\n    \n    # CRITICAL: Only prefetch, NO cache() to save memory\n    return ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n\ntrain_ds = prepare_dataset(train_ds, augment=True)\nval_ds = prepare_dataset(val_ds, augment=False)\ntest_ds = prepare_dataset(test_ds, augment=False)\n\nprint(\"âœ… Data loaded WITHOUT caching (memory efficient!)\")\n\n# ============================================================================\n# SOLUTION 1: TRANSFER LEARNING (BEST FOR OVERFITTING)\n# ============================================================================\n\ndef create_transfer_learning_model(num_classes, trainable_layers=10):\n    \"\"\"\n    MEMORY EFFICIENT: Smaller input + fewer trainable layers\n    \"\"\"\n    \n    # Load pre-trained model\n    base_model = MobileNetV2(\n        input_shape=(96, 96, 3),  # Smaller input size\n        include_top=False,\n        weights='imagenet'\n    )\n    \n    # Freeze MORE layers (less memory during training)\n    base_model.trainable = True\n    for layer in base_model.layers[:-trainable_layers]:  # Only train last 10 layers\n        layer.trainable = False\n    \n    print(f\"âœ… MobileNetV2: Only {trainable_layers} layers trainable (memory efficient)\")\n    \n    model = Sequential([\n        base_model,\n        GlobalAveragePooling2D(),\n        \n        # SMALLER dense layers\n        Dense(128, activation='relu', kernel_regularizer=l2(0.001)),  # Reduced from 256\n        Dropout(0.5),\n        \n        Dense(64, activation='relu', kernel_regularizer=l2(0.001)),  # Reduced from 128\n        Dropout(0.4),\n        \n        Dense(num_classes, activation='softmax')\n    ])\n    \n    return model\n\n# ============================================================================\n# SOLUTION 2: LIGHTWEIGHT CNN WITH HEAVY REGULARIZATION\n# ============================================================================\n\ndef create_regularized_cnn(num_classes):\n    \"\"\"\n    MEMORY EFFICIENT: Smaller, simpler CNN\n    \"\"\"\n    \n    model = Sequential([\n        # Block 1\n        Conv2D(32, (3,3), activation='relu', padding='same', \n               kernel_regularizer=l2(0.001), input_shape=(96,96,3)),  # 96x96 input\n        BatchNormalization(),\n        MaxPooling2D(2,2),\n        Dropout(0.3),\n        \n        # Block 2 - SMALLER filters\n        Conv2D(64, (3,3), activation='relu', padding='same', \n               kernel_regularizer=l2(0.001)),\n        BatchNormalization(),\n        MaxPooling2D(2,2),\n        Dropout(0.3),\n        \n        # Block 3 - Removed 4th block to save memory\n        Conv2D(128, (3,3), activation='relu', padding='same', \n               kernel_regularizer=l2(0.001)),\n        BatchNormalization(),\n        MaxPooling2D(2,2),\n        Dropout(0.4),\n        \n        # Smaller classification head\n        GlobalAveragePooling2D(),\n        \n        Dense(128, activation='relu', kernel_regularizer=l2(0.001)),  # Reduced from 256\n        Dropout(0.5),\n        \n        Dense(num_classes, activation='softmax')\n    ])\n    \n    return model\n\n# ============================================================================\n# SOLUTION 3: TINY MODEL (Harder to Overfit)\n# ============================================================================\n\ndef create_tiny_model(num_classes):\n    \"\"\"\n    ULTRA LIGHTWEIGHT - use if still crashing\n    \"\"\"\n    \n    model = Sequential([\n        Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(96,96,3)),\n        MaxPooling2D(2,2),\n        Dropout(0.25),\n        \n        Conv2D(64, (3,3), activation='relu', padding='same'),\n        MaxPooling2D(2,2),\n        Dropout(0.25),\n        \n        GlobalAveragePooling2D(),\n        \n        Dense(64, activation='relu'),  # Very small\n        Dropout(0.5),\n        \n        Dense(num_classes, activation='softmax')\n    ])\n    \n    return model\n\n# ============================================================================\n# CREATE MODEL (Choose based on memory)\n# ============================================================================\n\n# OPTION 1: If still crashing, use TINY model\nmodel = create_tiny_model(num_classes)\n\n# OPTION 2: If tiny works, try regularized CNN\n# model = create_regularized_cnn(num_classes)\n\n# OPTION 3: If you have enough memory, use transfer learning (best accuracy)\n# model = create_transfer_learning_model(num_classes, trainable_layers=10)\n\nprint(f\"\\nâœ… Model created with {model.count_params():,} parameters\")\n\n# ============================================================================\n# COMPILE WITH LOWER LEARNING RATE (Prevents Overfitting)\n# ============================================================================\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),  # Lower LR!\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel.summary()\n\n# ============================================================================\n# ANTI-OVERFITTING CALLBACKS\n# ============================================================================\n\n# Stop early when validation stops improving\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy',  # Watch validation, not training\n    patience=10,  # Stop if no improvement for 10 epochs\n    restore_best_weights=True,\n    verbose=1,\n    mode='max'\n)\n\n# Save best model based on VALIDATION accuracy\nmodel_checkpoint = ModelCheckpoint(\n    'best_generalized_model.keras',\n    monitor='val_accuracy',\n    save_best_only=True,\n    mode='max',\n    verbose=1\n)\n\n# Reduce learning rate when stuck\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5,\n    patience=5,\n    min_lr=1e-7,\n    verbose=1\n)\n\n# ============================================================================\n# TRAINING WITH VALIDATION MONITORING\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING WITH ANTI-OVERFITTING STRATEGIES\")\nprint(\"=\"*60)\n\nhistory = model.fit(\n    train_ds,\n    validation_data=val_ds,  # Monitor validation constantly\n    epochs=50,  # Will stop early if overfitting\n    callbacks=[early_stopping, model_checkpoint, reduce_lr],\n    verbose=1\n)\n\n# ============================================================================\n# DETECT OVERFITTING IN RESULTS\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"OVERFITTING CHECK\")\nprint(\"=\"*60)\n\nfinal_train_acc = history.history['accuracy'][-1]\nfinal_val_acc = history.history['val_accuracy'][-1]\ngap = final_train_acc - final_val_acc\n\nprint(f\"Final Training Accuracy:   {final_train_acc*100:.2f}%\")\nprint(f\"Final Validation Accuracy: {final_val_acc*100:.2f}%\")\nprint(f\"Gap (Train - Val):         {gap*100:.2f}%\")\n\nif gap > 0.15:  # More than 15% gap\n    print(\"\\nâš ï¸  HIGH OVERFITTING DETECTED!\")\n    print(\"Solutions:\")\n    print(\"  1. Add MORE data augmentation\")\n    print(\"  2. Use SMALLER model (fewer parameters)\")\n    print(\"  3. Increase Dropout rates\")\n    print(\"  4. Get more training data\")\nelif gap > 0.05:\n    print(\"\\nâš ï¸  Mild overfitting - manageable\")\nelse:\n    print(\"\\nâœ… Good generalization!\")\n\n# ============================================================================\n# TEST ON COMPLETELY UNSEEN DATA (MOST IMPORTANT!)\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"REAL-WORLD TEST (Different people/conditions)\")\nprint(\"=\"*60)\n\ntest_loss, test_accuracy = model.evaluate(test_ds)\n\nprint(f\"\\nTraining Accuracy:   {final_train_acc*100:.2f}%\")\nprint(f\"Validation Accuracy: {final_val_acc*100:.2f}%\")\nprint(f\"TEST Accuracy:       {test_accuracy*100:.2f}%  â† REAL PERFORMANCE\")\nprint(f\"{'='*60}\")\n\n# THIS IS THE NUMBER THAT MATTERS!\nif test_accuracy < 0.7 and final_val_acc > 0.95:\n    print(\"\\nðŸš¨ SEVERE DATA LEAKAGE DETECTED!\")\n    print(\"Val = {:.0f}% but Test = {:.0f}%\".format(final_val_acc*100, test_accuracy*100))\n    print(\"\\nYour validation set is TOO SIMILAR to training!\")\n    print(\"\\nSOLUTIONS:\")\n    print(\"  1. Get test images from DIFFERENT people\")\n    print(\"  2. Use DIFFERENT backgrounds\")\n    print(\"  3. Use DIFFERENT lighting conditions\")\n    print(\"  4. Use DIFFERENT cameras/phones\")\n    print(\"  5. Add natural variations (jewelry, sleeves, skin tones)\")\nelif test_accuracy < 0.85:\n    print(\"\\nâš ï¸  Model struggles with real-world variety\")\n    print(\"Need MORE diverse training data!\")\nelse:\n    print(\"\\nâœ… Good generalization to new data!\")\n\n# ============================================================================\n# ANALYZE WHAT THE MODEL IS ACTUALLY LEARNING\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"CHECKING: Is the model learning HAND SHAPES or BACKGROUND?\")\nprint(\"=\"*60)\n\n# Sample some test predictions to see what's failing\nsample_images = []\nsample_labels = []\nsample_preds = []\n\nfor batch_x, batch_y in test_ds.take(3):\n    preds = model.predict(batch_x, verbose=0)\n    sample_images.extend(batch_x.numpy()[:5])\n    sample_labels.extend(batch_y.numpy()[:5])\n    sample_preds.extend(np.argmax(preds[:5], axis=1))\n\nprint(\"\\nSample Predictions:\")\nfor i in range(min(15, len(sample_labels))):\n    true_label = class_names[sample_labels[i]]\n    pred_label = class_names[sample_preds[i]]\n    status = \"âœ…\" if true_label == pred_label else \"âŒ\"\n    print(f\"  {status} True: {true_label:2s} | Predicted: {pred_label:2s}\")\n\n# ============================================================================\n# VISUALIZATION: Spot Overfitting\n# ============================================================================\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 2, figsize=(14, 5))\n\n# Accuracy plot - should see train/val stay close together\nax[0].plot(history.history['accuracy'], label='Train Acc', linewidth=2)\nax[0].plot(history.history['val_accuracy'], label='Val Acc', linewidth=2)\nax[0].set_title(\"Training vs Validation Accuracy\", fontsize=14, fontweight='bold')\nax[0].set_xlabel(\"Epoch\", fontsize=12)\nax[0].set_ylabel(\"Accuracy\", fontsize=12)\nax[0].legend(fontsize=11)\nax[0].grid(True, alpha=0.3)\n\n# Add annotation if overfitting\nif gap > 0.15:\n    ax[0].text(0.5, 0.5, 'OVERFITTING!', \n               transform=ax[0].transAxes, \n               fontsize=20, color='red', alpha=0.3,\n               ha='center', va='center')\n\n# Loss plot - validation should decrease smoothly\nax[1].plot(history.history['loss'], label='Train Loss', linewidth=2)\nax[1].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\nax[1].set_title(\"Training vs Validation Loss\", fontsize=14, fontweight='bold')\nax[1].set_xlabel(\"Epoch\", fontsize=12)\nax[1].set_ylabel(\"Loss\", fontsize=12)\nax[1].legend(fontsize=11)\nax[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(\"overfitting_check.png\", dpi=300, bbox_inches='tight')\nplt.show()\n\n# ============================================================================\n# DETAILED METRICS\n# ============================================================================\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\n\ny_pred = []\ny_true = []\n\nfor batch_x, batch_y in test_ds:\n    preds = model.predict(batch_x, verbose=0)\n    y_pred.extend(np.argmax(preds, axis=1))\n    y_true.extend(batch_y.numpy())\n\ny_pred = np.array(y_pred)\ny_true = np.array(y_true)\n\nprint(\"\\nPer-Class Performance (find weak classes):\")\nprint(classification_report(y_true, y_pred, target_names=class_names))\n\n# Confusion matrix\ncm = confusion_matrix(y_true, y_pred)\nplt.figure(figsize=(16, 14))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=class_names, yticklabels=class_names)\nplt.title('Confusion Matrix - Generalized Model')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.tight_layout()\nplt.savefig('generalized_confusion_matrix.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n# ============================================================================\n# SAVE BEST MODEL\n# ============================================================================\n\nmodel.save(\"asl_generalized_model.keras\")\nprint(\"\\nâœ… Generalized model saved!\")\n\n# Convert to TFLite\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\ntflite_model = converter.convert()\n\nwith open(\"asl_generalized.tflite\", \"wb\") as f:\n    f.write(tflite_model)\n    \nprint(\"âœ… TFLite model saved!\")\n\n# ============================================================================\n# ADDITIONAL TIPS FOR 100% VAL BUT BAD REAL-WORLD PERFORMANCE\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"WHY 100% VAL BUT FAILS ON NEW IMAGES?\")\nprint(\"=\"*60)\nprint(\"\\nðŸ” Most Common Causes:\")\nprint(\"  1. SAME PERSON in train/val/test\")\nprint(\"     â†’ Model memorizes YOUR hand, not hand shapes\")\nprint(\"\\n  2. SAME BACKGROUND (white wall, desk, etc)\")\nprint(\"     â†’ Model uses background to classify, not hand!\")\nprint(\"\\n  3. SAME LIGHTING/CAMERA\")\nprint(\"     â†’ Model memorizes photo conditions\")\nprint(\"\\n  4. INSUFFICIENT VARIETY\")\nprint(\"     â†’ Need multiple people, skin tones, angles\")\nprint(\"\\nâœ… SOLUTIONS:\")\nprint(\"  1. Collect images from 5+ different people\")\nprint(\"  2. Use varied backgrounds (blur them if possible)\")\nprint(\"  3. Different lighting (indoor, outdoor, bright, dim)\")\nprint(\"  4. Different hand positions/rotations\")\nprint(\"  5. Include hand accessories (rings, watches, sleeves)\")\nprint(\"  6. Use data from multiple cameras/phones\")\nprint(\"\\nðŸ’¡ QUICK TEST:\")\nprint(\"  Take 10 photos of YOUR OWN hand right now\")\nprint(\"  â†’ If model gets <50% accuracy, it's memorizing!\")\nprint(\"=\"*60)\n\n# ============================================================================\n# EMERGENCY FIX: Background Randomization\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"EMERGENCY AUGMENTATION (if can't get new data)\")\nprint(\"=\"*60)\n\n# Add this to your data_augmentation\nextreme_augmentation = tf.keras.Sequential([\n    # Original augmentations\n    tf.keras.layers.RandomRotation(0.3),  # Even more rotation\n    tf.keras.layers.RandomTranslation(0.2, 0.2),\n    tf.keras.layers.RandomZoom(0.3),\n    tf.keras.layers.RandomFlip(\"horizontal\"),\n    \n    # EXTREME lighting changes\n    tf.keras.layers.RandomBrightness(0.4),  # Bigger range\n    tf.keras.layers.RandomContrast(0.5),\n    \n    # Color distortion to ignore background colors\n    # This forces model to focus on hand shape, not colors\n    tf.keras.layers.Lambda(lambda x: tf.image.random_hue(x, 0.2)),\n    tf.keras.layers.Lambda(lambda x: tf.image.random_saturation(x, 0.5, 1.5)),\n])\n\nprint(\"Added EXTREME augmentation to break background memorization\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TRAIN_PATH=\"/kaggle/input/asl-dataset-mine/archive/ASL_Alphabet_Dataset/asl_alphabet_train\"\nTEST_PATH=\"/kaggle/input/asl-dataset-mine/archive/ASL_Alphabet_Dataset/asl_alphabet_test\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reload raw data (0-255 integers)\ntrain_ds = tf.keras.utils.image_dataset_from_directory(\n    TRAIN_PATH,\n    validation_split=0.2,\n    subset=\"training\",\n    seed=42,\n    image_size=(96, 96),\n    batch_size=32,\n    label_mode='int' # IMPORTANT: Returns integers 0, 1, 2...\n)\n\nval_ds = tf.keras.utils.image_dataset_from_directory(\n    TRAIN_PATH,\n    validation_split=0.2,\n    subset=\"validation\",\n    seed=42,\n    image_size=(96, 96),\n    batch_size=32,\n    label_mode='int'\n)\n\n# Only performance optimizations, NO NORMALIZATION MAPPING\ntrain_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n\nprint(\"âœ… Data reloaded. Pixel values should be 0-255.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Sequential([\n    Input(shape=(96, 96, 3)),\n    \n    # NORMALIZATION HAPPENS HERE NOW\n    # This turns 0-255 inputs into 0-1\n    tf.keras.layers.Rescaling(1./255),\n    \n    # Simple CNN Architecture\n    Conv2D(32, (3,3), activation='relu', padding='same'),\n    MaxPooling2D(2,2),\n    \n    Conv2D(64, (3,3), activation='relu', padding='same'),\n    MaxPooling2D(2,2),\n    \n    Flatten(),\n    Dense(64, activation='relu'),\n    \n    # Output Layer\n    Dense(num_classes, activation='softmax')\n])\n\n# Use standard learning rate\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n    loss='sparse_categorical_crossentropy', \n    metrics=['accuracy']\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Take 1 batch\nbatch_X, batch_y = next(iter(train_ds))\n\nprint(\"Training on single batch...\")\n# Train 20 times on the SAME 32 images\nhistory = model.fit(batch_X, batch_y, epochs=20, verbose=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TRAIN_PATH=\"/kaggle/input/asl-dataset-mine/archive/ASL_Alphabet_Dataset/asl_alphabet_train\"\nTEST_PATH=\"/kaggle/input/asl-dataset-mine/archive/ASL_Alphabet_Dataset/asl_alphabet_test\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T16:01:15.367033Z","iopub.execute_input":"2025-12-15T16:01:15.367694Z","iopub.status.idle":"2025-12-15T16:01:15.376000Z","shell.execute_reply.started":"2025-12-15T16:01:15.367663Z","shell.execute_reply":"2025-12-15T16:01:15.374797Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"TRAIN_PATH=\"/kaggle/input/asl-updated-data/archive(1)/archive/ASL_Alphabet_Dataset/asl_alphabet_train\"\nTEST_PATH=\"/kaggle/input/asl-updated-data/archive(1)/archive/ASL_Alphabet_Dataset/asl_alphabet_test\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T16:02:07.711865Z","iopub.execute_input":"2025-12-15T16:02:07.712529Z","iopub.status.idle":"2025-12-15T16:02:07.717407Z","shell.execute_reply.started":"2025-12-15T16:02:07.712458Z","shell.execute_reply":"2025-12-15T16:02:07.716398Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import (\n    Conv2D, MaxPooling2D, Flatten, Dense, Dropout, \n    BatchNormalization, GlobalAveragePooling2D, Input\n)\nfrom tensorflow.keras.applications import MobileNetV2, EfficientNetB0\nfrom tensorflow.keras.callbacks import (\n    EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n)\nfrom tensorflow.keras.regularizers import l2\nimport numpy as np\n\n# ============================================================================\n# MEMORY OPTIMIZATION SETTINGS\n# ============================================================================\n\n# Clear any previous models from memory\ntf.keras.backend.clear_session()\n\n# Enable memory growth for GPU (prevents allocation of all GPU memory at once)\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"âœ… GPU memory growth enabled for {len(gpus)} GPU(s)\")\n    except RuntimeError as e:\n        print(f\"âš ï¸  {e}\")\n\n# Set TensorFlow to use less memory\nimport os\nos.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"MEMORY-EFFICIENT CONFIGURATION\")\nprint(\"=\"*60)\n\n# ============================================================================\n# ANTI-OVERFITTING CONFIGURATION\n# ============================================================================\n\n# TRAIN_PATH = \"/kaggle/input/american-sign-language/ASL_Dataset/Train\"\n# TEST_PATH = \"/kaggle/input/american-sign-language/ASL_Dataset/Test\"\n\nbatch_size = 32  # REDUCED from 32 - uses half the memory!\nimg_size = (96, 96)  # REDUCED from 128 - 30% less memory per image!\n\n# ============================================================================\n# MODERATE DATA AUGMENTATION (was too aggressive!)\n# ============================================================================\n\ndata_augmentation = tf.keras.Sequential([\n    # Moderate geometric transformations\n    tf.keras.layers.RandomRotation(0.15),  # Reduced from 0.2\n    tf.keras.layers.RandomTranslation(0.1, 0.1),  # Reduced from 0.15\n    tf.keras.layers.RandomZoom(0.15),  # Reduced from 0.2\n    tf.keras.layers.RandomFlip(\"horizontal\"),\n    \n    # Moderate lighting variations\n    tf.keras.layers.RandomBrightness(0.2, value_range=(0, 1)),  # SPECIFY RANGE!\n    tf.keras.layers.RandomContrast(0.2),  # Reduced from 0.3\n])\n\nnormalization = tf.keras.layers.Rescaling(1./255)\n\n# ============================================================================\n# LOAD DATA - MEMORY EFFICIENT VERSION\n# ============================================================================\n\n# CRITICAL: Don't load all data at once!\n# Use generators to load batches on-the-fly\n\ntrain_ds = tf.keras.utils.image_dataset_from_directory(\n    TRAIN_PATH,\n    validation_split=0.2,\n    subset=\"training\",\n    seed=42,\n    image_size=img_size,\n    batch_size=batch_size,\n    shuffle=True,\n    label_mode='int'\n)\n\nval_ds = tf.keras.utils.image_dataset_from_directory(\n    TRAIN_PATH,\n    validation_split=0.2,\n    subset=\"validation\",\n    seed=42,\n    image_size=img_size,\n    batch_size=batch_size,\n    shuffle=False,\n    label_mode='int'\n)\n\ntest_ds = tf.keras.utils.image_dataset_from_directory(\n    TEST_PATH,\n    seed=42,\n    image_size=img_size,\n    batch_size=batch_size,\n    shuffle=False\n)\n\nclass_names = train_ds.class_names\nnum_classes = len(class_names)\nprint(f\"Number of classes: {num_classes}\")\nprint(f\"Classes: {class_names}\")\n\n# Apply augmentation - NO CACHING (saves memory!)\ndef prepare_dataset(ds, augment=False):\n    # Normalize first\n    ds = ds.map(lambda x, y: (normalization(x), y), \n                num_parallel_calls=tf.data.AUTOTUNE)\n    \n    # Augment if training\n    if augment:\n        ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y),\n                    num_parallel_calls=tf.data.AUTOTUNE)\n    \n    # CRITICAL: Only prefetch, NO cache() to save memory\n    return ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n\ntrain_ds = prepare_dataset(train_ds, augment=True)\nval_ds = prepare_dataset(val_ds, augment=False)\n# test_ds = prepare_dataset(test_ds, augment=False)\n\nprint(\"âœ… Data loaded WITHOUT caching (memory efficient!)\")\n\n# ============================================================================\n# VERIFY DATA IS PROPERLY NORMALIZED\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"VERIFYING DATA PREPROCESSING\")\nprint(\"=\"*60)\n\nfor images, labels in train_ds.take(1):\n    print(f\"Batch shape: {images.shape}\")\n    print(f\"Image dtype: {images.dtype}\")\n    print(f\"Min value: {tf.reduce_min(images).numpy():.3f}\")\n    print(f\"Max value: {tf.reduce_max(images).numpy():.3f}\")\n    print(f\"Mean value: {tf.reduce_mean(images).numpy():.3f}\")\n    \n    if tf.reduce_min(images) >= 0.0 and tf.reduce_max(images) <= 1.1:\n        print(\"âœ… Images properly normalized to [0, 1] range!\")\n    else:\n        print(\"âŒ ERROR: Images NOT normalized! Check preprocessing!\")\n        print(\"   Images should be in range [0, 1]\")\n\nprint(\"=\"*60 + \"\\n\")\n\n# ============================================================================\n# SOLUTION 1: TRANSFER LEARNING (BEST FOR OVERFITTING)\n# ============================================================================\n\ndef create_transfer_learning_model(num_classes, trainable_layers=10):\n    \"\"\"\n    MEMORY EFFICIENT: Smaller input + fewer trainable layers\n    \"\"\"\n    \n    # Load pre-trained model\n    base_model = MobileNetV2(\n        input_shape=(96, 96, 3),  # Smaller input size\n        include_top=False,\n        weights='imagenet'\n    )\n    \n    # Freeze MORE layers (less memory during training)\n    base_model.trainable = True\n    for layer in base_model.layers[:-trainable_layers]:  # Only train last 10 layers\n        layer.trainable = False\n    \n    print(f\"âœ… MobileNetV2: Only {trainable_layers} layers trainable (memory efficient)\")\n    \n    model = Sequential([\n        base_model,\n        GlobalAveragePooling2D(),\n        \n        # SMALLER dense layers\n        Dense(128, activation='relu', kernel_regularizer=l2(0.001)),  # Reduced from 256\n        Dropout(0.5),\n        \n        Dense(64, activation='relu', kernel_regularizer=l2(0.001)),  # Reduced from 128\n        Dropout(0.4),\n        \n        Dense(num_classes, activation='softmax')\n    ])\n    \n    return model\n\n# ============================================================================\n# SOLUTION 2: LIGHTWEIGHT CNN WITH HEAVY REGULARIZATION\n# ============================================================================\n\ndef create_regularized_cnn(num_classes):\n    \"\"\"\n    MEMORY EFFICIENT: Smaller, simpler CNN\n    \"\"\"\n    \n    model = Sequential([\n        # Block 1\n        Conv2D(32, (3,3), activation='relu', padding='same', \n               kernel_regularizer=l2(0.001), input_shape=(96,96,3)),  # 96x96 input\n        BatchNormalization(),\n        MaxPooling2D(2,2),\n        Dropout(0.3),\n        \n        # Block 2 - SMALLER filters\n        Conv2D(64, (3,3), activation='relu', padding='same', \n               kernel_regularizer=l2(0.001)),\n        BatchNormalization(),\n        MaxPooling2D(2,2),\n        Dropout(0.3),\n        \n        # Block 3 - Removed 4th block to save memory\n        Conv2D(128, (3,3), activation='relu', padding='same', \n               kernel_regularizer=l2(0.001)),\n        BatchNormalization(),\n        MaxPooling2D(2,2),\n        Dropout(0.4),\n        \n        # Smaller classification head\n        GlobalAveragePooling2D(),\n        \n        Dense(128, activation='relu', kernel_regularizer=l2(0.001)),  # Reduced from 256\n        Dropout(0.5),\n        \n        Dense(num_classes, activation='softmax')\n    ])\n    \n    return model\n\n# ============================================================================\n# SOLUTION 3: TINY MODEL (Harder to Overfit)\n# ============================================================================\n\ndef create_tiny_model(num_classes):\n    \"\"\"\n    ULTRA LIGHTWEIGHT - use if still crashing\n    \"\"\"\n    \n    model = Sequential([\n        Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(96,96,3)),\n        MaxPooling2D(2,2),\n        Dropout(0.25),\n        \n        Conv2D(64, (3,3), activation='relu', padding='same'),\n        MaxPooling2D(2,2),\n        Dropout(0.25),\n        \n        GlobalAveragePooling2D(),\n        \n        Dense(64, activation='relu'),  # Very small\n        Dropout(0.5),\n        \n        Dense(num_classes, activation='softmax')\n    ])\n    \n    return model\n\n# ============================================================================\n# CREATE MODEL (Choose based on memory)\n# ============================================================================\n\n# Start with regularized CNN (better than tiny for learning)\nmodel = create_regularized_cnn(num_classes)\n\n# If this works well, try transfer learning for even better results\n# model = create_transfer_learning_model(num_classes, trainable_layers=10)\n\n# Only use tiny if you're still having memory issues\n# model = create_tiny_model(num_classes)\n\nprint(f\"\\nâœ… Model created with {model.count_params():,} parameters\")\n\n# ============================================================================\n# COMPILE WITH LOWER LEARNING RATE (Prevents Overfitting)\n# ============================================================================\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),  # INCREASED from 0.0001!\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel.summary()\n\n# ============================================================================\n# ANTI-OVERFITTING CALLBACKS\n# ============================================================================\n\n# Stop early when validation stops improving\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy',  # Watch validation, not training\n    patience=10,  # Stop if no improvement for 10 epochs\n    restore_best_weights=True,\n    verbose=1,\n    mode='max'\n)\n\n# Save best model based on VALIDATION accuracy\nmodel_checkpoint = ModelCheckpoint(\n    'best_generalized_model.keras',\n    monitor='val_accuracy',\n    save_best_only=True,\n    mode='max',\n    verbose=1\n)\n\n# Reduce learning rate when stuck\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5,\n    patience=5,\n    min_lr=1e-7,\n    verbose=1\n)\n\n# ============================================================================\n# TRAINING WITH VALIDATION MONITORING\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING WITH ANTI-OVERFITTING STRATEGIES\")\nprint(\"=\"*60)\n\nhistory = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=50,\n    # Removed steps_per_epoch limit - let it train on full data\n    callbacks=[early_stopping, model_checkpoint, reduce_lr],\n    verbose=1\n)\n\n# ============================================================================\n# DETECT OVERFITTING IN RESULTS\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"OVERFITTING CHECK\")\nprint(\"=\"*60)\n\nfinal_train_acc = history.history['accuracy'][-1]\nfinal_val_acc = history.history['val_accuracy'][-1]\ngap = final_train_acc - final_val_acc\n\nprint(f\"Final Training Accuracy:   {final_train_acc*100:.2f}%\")\nprint(f\"Final Validation Accuracy: {final_val_acc*100:.2f}%\")\nprint(f\"Gap (Train - Val):         {gap*100:.2f}%\")\n\nif gap > 0.15:  # More than 15% gap\n    print(\"\\nâš ï¸  HIGH OVERFITTING DETECTED!\")\n    print(\"Solutions:\")\n    print(\"  1. Add MORE data augmentation\")\n    print(\"  2. Use SMALLER model (fewer parameters)\")\n    print(\"  3. Increase Dropout rates\")\n    print(\"  4. Get more training data\")\nelif gap > 0.05:\n    print(\"\\nâš ï¸  Mild overfitting - manageable\")\nelse:\n    print(\"\\nâœ… Good generalization!\")\n\n# ============================================================================\n# TEST ON COMPLETELY UNSEEN DATA (MOST IMPORTANT!)\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"REAL-WORLD TEST (Different people/conditions)\")\nprint(\"=\"*60)\n\ntest_loss, test_accuracy = model.evaluate(test_ds)\n\nprint(f\"\\nTraining Accuracy:   {final_train_acc*100:.2f}%\")\nprint(f\"Validation Accuracy: {final_val_acc*100:.2f}%\")\nprint(f\"TEST Accuracy:       {test_accuracy*100:.2f}%  â† REAL PERFORMANCE\")\nprint(f\"{'='*60}\")\n\n# THIS IS THE NUMBER THAT MATTERS!\nif test_accuracy < 0.7 and final_val_acc > 0.95:\n    print(\"\\nðŸš¨ SEVERE DATA LEAKAGE DETECTED!\")\n    print(\"Val = {:.0f}% but Test = {:.0f}%\".format(final_val_acc*100, test_accuracy*100))\n    print(\"\\nYour validation set is TOO SIMILAR to training!\")\n    print(\"\\nSOLUTIONS:\")\n    print(\"  1. Get test images from DIFFERENT people\")\n    print(\"  2. Use DIFFERENT backgrounds\")\n    print(\"  3. Use DIFFERENT lighting conditions\")\n    print(\"  4. Use DIFFERENT cameras/phones\")\n    print(\"  5. Add natural variations (jewelry, sleeves, skin tones)\")\nelif test_accuracy < 0.85:\n    print(\"\\nâš ï¸  Model struggles with real-world variety\")\n    print(\"Need MORE diverse training data!\")\nelse:\n    print(\"\\nâœ… Good generalization to new data!\")\n\n# ============================================================================\n# ANALYZE WHAT THE MODEL IS ACTUALLY LEARNING\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"CHECKING: Is the model learning HAND SHAPES or BACKGROUND?\")\nprint(\"=\"*60)\n\n# Sample some test predictions to see what's failing\nsample_images = []\nsample_labels = []\nsample_preds = []\n\nfor batch_x, batch_y in test_ds.take(3):\n    preds = model.predict(batch_x, verbose=0)\n    sample_images.extend(batch_x.numpy()[:5])\n    sample_labels.extend(batch_y.numpy()[:5])\n    sample_preds.extend(np.argmax(preds[:5], axis=1))\n\nprint(\"\\nSample Predictions:\")\nfor i in range(min(15, len(sample_labels))):\n    true_label = class_names[sample_labels[i]]\n    pred_label = class_names[sample_preds[i]]\n    status = \"âœ…\" if true_label == pred_label else \"âŒ\"\n    print(f\"  {status} True: {true_label:2s} | Predicted: {pred_label:2s}\")\n\n# ============================================================================\n# VISUALIZATION: Spot Overfitting\n# ============================================================================\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 2, figsize=(14, 5))\n\n# Accuracy plot - should see train/val stay close together\nax[0].plot(history.history['accuracy'], label='Train Acc', linewidth=2)\nax[0].plot(history.history['val_accuracy'], label='Val Acc', linewidth=2)\nax[0].set_title(\"Training vs Validation Accuracy\", fontsize=14, fontweight='bold')\nax[0].set_xlabel(\"Epoch\", fontsize=12)\nax[0].set_ylabel(\"Accuracy\", fontsize=12)\nax[0].legend(fontsize=11)\nax[0].grid(True, alpha=0.3)\n\n# Add annotation if overfitting\nif gap > 0.15:\n    ax[0].text(0.5, 0.5, 'OVERFITTING!', \n               transform=ax[0].transAxes, \n               fontsize=20, color='red', alpha=0.3,\n               ha='center', va='center')\n\n# Loss plot - validation should decrease smoothly\nax[1].plot(history.history['loss'], label='Train Loss', linewidth=2)\nax[1].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\nax[1].set_title(\"Training vs Validation Loss\", fontsize=14, fontweight='bold')\nax[1].set_xlabel(\"Epoch\", fontsize=12)\nax[1].set_ylabel(\"Loss\", fontsize=12)\nax[1].legend(fontsize=11)\nax[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(\"overfitting_check.png\", dpi=300, bbox_inches='tight')\nplt.show()\n\n# ============================================================================\n# DETAILED METRICS\n# ============================================================================\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\n\ny_pred = []\ny_true = []\n\nfor batch_x, batch_y in test_ds:\n    preds = model.predict(batch_x, verbose=0)\n    y_pred.extend(np.argmax(preds, axis=1))\n    y_true.extend(batch_y.numpy())\n\ny_pred = np.array(y_pred)\ny_true = np.array(y_true)\n\nprint(\"\\nPer-Class Performance (find weak classes):\")\nprint(classification_report(y_true, y_pred, target_names=class_names))\n\n# Confusion matrix\ncm = confusion_matrix(y_true, y_pred)\nplt.figure(figsize=(16, 14))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=class_names, yticklabels=class_names)\nplt.title('Confusion Matrix - Generalized Model')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.tight_layout()\nplt.savefig('generalized_confusion_matrix.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n# ============================================================================\n# SAVE BEST MODEL\n# ============================================================================\n\nmodel.save(\"asl_generalized_model_1.keras\")\nprint(\"\\nâœ… Generalized model saved!\")\n\n# Convert to TFLite\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\ntflite_model = converter.convert()\n\nwith open(\"asl_generalized.tflite\", \"wb\") as f:\n    f.write(tflite_model)\n    \nprint(\"âœ… TFLite model saved!\")\n\n# ============================================================================\n# ADDITIONAL TIPS FOR 100% VAL BUT BAD REAL-WORLD PERFORMANCE\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"WHY 100% VAL BUT FAILS ON NEW IMAGES?\")\nprint(\"=\"*60)\nprint(\"\\nðŸ” Most Common Causes:\")\nprint(\"  1. SAME PERSON in train/val/test\")\nprint(\"     â†’ Model memorizes YOUR hand, not hand shapes\")\nprint(\"\\n  2. SAME BACKGROUND (white wall, desk, etc)\")\nprint(\"     â†’ Model uses background to classify, not hand!\")\nprint(\"\\n  3. SAME LIGHTING/CAMERA\")\nprint(\"     â†’ Model memorizes photo conditions\")\nprint(\"\\n  4. INSUFFICIENT VARIETY\")\nprint(\"     â†’ Need multiple people, skin tones, angles\")\nprint(\"\\nâœ… SOLUTIONS:\")\nprint(\"  1. Collect images from 5+ different people\")\nprint(\"  2. Use varied backgrounds (blur them if possible)\")\nprint(\"  3. Different lighting (indoor, outdoor, bright, dim)\")\nprint(\"  4. Different hand positions/rotations\")\nprint(\"  5. Include hand accessories (rings, watches, sleeves)\")\nprint(\"  6. Use data from multiple cameras/phones\")\nprint(\"\\nðŸ’¡ QUICK TEST:\")\nprint(\"  Take 10 photos of YOUR OWN hand right now\")\nprint(\"  â†’ If model gets <50% accuracy, it's memorizing!\")\nprint(\"=\"*60)\n\n# ============================================================================\n# EMERGENCY FIX: Background Randomization\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"EMERGENCY AUGMENTATION (if can't get new data)\")\nprint(\"=\"*60)\n\n# Add this to your data_augmentation\nextreme_augmentation = tf.keras.Sequential([\n    # Original augmentations\n    tf.keras.layers.RandomRotation(0.3),  # Even more rotation\n    tf.keras.layers.RandomTranslation(0.2, 0.2),\n    tf.keras.layers.RandomZoom(0.3),\n    tf.keras.layers.RandomFlip(\"horizontal\"),\n    \n    # EXTREME lighting changes\n    tf.keras.layers.RandomBrightness(0.4),  # Bigger range\n    tf.keras.layers.RandomContrast(0.5),\n    \n    # Color distortion to ignore background colors\n    # This forces model to focus on hand shape, not colors\n    tf.keras.layers.Lambda(lambda x: tf.image.random_hue(x, 0.2)),\n    tf.keras.layers.Lambda(lambda x: tf.image.random_saturation(x, 0.5, 1.5)),\n])\n\nprint(\"Added EXTREME augmentation to break background memorization\")","metadata":{"trusted":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2025-12-15T16:02:08.303199Z","iopub.execute_input":"2025-12-15T16:02:08.303734Z"}},"outputs":[{"name":"stdout","text":"âœ… GPU memory growth enabled for 2 GPU(s)\n\n============================================================\nMEMORY-EFFICIENT CONFIGURATION\n============================================================\nFound 223325 files belonging to 29 classes.\nUsing 178660 files for training.\nFound 223325 files belonging to 29 classes.\nUsing 44665 files for validation.\nFound 28 files belonging to 28 classes.\nNumber of classes: 29\nClasses: ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\nâœ… Data loaded WITHOUT caching (memory efficient!)\n\n============================================================\nVERIFYING DATA PREPROCESSING\n============================================================\nBatch shape: (32, 96, 96, 3)\nImage dtype: <dtype: 'float32'>\nMin value: 0.000\nMax value: 1.087\nMean value: 0.528\nâœ… Images properly normalized to [0, 1] range!\n============================================================\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\nâœ… Model created with 114,397 parameters\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚           \u001b[38;5;34m896\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚           \u001b[38;5;34m128\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚        \u001b[38;5;34m18,496\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_1           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚           \u001b[38;5;34m256\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m128\u001b[0m)    â”‚        \u001b[38;5;34m73,856\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_2           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m128\u001b[0m)    â”‚           \u001b[38;5;34m512\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ global_average_pooling2d        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚        \u001b[38;5;34m16,512\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m)             â”‚         \u001b[38;5;34m3,741\u001b[0m â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_1           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_2           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ global_average_pooling2d        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,741</span> â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m114,397\u001b[0m (446.86 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">114,397</span> (446.86 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m113,949\u001b[0m (445.11 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">113,949</span> (445.11 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"\n============================================================\nTRAINING WITH ANTI-OVERFITTING STRATEGIES\n============================================================\nEpoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1765814589.004721     772 service.cc:148] XLA service 0x7cfba8018e00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1765814589.004760     772 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1765814589.004765     772 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1765814589.664289     772 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m   4/5584\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m1:59\u001b[0m 21ms/step - accuracy: 0.0195 - loss: 4.2645       ","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1765814596.863821     772 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2711/5584\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m2:34\u001b[0m 54ms/step - accuracy: 0.0756 - loss: 3.4082","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"TRAIN_PATH=\"/kaggle/input/asl-dataset-mine/archive/ASL_Alphabet_Dataset/asl_alphabet_train\"\nTEST_PATH=\"/kaggle/input/asl-dataset-mine/archive/ASL_Alphabet_Dataset/asl_alphabet_test\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import (\n    Conv2D, MaxPooling2D, Flatten, Dense, Dropout, \n    BatchNormalization, GlobalAveragePooling2D, Input\n)\nfrom tensorflow.keras.applications import MobileNetV2, EfficientNetB0\nfrom tensorflow.keras.callbacks import (\n    EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n)\nfrom tensorflow.keras.regularizers import l2\nimport numpy as np\n\n# ============================================================================\n# MEMORY OPTIMIZATION SETTINGS\n# ============================================================================\n\ntf.keras.backend.clear_session()\n\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"âœ… GPU memory growth enabled for {len(gpus)} GPU(s)\")\n    except RuntimeError as e:\n        print(f\"âš ï¸  {e}\")\n\nimport os\nos.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\nos.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'  # Better GPU threading\nos.environ['TF_GPU_THREAD_COUNT'] = '2'\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"MEMORY-EFFICIENT CONFIGURATION + PIPELINE FIX\")\nprint(\"=\"*60)\n\n# ============================================================================\n# CRITICAL: REDUCED BATCH SIZE & OPTIMIZED SETTINGS\n# ============================================================================\n\nbatch_size = 8  # REDUCED from 32 - less I/O pressure\nimg_size = (96, 96)\n\n# ============================================================================\n# SIMPLIFIED AUGMENTATION (Less CPU overhead)\n# ============================================================================\n\n# CRITICAL: Make augmentation MUCH simpler to reduce CPU load\ndata_augmentation = tf.keras.Sequential([\n    tf.keras.layers.RandomRotation(0.15),\n    tf.keras.layers.RandomTranslation(0.1, 0.1),\n    tf.keras.layers.RandomZoom(0.15),\n    tf.keras.layers.RandomFlip(\"horizontal\"),\n])\n\nnormalization = tf.keras.layers.Rescaling(1./255)\n\n# ============================================================================\n# OPTIMIZED DATA LOADING - NO MORE STALLS!\n# ============================================================================\n\nprint(\"Loading datasets with optimized pipeline...\")\n\n# CRITICAL: Use more parallel calls and better caching strategy\nAUTOTUNE = tf.data.AUTOTUNE\n\ntrain_ds = tf.keras.utils.image_dataset_from_directory(\n    TRAIN_PATH,\n    validation_split=0.2,\n    subset=\"training\",\n    seed=42,\n    image_size=img_size,\n    batch_size=batch_size,\n    shuffle=True,\n    label_mode='int'\n)\n\nval_ds = tf.keras.utils.image_dataset_from_directory(\n    TRAIN_PATH,\n    validation_split=0.2,\n    subset=\"validation\",\n    seed=42,\n    image_size=img_size,\n    batch_size=batch_size,\n    shuffle=False,\n    label_mode='int'\n)\n\ntest_ds = tf.keras.utils.image_dataset_from_directory(\n    TEST_PATH,\n    seed=42,\n    image_size=img_size,\n    batch_size=batch_size,\n    shuffle=False\n)\n\nclass_names = train_ds.class_names\nnum_classes = len(class_names)\nprint(f\"Number of classes: {num_classes}\")\nprint(f\"Classes: {class_names}\")\n\n# ============================================================================\n# CRITICAL FIX: OPTIMIZED PIPELINE TO PREVENT STALLING\n# ============================================================================\n\ndef prepare_dataset(ds, augment=False, cache_data=False):\n    \"\"\"\n    OPTIMIZED: Better ordering and parallelization\n    \"\"\"\n    # Step 1: Cache BEFORE augmentation (if dataset is small enough)\n    if cache_data:\n        ds = ds.cache()  # Cache the raw images\n        print(\"âœ… Dataset cached in memory\")\n    \n    # Step 2: Normalize (fast operation)\n    ds = ds.map(lambda x, y: (normalization(x), y), \n                num_parallel_calls=AUTOTUNE)\n    \n    # Step 3: Augment if training (CPU-intensive, so parallelize heavily)\n    if augment:\n        ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y),\n                    num_parallel_calls=AUTOTUNE,\n                    deterministic=False)  # Allow parallel non-deterministic processing\n    \n    # Step 4: CRITICAL - Large prefetch buffer\n    ds = ds.prefetch(buffer_size=AUTOTUNE)\n    \n    return ds\n\n# Determine if we can cache (check dataset size)\n# If your dataset is < 5GB, caching helps A LOT\ndataset_size_estimate = len(list(train_ds)) * batch_size * 96 * 96 * 3 * 4 / (1024**3)\nprint(f\"Estimated train dataset size: {dataset_size_estimate:.2f} GB\")\n\ncan_cache = dataset_size_estimate < 3  # Only cache if < 3GB\n\nif can_cache:\n    print(\"âœ… Dataset small enough - will cache for speed\")\nelse:\n    print(\"âš ï¸ Dataset too large to cache - using streaming mode\")\n\ntrain_ds = prepare_dataset(train_ds, augment=True, cache_data=can_cache)\nval_ds = prepare_dataset(val_ds, augment=False, cache_data=can_cache)\ntest_ds = prepare_dataset(test_ds, augment=False, cache_data=False)\n\nprint(\"âœ… Optimized data pipeline configured!\")\n\n# ============================================================================\n# VERIFY DATA PIPELINE IS WORKING\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TESTING DATA PIPELINE (checking for stalls)\")\nprint(\"=\"*60)\n\nimport time\n\n# Test loading speed\nstart = time.time()\nbatch_count = 0\nfor images, labels in train_ds.take(50):  # Test 50 batches\n    batch_count += 1\n    if batch_count % 10 == 0:\n        print(f\"  Loaded {batch_count} batches... ({time.time() - start:.2f}s)\")\n\nelapsed = time.time() - start\nbatches_per_sec = batch_count / elapsed\n\nprint(f\"\\nâœ… Pipeline test complete:\")\nprint(f\"   {batch_count} batches in {elapsed:.2f}s\")\nprint(f\"   Speed: {batches_per_sec:.2f} batches/sec\")\n\nif batches_per_sec < 1.0:\n    print(\"\\nâš ï¸ WARNING: Pipeline is slow! This will cause stalls.\")\n    print(\"   Solutions:\")\n    print(\"   1. Reduce batch_size further (try 8)\")\n    print(\"   2. Simplify augmentation even more\")\n    print(\"   3. Use smaller image size (64x64)\")\nelse:\n    print(\"\\nâœ… Pipeline speed looks good!\")\n\nprint(\"=\"*60 + \"\\n\")\n\n# ============================================================================\n# MODEL CREATION\n# ============================================================================\n\ndef create_regularized_cnn(num_classes):\n    model = Sequential([\n        Conv2D(32, (3,3), activation='relu', padding='same', \n               kernel_regularizer=l2(0.001), input_shape=(96,96,3)),\n        BatchNormalization(),\n        MaxPooling2D(2,2),\n        Dropout(0.3),\n        \n        Conv2D(64, (3,3), activation='relu', padding='same', \n               kernel_regularizer=l2(0.001)),\n        BatchNormalization(),\n        MaxPooling2D(2,2),\n        Dropout(0.3),\n        \n        Conv2D(128, (3,3), activation='relu', padding='same', \n               kernel_regularizer=l2(0.001)),\n        BatchNormalization(),\n        MaxPooling2D(2,2),\n        Dropout(0.4),\n        \n        GlobalAveragePooling2D(),\n        \n        Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n        Dropout(0.5),\n        \n        Dense(num_classes, activation='softmax')\n    ])\n    \n    return model\n\nmodel = create_regularized_cnn(num_classes)\nprint(f\"\\nâœ… Model created with {model.count_params():,} parameters\")\n\n# ============================================================================\n# COMPILE\n# ============================================================================\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel.summary()\n\n# ============================================================================\n# CALLBACKS WITH PROGRESS MONITORING\n# ============================================================================\n\nclass StallDetector(tf.keras.callbacks.Callback):\n    \"\"\"Detect if training is stalling\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.last_batch_time = time.time()\n        self.batch_times = []\n    \n    def on_batch_begin(self, batch, logs=None):\n        self.last_batch_time = time.time()\n    \n    def on_batch_end(self, batch, logs=None):\n        batch_time = time.time() - self.last_batch_time\n        self.batch_times.append(batch_time)\n        \n        # Alert if batch takes too long\n        if batch_time > 10.0:  # More than 10 seconds per batch\n            print(f\"\\nâš ï¸ SLOW BATCH DETECTED: {batch_time:.1f}s for batch {batch}\")\n    \n    def on_epoch_end(self, epoch, logs=None):\n        if len(self.batch_times) > 0:\n            avg_time = np.mean(self.batch_times)\n            print(f\"\\nðŸ“Š Avg batch time: {avg_time:.3f}s\")\n            self.batch_times = []  # Reset for next epoch\n\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy',\n    patience=15,\n    restore_best_weights=True,\n    verbose=1,\n    mode='max'\n)\n\nmodel_checkpoint = ModelCheckpoint(\n    'best_generalized_model.keras',\n    monitor='val_accuracy',\n    save_best_only=True,\n    mode='max',\n    verbose=1\n)\n\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5,\n    patience=5,\n    min_lr=1e-6,\n    verbose=1\n)\n\nstall_detector = StallDetector()\n\n# ============================================================================\n# TRAINING WITH STALL DETECTION\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING WITH STALL DETECTION\")\nprint(\"=\"*60)\n\ntry:\n    history = model.fit(\n        train_ds,\n        validation_data=val_ds,\n        epochs=50,\n        callbacks=[early_stopping, model_checkpoint, reduce_lr, stall_detector],\n        verbose=1,\n        workers=4,  # Parallel data loading\n        use_multiprocessing=False  # Set to False on Kaggle to avoid issues\n    )\nexcept KeyboardInterrupt:\n    print(\"\\nâš ï¸ Training interrupted by user\")\nexcept Exception as e:\n    print(f\"\\nâŒ Training failed: {e}\")\n    raise\n\n# ============================================================================\n# RESULTS\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING COMPLETE\")\nprint(\"=\"*60)\n\nfinal_train_acc = history.history['accuracy'][-1]\nfinal_val_acc = history.history['val_accuracy'][-1]\ngap = final_train_acc - final_val_acc\n\nprint(f\"Final Training Accuracy:   {final_train_acc*100:.2f}%\")\nprint(f\"Final Validation Accuracy: {final_val_acc*100:.2f}%\")\nprint(f\"Gap (Train - Val):         {gap*100:.2f}%\")\n\n# Test evaluation\ntest_loss, test_accuracy = model.evaluate(test_ds)\nprint(f\"Test Accuracy:             {test_accuracy*100:.2f}%\")\n\n# ============================================================================\n# SAVE MODEL\n# ============================================================================\n\nmodel.save(\"asl_generalized_model_fixed.keras\")\nprint(\"\\nâœ… Model saved!\")\n\n# ============================================================================\n# EMERGENCY TROUBLESHOOTING\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"IF STILL EXPERIENCING STALLS:\")\nprint(\"=\"*60)\nprint(\"\\n1. REDUCE BATCH SIZE EVEN MORE:\")\nprint(\"   batch_size = 8  # or even 4\")\nprint(\"\\n2. SIMPLIFY AUGMENTATION:\")\nprint(\"   Remove RandomBrightness and RandomContrast\")\nprint(\"\\n3. REDUCE IMAGE SIZE:\")\nprint(\"   img_size = (64, 64)\")\nprint(\"\\n4. CHECK KAGGLE RESOURCES:\")\nprint(\"   !nvidia-smi  # Check GPU usage\")\nprint(\"   !free -h     # Check RAM usage\")\nprint(\"\\n5. RESTART KERNEL:\")\nprint(\"   Sometimes Kaggle sessions get corrupted\")\nprint(\"=\"*60)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TRAIN_PATH=\"/kaggle/input/asl-dataset-mine/archive/ASL_Alphabet_Dataset/asl_alphabet_train\"\nTEST_PATH=\"/kaggle/input/asl-dataset-mine/archive/ASL_Alphabet_Dataset/asl_alphabet_test\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, GlobalAveragePooling2D\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nimport numpy as np\n\n# ============================================================================\n# NUCLEAR MEMORY FIX - THIS WILL WORK\n# ============================================================================\n\nprint(\"ðŸ”¥ ULTRA-LOW MEMORY MODE ACTIVATED ðŸ”¥\\n\")\n\n# Clear everything\ntf.keras.backend.clear_session()\nimport gc\ngc.collect()\n\n# CRITICAL: Prevent TensorFlow from being greedy\nimport os\nos.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\nos.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'  # Better memory management\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n# GPU settings - CRUCIAL for preventing OOM\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        # Limit GPU memory to 4GB (adjust if needed)\n        tf.config.set_logical_device_configuration(\n            gpus[0],\n            [tf.config.LogicalDeviceConfiguration(memory_limit=4096)]\n        )\n        print(f\"âœ… GPU memory limited to 4GB\")\n    except:\n        # Fallback: just enable memory growth\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"âœ… GPU memory growth enabled\")\n\n# ============================================================================\n# MINIMAL SETTINGS - NO MORE BS\n# ============================================================================\n\nBATCH_SIZE = 8  # TINY batch size\nIMG_SIZE = (64, 64)  # SMALLER images (64x64 instead of 96x96)\nPREFETCH = 2  # Minimal prefetch\n\nprint(f\"Batch size: {BATCH_SIZE}\")\nprint(f\"Image size: {IMG_SIZE}\")\n\n# ============================================================================\n# ULTRA-SIMPLE AUGMENTATION (Almost nothing)\n# ============================================================================\n\n# Just normalization and basic flip - NO heavy augmentation\ndef preprocess_image(image, label, augment=False):\n    # Normalize\n    image = tf.cast(image, tf.float32) / 255.0\n    \n    # Only basic augmentation if training\n    if augment:\n        image = tf.image.random_flip_left_right(image)\n        image = tf.image.random_brightness(image, 0.1)\n    \n    return image, label\n\n# ============================================================================\n# LOAD DATA - STREAMING MODE (NO CACHING!)\n# ============================================================================\n\nprint(\"\\nLoading data in streaming mode (no cache)...\")\n\ntrain_ds = tf.keras.utils.image_dataset_from_directory(\n    TRAIN_PATH,\n    validation_split=0.2,\n    subset=\"training\",\n    seed=42,\n    image_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    label_mode='int'\n)\n\nval_ds = tf.keras.utils.image_dataset_from_directory(\n    TRAIN_PATH,\n    validation_split=0.2,\n    subset=\"validation\",\n    seed=42,\n    image_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    label_mode='int'\n)\n\ntest_ds = tf.keras.utils.image_dataset_from_directory(\n    TEST_PATH,\n    seed=42,\n    image_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)\n\nclass_names = train_ds.class_names\nnum_classes = len(class_names)\nprint(f\"âœ… {num_classes} classes loaded: {class_names[:5]}...\")\n\n# ============================================================================\n# APPLY PREPROCESSING - NO CACHE, MINIMAL PREFETCH\n# ============================================================================\n\n# Apply preprocessing\ntrain_ds = train_ds.map(\n    lambda x, y: preprocess_image(x, y, augment=True),\n    num_parallel_calls=1  # SINGLE thread to save memory\n).prefetch(PREFETCH)\n\nval_ds = val_ds.map(\n    lambda x, y: preprocess_image(x, y, augment=False),\n    num_parallel_calls=1\n).prefetch(PREFETCH)\n\ntest_ds = test_ds.map(\n    lambda x, y: preprocess_image(x, y, augment=False),\n    num_parallel_calls=1\n).prefetch(PREFETCH)\n\nprint(\"âœ… Data pipeline ready (minimal memory)\\n\")\n\n# ============================================================================\n# TINY MODEL - Can't OOM if model is small\n# ============================================================================\n\ndef create_tiny_model(num_classes):\n    \"\"\"\n    Smallest model that still learns\n    ~50K parameters vs millions\n    \"\"\"\n    model = Sequential([\n        # Block 1\n        Conv2D(16, (3,3), activation='relu', padding='same', input_shape=(64,64,3)),\n        MaxPooling2D(2,2),\n        Dropout(0.25),\n        \n        # Block 2\n        Conv2D(32, (3,3), activation='relu', padding='same'),\n        MaxPooling2D(2,2),\n        Dropout(0.25),\n        \n        # Block 3\n        Conv2D(64, (3,3), activation='relu', padding='same'),\n        MaxPooling2D(2,2),\n        Dropout(0.25),\n        \n        # Classifier\n        GlobalAveragePooling2D(),\n        Dense(64, activation='relu'),\n        Dropout(0.5),\n        Dense(num_classes, activation='softmax')\n    ])\n    \n    return model\n\nmodel = create_tiny_model(num_classes)\nprint(f\"Model parameters: {model.count_params():,}\")\nprint(f\"Estimated model size: {model.count_params() * 4 / 1024 / 1024:.1f} MB\\n\")\n\n# ============================================================================\n# COMPILE\n# ============================================================================\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel.summary()\n\n# ============================================================================\n# CALLBACKS - MINIMAL\n# ============================================================================\n\ncallbacks = [\n    EarlyStopping(\n        monitor='val_accuracy',\n        patience=10,\n        restore_best_weights=True,\n        verbose=1\n    ),\n    ModelCheckpoint(\n        'best_model_tiny.keras',\n        monitor='val_accuracy',\n        save_best_only=True,\n        verbose=1\n    ),\n    ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.5,\n        patience=3,\n        min_lr=1e-6,\n        verbose=1\n    )\n]\n\n# ============================================================================\n# TRAIN WITH MANUAL GARBAGE COLLECTION\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"STARTING TRAINING - MINIMAL MEMORY MODE\")\nprint(\"=\"*60 + \"\\n\")\n\n# Train in smaller chunks if needed\ntry:\n    history = model.fit(\n        train_ds,\n        validation_data=val_ds,\n        epochs=50,\n        callbacks=callbacks,\n        verbose=1\n    )\n    \n    print(\"\\nâœ… TRAINING COMPLETED!\")\n    \nexcept tf.errors.ResourceExhaustedError as e:\n    print(f\"\\nâŒ STILL OUT OF MEMORY!\")\n    print(f\"Error: {e}\")\n    print(\"\\nTRY THESE:\")\n    print(\"1. Restart kernel completely\")\n    print(\"2. Change BATCH_SIZE to 4\")\n    print(\"3. Change IMG_SIZE to (48, 48)\")\n    print(\"4. Use CPU instead: os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\")\n    raise\n\n# ============================================================================\n# EVALUATE\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"EVALUATION\")\nprint(\"=\"*60)\n\nfinal_train_acc = history.history['accuracy'][-1]\nfinal_val_acc = history.history['val_accuracy'][-1]\n\nprint(f\"\\nTraining Accuracy:   {final_train_acc*100:.2f}%\")\nprint(f\"Validation Accuracy: {final_val_acc*100:.2f}%\")\n\ntest_loss, test_accuracy = model.evaluate(test_ds, verbose=0)\nprint(f\"Test Accuracy:       {test_accuracy*100:.2f}%\")\n\n# ============================================================================\n# SAVE\n# ============================================================================\n\nmodel.save(\"asl_tiny_model.keras\")\nprint(\"\\nâœ… Model saved!\")\n\n# Convert to TFLite (MUCH smaller)\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\ntflite_model = converter.convert()\n\nwith open(\"asl_tiny.tflite\", \"wb\") as f:\n    f.write(tflite_model)\n\nprint(f\"âœ… TFLite model saved! ({len(tflite_model) / 1024:.1f} KB)\")\n\n# ============================================================================\n# PLOT RESULTS\n# ============================================================================\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Train')\nplt.plot(history.history['val_accuracy'], label='Val')\nplt.title('Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Train')\nplt.plot(history.history['val_loss'], label='Val')\nplt.title('Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('training_results.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"IF THIS STILL FAILS:\")\nprint(\"=\"*60)\nprint(\"\\n1. RESTART KERNEL (Runtime -> Restart Runtime)\")\nprint(\"2. Run ONLY this script (no other code)\")\nprint(\"3. If still fails, add this at the TOP:\")\nprint(\"   os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\")\nprint(\"   (This forces CPU training)\")\nprint(\"=\"*60)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nimport numpy as np\n\n# ============================================================================\n# MEMORY OPTIMIZATION\n# ============================================================================\n\nprint(\"ðŸš€ IMPROVED ASL MODEL - TARGET: 95%+ ACCURACY\\n\")\n\ntf.keras.backend.clear_session()\nimport gc\ngc.collect()\n\nimport os\nos.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\nos.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        tf.config.set_logical_device_configuration(\n            gpus[0],\n            [tf.config.LogicalDeviceConfiguration(memory_limit=4096)]\n        )\n        print(f\"âœ… GPU memory limited to 4GB\")\n    except:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"âœ… GPU memory growth enabled\")\n\n# ============================================================================\n# IMPROVED SETTINGS\n# ============================================================================\n\nBATCH_SIZE = 16  # Slightly larger for stability\nIMG_SIZE = (96, 96)  # Larger image = more detail (changed from 64x64)\nPREFETCH = 3\n\nprint(f\"Batch size: {BATCH_SIZE}\")\nprint(f\"Image size: {IMG_SIZE}\")\n\n# ============================================================================\n# BETTER AUGMENTATION - Key to 95%+\n# ============================================================================\n\n# ============================================================================\n# DIRTY DATA AUGMENTATION (Simulates Webcams)\n# ============================================================================\n\ndef tough_augmentation(x, training=True):\n    if not training:\n        return x\n    \n    # 1. Geometry (Standard)\n    x = tf.image.random_rotation(x, 0.15)  # Less rotation, hands don't spin 360\n    x = tf.image.random_crop(x, [BATCH_SIZE, IMG_SIZE[0], IMG_SIZE[1], 3]) # Simulate imperfect tracking\n    \n    # 2. Lighting/Color (CRITICAL for video)\n    x = tf.image.random_brightness(x, 0.2)\n    x = tf.image.random_contrast(x, 0.8, 1.2)\n    x = tf.image.random_saturation(x, 0.7, 1.3) # Webcams wash out colors\n    \n    # 3. Simulate low quality/Blur (The Secret Sauce)\n    # random_jpeg_quality introduces artifacts common in video streams\n    x = tf.image.random_jpeg_quality(x, 70, 100) \n    \n    return x\n\n# Update prepare function\ndef prepare(ds, shuffle=False, augment=False):\n    if shuffle:\n        ds = ds.shuffle(1000)\n    \n    # Normalize\n    ds = ds.map(lambda x, y: (normalization(x), y))\n    \n    # Use custom augmentation\n    if augment:\n        ds = ds.map(lambda x, y: (tough_augmentation(x, training=True), y))\n    \n    return ds.prefetch(2)\n# ============================================================================\n# LOAD DATA\n# ============================================================================\n\nprint(\"\\nLoading data...\")\n\ntrain_ds = tf.keras.utils.image_dataset_from_directory(\n    TRAIN_PATH,\n    validation_split=0.2,\n    subset=\"training\",\n    seed=42,\n    image_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    label_mode='int'\n)\n\nval_ds = tf.keras.utils.image_dataset_from_directory(\n    TRAIN_PATH,\n    validation_split=0.2,\n    subset=\"validation\",\n    seed=42,\n    image_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    label_mode='int'\n)\n\ntest_ds = tf.keras.utils.image_dataset_from_directory(\n    TEST_PATH,\n    seed=42,\n    image_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)\n\nclass_names = train_ds.class_names\nnum_classes = len(class_names)\nprint(f\"âœ… {num_classes} classes loaded\")\n\n# ============================================================================\n# APPLY PREPROCESSING\n# ============================================================================\n\ntrain_ds = train_ds.map(\n    lambda x, y: preprocess_image(x, y, augment=True),\n    num_parallel_calls=tf.data.AUTOTUNE\n).prefetch(PREFETCH)\n\nval_ds = val_ds.map(\n    lambda x, y: preprocess_image(x, y, augment=False),\n    num_parallel_calls=tf.data.AUTOTUNE\n).prefetch(PREFETCH)\n\ntest_ds = test_ds.map(\n    lambda x, y: preprocess_image(x, y, augment=False),\n    num_parallel_calls=tf.data.AUTOTUNE\n).prefetch(PREFETCH)\n\nprint(\"âœ… Data pipeline ready\\n\")\n\n# ============================================================================\n# IMPROVED MODEL - Deeper but still memory-efficient\n# ============================================================================\n\ndef create_improved_model(num_classes):\n    \"\"\"\n    Better model: deeper + BatchNorm for 95%+ accuracy\n    Still under 500K parameters to avoid memory issues\n    \"\"\"\n    model = Sequential([\n        # Block 1\n        Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(96,96,3)),\n        BatchNormalization(),\n        Conv2D(32, (3,3), activation='relu', padding='same'),\n        BatchNormalization(),\n        MaxPooling2D(2,2),\n        Dropout(0.25),\n        \n        # Block 2\n        Conv2D(64, (3,3), activation='relu', padding='same'),\n        BatchNormalization(),\n        Conv2D(64, (3,3), activation='relu', padding='same'),\n        BatchNormalization(),\n        MaxPooling2D(2,2),\n        Dropout(0.25),\n        \n        # Block 3\n        Conv2D(128, (3,3), activation='relu', padding='same'),\n        BatchNormalization(),\n        Conv2D(128, (3,3), activation='relu', padding='same'),\n        BatchNormalization(),\n        MaxPooling2D(2,2),\n        Dropout(0.3),\n        \n        # Block 4 - Extra depth for complex patterns\n        Conv2D(256, (3,3), activation='relu', padding='same'),\n        BatchNormalization(),\n        MaxPooling2D(2,2),\n        Dropout(0.4),\n        \n        # Classifier\n        GlobalAveragePooling2D(),\n        \n        Dense(256, activation='relu'),\n        BatchNormalization(),\n        Dropout(0.5),\n        \n        Dense(128, activation='relu'),\n        Dropout(0.4),\n        \n        Dense(num_classes, activation='softmax')\n    ])\n    \n    return model\n\nmodel = create_improved_model(num_classes)\nprint(f\"Model parameters: {model.count_params():,}\")\nprint(f\"Estimated size: {model.count_params() * 4 / 1024 / 1024:.1f} MB\\n\")\n\n# ============================================================================\n# COMPILE WITH BETTER OPTIMIZER\n# ============================================================================\n\n# Use AdamW (Adam with weight decay) for better generalization\nmodel.compile(\n    optimizer=tf.keras.optimizers.AdamW(\n        learning_rate=0.001,\n        weight_decay=0.0001  # Helps prevent overfitting\n    ),\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel.summary()\n\n# ============================================================================\n# CALLBACKS - TUNED FOR PERFORMANCE\n# ============================================================================\n\ncallbacks = [\n    EarlyStopping(\n        monitor='val_accuracy',\n        patience=15,  # More patience for better convergence\n        restore_best_weights=True,\n        verbose=1,\n        min_delta=0.001  # Only stop if no 0.1% improvement\n    ),\n    ModelCheckpoint(\n        'best_asl_model.keras',\n        monitor='val_accuracy',\n        save_best_only=True,\n        verbose=1\n    ),\n    ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.5,\n        patience=5,\n        min_lr=1e-7,\n        verbose=1\n    )\n]\n\n# ============================================================================\n# TRAIN\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING - TARGET: 95%+ ACCURACY\")\nprint(\"=\"*60 + \"\\n\")\n\nhistory = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=100,  # More epochs with early stopping\n    callbacks=callbacks,\n    verbose=1\n)\n\nprint(\"\\nâœ… TRAINING COMPLETED!\")\n\n# ============================================================================\n# EVALUATE\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"RESULTS\")\nprint(\"=\"*60)\n\nfinal_train_acc = history.history['accuracy'][-1]\nfinal_val_acc = history.history['val_accuracy'][-1]\ngap = final_train_acc - final_val_acc\n\nprint(f\"\\nTraining Accuracy:   {final_train_acc*100:.2f}%\")\nprint(f\"Validation Accuracy: {final_val_acc*100:.2f}%\")\nprint(f\"Gap (Train - Val):   {gap*100:.2f}%\")\n\ntest_loss, test_accuracy = model.evaluate(test_ds, verbose=0)\nprint(f\"Test Accuracy:       {test_accuracy*100:.2f}%\")\n\nif test_accuracy >= 0.95:\n    print(\"\\nðŸŽ‰ EXCELLENT! 95%+ achieved!\")\nelif test_accuracy >= 0.92:\n    print(\"\\nâœ… Very good! Close to target.\")\nelse:\n    print(\"\\nðŸ“ˆ Good progress! See tips below for 95%+\")\n\n# ============================================================================\n# ANALYZE MISTAKES\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"ANALYZING MISTAKES\")\nprint(\"=\"*60)\n\nfrom sklearn.metrics import classification_report, confusion_matrix\n\ny_pred = []\ny_true = []\n\nfor batch_x, batch_y in test_ds:\n    preds = model.predict(batch_x, verbose=0)\n    y_pred.extend(np.argmax(preds, axis=1))\n    y_true.extend(batch_y.numpy())\n\ny_pred = np.array(y_pred)\ny_true = np.array(y_true)\n\n# Per-class accuracy\nreport = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n\nprint(\"\\nðŸ“Š Per-class Accuracy:\")\nclass_accs = [(name, report[name]['precision']) for name in class_names]\nclass_accs.sort(key=lambda x: x[1])\n\nprint(\"\\nðŸ”´ WORST performing classes (focus on these):\")\nfor name, acc in class_accs[:5]:\n    print(f\"  {name}: {acc*100:.1f}%\")\n\nprint(\"\\nðŸŸ¢ BEST performing classes:\")\nfor name, acc in class_accs[-5:]:\n    print(f\"  {name}: {acc*100:.1f}%\")\n\n# ============================================================================\n# CONFUSION MATRIX\n# ============================================================================\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ncm = confusion_matrix(y_true, y_pred)\n\nplt.figure(figsize=(20, 18))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=class_names, yticklabels=class_names,\n            cbar_kws={'label': 'Count'})\nplt.title('Confusion Matrix - Find Similar Signs That Get Confused', fontsize=16)\nplt.ylabel('True Label', fontsize=14)\nplt.xlabel('Predicted Label', fontsize=14)\nplt.tight_layout()\nplt.savefig('confusion_matrix_improved.png', dpi=200, bbox_inches='tight')\nplt.show()\n\n# ============================================================================\n# TRAINING HISTORY\n# ============================================================================\n\nfig, ax = plt.subplots(1, 2, figsize=(14, 5))\n\nax[0].plot(history.history['accuracy'], label='Train', linewidth=2)\nax[0].plot(history.history['val_accuracy'], label='Val', linewidth=2)\nax[0].set_title(\"Accuracy Over Time\", fontsize=14, fontweight='bold')\nax[0].set_xlabel(\"Epoch\")\nax[0].set_ylabel(\"Accuracy\")\nax[0].legend()\nax[0].grid(True, alpha=0.3)\n\nax[1].plot(history.history['loss'], label='Train', linewidth=2)\nax[1].plot(history.history['val_loss'], label='Val', linewidth=2)\nax[1].set_title(\"Loss Over Time\", fontsize=14, fontweight='bold')\nax[1].set_xlabel(\"Epoch\")\nax[1].set_ylabel(\"Loss\")\nax[1].legend()\nax[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('training_history_improved.png', dpi=200, bbox_inches='tight')\nplt.show()\n\n# ============================================================================\n# SAVE MODEL\n# ============================================================================\n\nmodel.save(\"asl_improved_model.keras\")\nprint(\"\\nâœ… Model saved!\")\n\n# TFLite conversion\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\ntflite_model = converter.convert()\n\nwith open(\"asl_improved.tflite\", \"wb\") as f:\n    f.write(tflite_model)\n\nprint(f\"âœ… TFLite model saved! ({len(tflite_model) / 1024:.1f} KB)\")\n\n# ============================================================================\n# TIPS TO REACH 95%+\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"HOW TO REACH 95%+ ACCURACY\")\nprint(\"=\"*60)\n\nprint(\"\\nðŸŽ¯ If you're at 89-92%:\")\nprint(\"  1. âœ… DONE: Better model architecture (this script)\")\nprint(\"  2. âœ… DONE: Stronger augmentation\")\nprint(\"  3. âœ… DONE: BatchNormalization for stability\")\n\nprint(\"\\nðŸŽ¯ To reach 95%+, you need:\")\nprint(\"\\n  ðŸ“¸ MORE DATA:\")\nprint(\"     - Collect 2x more images per class\")\nprint(\"     - Get images from 5+ different people\")\nprint(\"     - Use different backgrounds/lighting\")\n\nprint(\"\\n  ðŸ” FIX CONFUSING CLASSES:\")\nprint(\"     - Look at confusion matrix above\")\nprint(\"     - Similar signs (M/N, K/V) need MORE examples\")\nprint(\"     - Add 2x data for the worst 5 classes\")\n\nprint(\"\\n  ðŸ—ï¸ TRY TRANSFER LEARNING:\")\nprint(\"     - Use pre-trained MobileNetV2 (better features)\")\nprint(\"     - Will get you to 95%+ with same data\")\n\nprint(\"\\n  âš¡ ADVANCED TECHNIQUES:\")\nprint(\"     - Mixup augmentation (blend two images)\")\nprint(\"     - Test-time augmentation (predict on 5 augmented versions)\")\nprint(\"     - Ensemble (combine 3 models)\")\n\nprint(\"\\nðŸ’¡ QUICK WIN:\")\nprint(\"   Run this script 3 times, save each model\")\nprint(\"   Average their predictions (ensemble)\")\nprint(\"   +2-3% accuracy boost for free!\")\n\nprint(\"=\"*60)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TRAIN_PATH=\"/kaggle/input/asl-updated-data/archive(1)/archive/ASL_Alphabet_Dataset/asl_alphabet_train\"\nTEST_PATH=\"/kaggle/input/asl-updated-data/archive(1)/archive/ASL_Alphabet_Dataset/asl_alphabet_test\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nimport numpy as np\nimport time\n\n# ============================================================================\n# NUCLEAR MODE - NO MORE BULLSHIT\n# ============================================================================\n\nprint(\"ðŸ’¥ NUCLEAR MODE - FAST & STABLE ðŸ’¥\\n\")\n\n# RESTART EVERYTHING\ntf.keras.backend.clear_session()\nimport gc\ngc.collect()\n\n# FORCE STABILITY\nimport os\nos.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\nos.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n# Limit GPU to prevent OOM\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        tf.config.set_logical_device_configuration(\n            gpus[0],\n            [tf.config.LogicalDeviceConfiguration(memory_limit=3072)]  # Only 3GB\n        )\n        print(\"âœ… GPU limited to 3GB (stable)\")\n    except:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(\"âœ… Memory growth enabled\")\n\n# ============================================================================\n# MINIMAL SETTINGS - GUARANTEED TO WORK\n# ============================================================================\n\nBATCH_SIZE = 8  # SMALL - won't stall\nIMG_SIZE = (64, 64)  # SMALL - fast loading\nEPOCHS = 50  # Enough for 8 hours\n\nprint(f\"Batch: {BATCH_SIZE} | Image: {IMG_SIZE} | Epochs: {EPOCHS}\\n\")\n\n# ============================================================================\n# SIMPLE AUGMENTATION - NO FANCY SHIT\n# ============================================================================\n\naugmentation = tf.keras.Sequential([\n    tf.keras.layers.RandomFlip(\"horizontal\"),\n    tf.keras.layers.RandomRotation(0.2),\n    tf.keras.layers.RandomZoom(0.1),\n], name=\"augmentation\")\n\n# ============================================================================\n# LOAD DATA - STREAMING ONLY\n# ============================================================================\n\nprint(\"Loading data (streaming mode)...\")\n\ntrain_ds = tf.keras.utils.image_dataset_from_directory(\n    TRAIN_PATH,\n    validation_split=0.2,\n    subset=\"training\",\n    seed=42,\n    image_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    label_mode='int'\n)\n\nval_ds = tf.keras.utils.image_dataset_from_directory(\n    TRAIN_PATH,\n    validation_split=0.2,\n    subset=\"validation\",\n    seed=42,\n    image_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    label_mode='int'\n)\n\ntest_ds = tf.keras.utils.image_dataset_from_directory(\n    TEST_PATH,\n    seed=42,\n    image_size=IMG_SIZE,\n    batch_size=BATCH_SIZE\n)\n\nclass_names = train_ds.class_names\nnum_classes = len(class_names)\nprint(f\"âœ… {num_classes} classes\")\n\n# ============================================================================\n# NORMALIZE ONLY - KEEP IT SIMPLE\n# ============================================================================\n\nnormalization = tf.keras.layers.Rescaling(1./255)\n\ndef prepare(ds, shuffle=False, augment=False):\n    if shuffle:\n        ds = ds.shuffle(1000)\n    \n    # Normalize\n    ds = ds.map(lambda x, y: (normalization(x), y))\n    \n    # Augment AFTER normalize\n    if augment:\n        ds = ds.map(lambda x, y: (augmentation(x, training=True), y))\n    \n    # Minimal prefetch\n    return ds.prefetch(2)\n\ntrain_ds = prepare(train_ds, shuffle=True, augment=True)\nval_ds = prepare(val_ds)\ntest_ds = prepare(test_ds)\n\nprint(\"âœ… Pipeline ready\\n\")\n\n# ============================================================================\n# TEST PIPELINE SPEED (CRITICAL!)\n# ============================================================================\n\nprint(\"Testing pipeline speed...\")\nstart = time.time()\nfor i, (images, labels) in enumerate(train_ds.take(20)):\n    if i % 5 == 0:\n        print(f\"  Batch {i+1}/20... {time.time()-start:.1f}s\")\nelapsed = time.time() - start\nprint(f\"âœ… 20 batches in {elapsed:.1f}s ({20/elapsed:.1f} batch/sec)\")\n\nif elapsed > 30:\n    print(\"âš ï¸ WARNING: Pipeline is SLOW - training will stall!\")\n    print(\"   Reduce batch_size to 4 or restart kernel\")\nelse:\n    print(\"âœ… Pipeline speed OK!\\n\")\n\n# ============================================================================\n# SOLID MODEL - NOT TOO BIG, NOT TOO SMALL\n# ============================================================================\n\nmodel = Sequential([\n    # Block 1\n    Conv2D(32, 3, activation='relu', padding='same', input_shape=(64,64,3)),\n    BatchNormalization(),\n    Conv2D(32, 3, activation='relu', padding='same'),\n    BatchNormalization(),\n    MaxPooling2D(2),\n    Dropout(0.25),\n    \n    # Block 2\n    Conv2D(64, 3, activation='relu', padding='same'),\n    BatchNormalization(),\n    Conv2D(64, 3, activation='relu', padding='same'),\n    BatchNormalization(),\n    MaxPooling2D(2),\n    Dropout(0.25),\n    \n    # Block 3\n    Conv2D(128, 3, activation='relu', padding='same'),\n    BatchNormalization(),\n    MaxPooling2D(2),\n    Dropout(0.3),\n    \n    # Classifier\n    GlobalAveragePooling2D(),\n    Dense(256, activation='relu'),\n    BatchNormalization(),\n    Dropout(0.5),\n    Dense(num_classes, activation='softmax')\n])\n\nprint(f\"Model: {model.count_params():,} params\")\nprint(f\"Size: {model.count_params()*4/1024/1024:.1f} MB\\n\")\n\n# ============================================================================\n# COMPILE\n# ============================================================================\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(0.001),\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel.summary()\n\n# ============================================================================\n# CALLBACKS WITH PROGRESS TRACKING\n# ============================================================================\n\nclass ProgressTracker(tf.keras.callbacks.Callback):\n    def __init__(self):\n        self.start_time = time.time()\n        self.epoch_start = None\n    \n    def on_epoch_begin(self, epoch, logs=None):\n        self.epoch_start = time.time()\n        elapsed = (time.time() - self.start_time) / 3600\n        remaining = 8 - elapsed\n        print(f\"\\nâ° Epoch {epoch+1}/{EPOCHS} | Time used: {elapsed:.1f}h | Remaining: {remaining:.1f}h\")\n    \n    def on_epoch_end(self, epoch, logs=None):\n        epoch_time = (time.time() - self.epoch_start) / 60\n        print(f\"âœ… Epoch took {epoch_time:.1f} min | Val Acc: {logs['val_accuracy']*100:.2f}%\")\n\ncallbacks = [\n    ProgressTracker(),\n    \n    EarlyStopping(\n        monitor='val_accuracy',\n        patience=10,\n        restore_best_weights=True,\n        verbose=1\n    ),\n    \n    ModelCheckpoint(\n        'best_asl.keras',\n        monitor='val_accuracy',\n        save_best_only=True,\n        verbose=1\n    ),\n    \n    ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.5,\n        patience=5,\n        min_lr=1e-7,\n        verbose=1\n    )\n]\n\n# ============================================================================\n# TRAIN WITH ERROR HANDLING\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"STARTING TRAINING\")\nprint(\"=\"*60)\n\ntry:\n    history = model.fit(\n        train_ds,\n        validation_data=val_ds,\n        epochs=EPOCHS,\n        callbacks=callbacks,\n        verbose=2  # Less verbose = faster\n    )\n    \n    print(\"\\nâœ… TRAINING COMPLETED!\")\n    \nexcept tf.errors.ResourceExhaustedError as e:\n    print(\"\\nâŒ OUT OF MEMORY!\")\n    print(\"EMERGENCY FIX:\")\n    print(\"1. Runtime -> Restart Runtime\")\n    print(\"2. Change BATCH_SIZE = 4\")\n    print(\"3. Change IMG_SIZE = (48, 48)\")\n    raise\n\nexcept KeyboardInterrupt:\n    print(\"\\nâš ï¸ Training stopped by user\")\n    print(\"Saving current model...\")\n    model.save(\"interrupted_model.keras\")\n\nexcept Exception as e:\n    print(f\"\\nâŒ ERROR: {e}\")\n    raise\n\n# ============================================================================\n# RESULTS\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"FINAL RESULTS\")\nprint(\"=\"*60)\n\ntrain_acc = history.history['accuracy'][-1]\nval_acc = history.history['val_accuracy'][-1]\n\nprint(f\"\\nTrain Accuracy: {train_acc*100:.2f}%\")\nprint(f\"Val Accuracy:   {val_acc*100:.2f}%\")\n\n# Test\ntest_loss, test_acc = model.evaluate(test_ds, verbose=0)\nprint(f\"Test Accuracy:  {test_acc*100:.2f}%\")\n\nif test_acc >= 0.95:\n    print(\"\\nðŸŽ‰ EXCELLENT! 95%+\")\nelif test_acc >= 0.90:\n    print(\"\\nâœ… GOOD! 90%+\")\nelif test_acc >= 0.85:\n    print(\"\\nðŸ‘ Decent! 85%+\")\nelse:\n    print(\"\\nðŸ“ˆ Needs improvement\")\n\n# ============================================================================\n# ANALYZE WORST CLASSES\n# ============================================================================\n\nfrom sklearn.metrics import classification_report\n\ny_pred, y_true = [], []\nfor x, y in test_ds:\n    preds = model.predict(x, verbose=0)\n    y_pred.extend(np.argmax(preds, axis=1))\n    y_true.extend(y.numpy())\n\nreport = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"WORST 5 CLASSES (collect more data for these)\")\nprint(\"=\"*60)\n\nclass_accs = [(name, report[name]['f1-score']) for name in class_names]\nclass_accs.sort(key=lambda x: x[1])\n\nfor name, score in class_accs[:5]:\n    print(f\"  {name}: {score*100:.1f}%\")\n\n# ============================================================================\n# SAVE\n# ============================================================================\n\nmodel.save(\"asl_final_model.keras\")\nprint(\"\\nâœ… Model saved: asl_final_model.keras\")\n\n# TFLite\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\ntflite_model = converter.convert()\n\nwith open(\"asl_final.tflite\", \"wb\") as f:\n    f.write(tflite_model)\n\nprint(f\"âœ… TFLite saved: {len(tflite_model)/1024:.1f} KB\")\n\n# ============================================================================\n# TRAINING PLOT\n# ============================================================================\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Train')\nplt.plot(history.history['val_accuracy'], label='Val')\nplt.title('Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(alpha=0.3)\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Train')\nplt.plot(history.history['val_loss'], label='Val')\nplt.title('Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('training_plot.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"\\nâœ… DONE!\")\n\n# ============================================================================\n# IF IT STALLS AGAIN\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"IF IT STALLS AT EPOCH 8 AGAIN:\")\nprint(\"=\"*60)\nprint(\"\\n1. RESTART KERNEL (Runtime -> Restart)\")\nprint(\"2. Clear all outputs (Edit -> Clear all outputs)\")\nprint(\"3. Run ONLY this cell\")\nprint(\"4. If still fails:\")\nprint(\"   BATCH_SIZE = 4\")\nprint(\"   IMG_SIZE = (48, 48)\")\nprint(\"\\n5. NUCLEAR OPTION - Force CPU:\")\nprint(\"   os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\")\nprint(\"   (Add at very top before imports)\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-15T16:01:07.308Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TRAIN_PATH=\"/kaggle/input/asl-updated-data/archive(1)/archive/ASL_Alphabet_Dataset/asl_alphabet_train\"\nTEST_PATH=\"/kaggle/input/asl-updated-data/archive(1)/archive/ASL_Alphabet_Dataset/asl_alphabet_test\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T06:22:40.966285Z","iopub.execute_input":"2025-12-17T06:22:40.966575Z","iopub.status.idle":"2025-12-17T06:22:40.970286Z","shell.execute_reply.started":"2025-12-17T06:22:40.966551Z","shell.execute_reply":"2025-12-17T06:22:40.969511Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nimport numpy as np\nimport time\n\n# ============================================================================\n# NUCLEAR MODE - NO MORE BULLSHIT\n# ============================================================================\n\nprint(\"ðŸ’¥ NUCLEAR MODE - FAST & STABLE ðŸ’¥\\n\")\n\n# RESTART EVERYTHING\ntf.keras.backend.clear_session()\nimport gc\ngc.collect()\n\n# FORCE STABILITY\nimport os\nos.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\nos.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n# Limit GPU to prevent OOM\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        tf.config.set_logical_device_configuration(\n            gpus[0],\n            [tf.config.LogicalDeviceConfiguration(memory_limit=3072)]  # Only 3GB\n        )\n        print(\"âœ… GPU limited to 3GB (stable)\")\n    except:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(\"âœ… Memory growth enabled\")\n\n# ============================================================================\n# SETTINGS - TUNED FOR VIDEO SURVIVAL\n# ============================================================================\n\nBATCH_SIZE = 8   # Kept small for stability\nIMG_SIZE = (96, 96)  # INCREASED: 64x64 is too small for video features\nEPOCHS = 50 \n\nprint(f\"Batch: {BATCH_SIZE} | Image: {IMG_SIZE} | Epochs: {EPOCHS}\\n\")\n\n# ============================================================================\n# WEBCAM SIMULATOR - THE \"DIRTY DATA\" AUGMENTATION\n# ============================================================================\n# This trains the model to handle bad lighting and off-center hands\n\naugmentation = tf.keras.Sequential([\n    # 1. Geometry (Simulate imperfect hand tracking)\n    tf.keras.layers.RandomFlip(\"horizontal\"),\n    tf.keras.layers.RandomRotation(0.15),          # Reduced rotation (hands don't spin 360)\n    tf.keras.layers.RandomZoom(0.2),               # Increased zoom (hand moving back/forth)\n    tf.keras.layers.RandomTranslation(0.1, 0.1),   # Hand isn't always perfectly centered\n    \n    # 2. Lighting & Quality (CRITICAL for webcam video)\n    tf.keras.layers.RandomContrast(0.2),           # Webcams often have bad contrast\n    tf.keras.layers.RandomBrightness(0.2),         # Simulates dark rooms or bright windows\n], name=\"webcam_augmentation\")\n\n# ============================================================================\n# LOAD DATA - STREAMING ONLY\n# ============================================================================\n\nprint(\"Loading data (streaming mode)...\")\n\n# NOTE: Make sure TRAIN_PATH and TEST_PATH are defined in your environment!\n# If you are copy-pasting this, ensure those variables exist.\n\ntrain_ds = tf.keras.utils.image_dataset_from_directory(\n    TRAIN_PATH,\n    validation_split=0.2,\n    subset=\"training\",\n    seed=42,\n    image_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    label_mode='int'\n)\n\nval_ds = tf.keras.utils.image_dataset_from_directory(\n    TRAIN_PATH,\n    validation_split=0.2,\n    subset=\"validation\",\n    seed=42,\n    image_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    label_mode='int'\n)\n\ntest_ds = tf.keras.utils.image_dataset_from_directory(\n    TEST_PATH,\n    seed=42,\n    image_size=IMG_SIZE,\n    batch_size=BATCH_SIZE\n)\n\nclass_names = train_ds.class_names\nnum_classes = len(class_names)\nprint(f\"âœ… {num_classes} classes\")\n\n# ============================================================================\n# PIPELINE SETUP\n# ============================================================================\n\nnormalization = tf.keras.layers.Rescaling(1./255)\n\ndef prepare(ds, shuffle=False, augment=False):\n    if shuffle:\n        ds = ds.shuffle(1000)\n    \n    # Normalize FIRST\n    ds = ds.map(lambda x, y: (normalization(x), y))\n    \n    # Augment SECOND (using the new Webcam Simulator)\n    if augment:\n        ds = ds.map(lambda x, y: (augmentation(x, training=True), y))\n    \n    # Prefetch for speed\n    return ds.prefetch(tf.data.AUTOTUNE)\n\ntrain_ds = prepare(train_ds, shuffle=True, augment=True)\nval_ds = prepare(val_ds)\ntest_ds = prepare(test_ds)\n\nprint(\"âœ… Pipeline ready\\n\")\n\n# ============================================================================\n# TEST PIPELINE SPEED\n# ============================================================================\n\nprint(\"Testing pipeline speed...\")\nstart = time.time()\nfor i, (images, labels) in enumerate(train_ds.take(20)):\n    if i % 5 == 0:\n        print(f\"  Batch {i+1}/20... {time.time()-start:.1f}s\")\nelapsed = time.time() - start\nprint(f\"âœ… 20 batches in {elapsed:.1f}s ({20/elapsed:.1f} batch/sec)\")\n\nif elapsed > 40: # Increased threshold slightly for bigger images\n    print(\"âš ï¸ WARNING: Pipeline is SLOW. Reduce batch_size or restart kernel.\")\nelse:\n    print(\"âœ… Pipeline speed OK!\\n\")\n\n# ============================================================================\n# MODEL - UPDATED FOR NEW RESOLUTION\n# ============================================================================\n\n\nmodel = Sequential([\n    # Block 1\n    # Using IMG_SIZE variable so it updates automatically\n    Conv2D(32, 3, activation='relu', padding='same', input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),\n    BatchNormalization(),\n    Conv2D(32, 3, activation='relu', padding='same'),\n    BatchNormalization(),\n    MaxPooling2D(2),\n    Dropout(0.25),\n    \n    # Block 2\n    Conv2D(64, 3, activation='relu', padding='same'),\n    BatchNormalization(),\n    Conv2D(64, 3, activation='relu', padding='same'),\n    BatchNormalization(),\n    MaxPooling2D(2),\n    Dropout(0.25),\n    \n    # Block 3\n    Conv2D(128, 3, activation='relu', padding='same'),\n    BatchNormalization(),\n    MaxPooling2D(2),\n    Dropout(0.3),\n    \n    # Classifier\n    GlobalAveragePooling2D(),\n    Dense(256, activation='relu'),\n    BatchNormalization(),\n    Dropout(0.5),\n    Dense(num_classes, activation='softmax')\n])\n\nprint(f\"Model: {model.count_params():,} params\")\nprint(f\"Size: {model.count_params()*4/1024/1024:.1f} MB\\n\")\n\n# ============================================================================\n# COMPILE & CALLBACKS\n# ============================================================================\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(0.001),\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nclass ProgressTracker(tf.keras.callbacks.Callback):\n    def __init__(self):\n        self.start_time = time.time()\n        self.epoch_start = None\n    \n    def on_epoch_begin(self, epoch, logs=None):\n        self.epoch_start = time.time()\n        elapsed = (time.time() - self.start_time) / 3600\n        remaining = 8 - elapsed\n        print(f\"\\nâ° Epoch {epoch+1}/{EPOCHS} | Time used: {elapsed:.1f}h | Remaining: {remaining:.1f}h\")\n    \n    def on_epoch_end(self, epoch, logs=None):\n        epoch_time = (time.time() - self.epoch_start) / 60\n        print(f\"âœ… Epoch took {epoch_time:.1f} min | Val Acc: {logs['val_accuracy']*100:.2f}%\")\n\ncallbacks = [\n    ProgressTracker(),\n    EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True, verbose=1),\n    ModelCheckpoint('best_asl_video_robust.keras', monitor='val_accuracy', save_best_only=True, verbose=1),\n    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, verbose=1)\n]\n\n# ============================================================================\n# TRAIN\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"STARTING ROBUST TRAINING\")\nprint(\"=\"*60)\n\ntry:\n    history = model.fit(\n        train_ds,\n        validation_data=val_ds,\n        epochs=EPOCHS,\n        callbacks=callbacks,\n        verbose=2 \n    )\n    \n    print(\"\\nâœ… TRAINING COMPLETED!\")\n    \nexcept tf.errors.ResourceExhaustedError as e:\n    print(\"\\nâŒ OUT OF MEMORY!\")\n    print(\"EMERGENCY FIX:\")\n    print(\"1. Restart Runtime\")\n    print(\"2. Change BATCH_SIZE = 4\")\n    raise\n\nexcept Exception as e:\n    print(f\"\\nâŒ ERROR: {e}\")\n    raise\n\n# ============================================================================\n# RESULTS & SAVE\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"FINAL RESULTS\")\nprint(\"=\"*60)\n\n# Evaluate\ntest_loss, test_acc = model.evaluate(test_ds, verbose=0)\nprint(f\"Test Accuracy:  {test_acc*100:.2f}%\")\n\n# Save\nmodel.save(\"asl_video_robust.keras\")\nprint(\"\\nâœ… Model saved: asl_video_robust.keras\")\n\n# TFLite conversion\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\ntflite_model = converter.convert()\n\nwith open(\"asl_video_robust.tflite\", \"wb\") as f:\n    f.write(tflite_model)\n\nprint(f\"âœ… TFLite saved: {len(tflite_model)/1024:.1f} KB\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# 1. SAFER AUGMENTATION (Toned Down)\n# ============================================================================\naugmentation = tf.keras.Sequential([\n    tf.keras.layers.RandomFlip(\"horizontal\"),\n    tf.keras.layers.RandomRotation(0.1),    # Reduced from 0.15\n    tf.keras.layers.RandomZoom(0.1),        # Reduced from 0.2 (Less risk of cropping fingers)\n    # REMOVED RandomTranslation (It was likely pushing hands off-screen)\n    \n    # Lighting is still important, but let's be gentler\n    tf.keras.layers.RandomContrast(0.1),\n    tf.keras.layers.RandomBrightness(0.1),\n], name=\"webcam_augmentation\")\n\n# ============================================================================\n# 2. STRONGER MODEL (Lower Dropout = Easier to Learn)\n# ============================================================================\nmodel = Sequential([\n    # Block 1\n    Conv2D(32, 3, activation='relu', padding='same', input_shape=(96, 96, 3)),\n    BatchNormalization(),\n    MaxPooling2D(2),\n    # REMOVED Dropout here (Let early layers learn features fully)\n    \n    # Block 2\n    Conv2D(64, 3, activation='relu', padding='same'),\n    BatchNormalization(),\n    MaxPooling2D(2),\n    Dropout(0.2), # Reduced from 0.25\n    \n    # Block 3\n    Conv2D(128, 3, activation='relu', padding='same'),\n    BatchNormalization(),\n    MaxPooling2D(2),\n    Dropout(0.2), # Reduced from 0.3\n    \n    # Classifier\n    GlobalAveragePooling2D(),\n    Dense(256, activation='relu'),\n    BatchNormalization(),\n    Dropout(0.3), # Reduced from 0.5 (Critical change!)\n    Dense(num_classes, activation='softmax')\n])\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(0.0005), # Slightly lower LR for stability\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ProgressTracker(tf.keras.callbacks.Callback):\n    def __init__(self):\n        self.start_time = time.time()\n        self.epoch_start = None\n    \n    def on_epoch_begin(self, epoch, logs=None):\n        self.epoch_start = time.time()\n        elapsed = (time.time() - self.start_time) / 3600\n        remaining = 8 - elapsed\n        print(f\"\\nâ° Epoch {epoch+1}/{EPOCHS} | Time used: {elapsed:.1f}h | Remaining: {remaining:.1f}h\")\n    \n    def on_epoch_end(self, epoch, logs=None):\n        epoch_time = (time.time() - self.epoch_start) / 60\n        print(f\"âœ… Epoch took {epoch_time:.1f} min | Val Acc: {logs['val_accuracy']*100:.2f}%\")\n\ncallbacks = [\n    ProgressTracker(),\n    EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True, verbose=1),\n    ModelCheckpoint('best_asl_video_robust.keras', monitor='val_accuracy', save_best_only=True, verbose=1),\n    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, verbose=1)\n]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# TRAIN\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"STARTING ROBUST TRAINING\")\nprint(\"=\"*60)\n\ntry:\n    history = model.fit(\n        train_ds,\n        validation_data=val_ds,\n        epochs=EPOCHS,\n        callbacks=callbacks,\n        verbose=2 \n    )\n    \n    print(\"\\nâœ… TRAINING COMPLETED!\")\n    \nexcept tf.errors.ResourceExhaustedError as e:\n    print(\"\\nâŒ OUT OF MEMORY!\")\n    print(\"EMERGENCY FIX:\")\n    print(\"1. Restart Runtime\")\n    print(\"2. Change BATCH_SIZE = 4\")\n    raise\n\nexcept Exception as e:\n    print(f\"\\nâŒ ERROR: {e}\")\n    raise\n\n# ============================================================================\n# RESULTS & SAVE\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"FINAL RESULTS\")\nprint(\"=\"*60)\n\n# Evaluate\ntest_loss, test_acc = model.evaluate(test_ds, verbose=0)\nprint(f\"Test Accuracy:  {test_acc*100:.2f}%\")\n\n# Save\nmodel.save(\"asl_video_robust.keras\")\nprint(\"\\nâœ… Model saved: asl_video_robust.keras\")\n\n# TFLite conversion\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\ntflite_model = converter.convert()\n\nwith open(\"asl_video_robust.tflite\", \"wb\") as f:\n    f.write(tflite_model)\n\nprint(f\"âœ… TFLite saved: {len(tflite_model)/1024:.1f} KB\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save(\"asl_tiny_model_4.keras\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\nshutil.make_archive(\"asl_tiny_savedmodel\", 'zip', \"asl_tiny_savedmodel\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models, regularizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nimport os\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T06:32:29.300651Z","iopub.execute_input":"2025-12-17T06:32:29.301234Z","iopub.status.idle":"2025-12-17T06:32:52.297681Z","shell.execute_reply.started":"2025-12-17T06:32:29.301211Z","shell.execute_reply":"2025-12-17T06:32:52.296964Z"}},"outputs":[{"name":"stderr","text":"2025-12-17 06:32:33.533166: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765953154.102510      38 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765953154.246593      38 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"TRAIN_DIR=\"/kaggle/input/asl-dataset-mine/archive/ASL_Alphabet_Dataset/asl_alphabet_train/\"\n\nTEST_DIR=\"/kaggle/input/asl-dataset-mine/archive/ASL_Alphabet_Dataset/asl_alphabet_test/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T06:52:43.245403Z","iopub.execute_input":"2025-12-17T06:52:43.245885Z","iopub.status.idle":"2025-12-17T06:52:43.249369Z","shell.execute_reply.started":"2025-12-17T06:52:43.245862Z","shell.execute_reply":"2025-12-17T06:52:43.248701Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"IMG_SIZE = 64\nBATCH_SIZE = 32\nEPOCHS = 30\nNUM_CLASSES = 29    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T06:45:58.229964Z","iopub.execute_input":"2025-12-17T06:45:58.230701Z","iopub.status.idle":"2025-12-17T06:45:58.234303Z","shell.execute_reply.started":"2025-12-17T06:45:58.230675Z","shell.execute_reply":"2025-12-17T06:45:58.233717Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"train_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    brightness_range=[0.7, 1.3],\n    fill_mode='nearest'\n)\n\ntest_datagen = ImageDataGenerator(\n    rescale=1./255\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T06:45:59.371126Z","iopub.execute_input":"2025-12-17T06:45:59.371486Z","iopub.status.idle":"2025-12-17T06:45:59.376062Z","shell.execute_reply.started":"2025-12-17T06:45:59.371465Z","shell.execute_reply":"2025-12-17T06:45:59.375228Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"train_generator = train_datagen.flow_from_directory(\n    TRAIN_DIR,\n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical'\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    TEST_DIR,\n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical'\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T06:46:01.797989Z","iopub.execute_input":"2025-12-17T06:46:01.798284Z","iopub.status.idle":"2025-12-17T06:46:42.104706Z","shell.execute_reply.started":"2025-12-17T06:46:01.798265Z","shell.execute_reply":"2025-12-17T06:46:42.104148Z"}},"outputs":[{"name":"stdout","text":"Found 223074 images belonging to 29 classes.\nFound 28 images belonging to 28 classes.\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"weight_decay = 1e-4\n\nmodel = models.Sequential([\n    layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3)),\n\n    layers.Conv2D(32, (3,3), activation='relu',\n                  kernel_regularizer=regularizers.l2(weight_decay)),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D(2,2),\n    layers.Dropout(0.25),\n\n    layers.Conv2D(64, (3,3), activation='relu',\n                  kernel_regularizer=regularizers.l2(weight_decay)),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D(2,2),\n    layers.Dropout(0.25),\n\n    layers.Conv2D(128, (3,3), activation='relu',\n                  kernel_regularizer=regularizers.l2(weight_decay)),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D(2,2),\n    layers.Dropout(0.3),\n\n    layers.Conv2D(256, (3,3), activation='relu',\n                  kernel_regularizer=regularizers.l2(weight_decay)),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D(2,2),\n    layers.Dropout(0.4),\n\n    layers.Flatten(),\n\n    layers.Dense(256, activation='relu',\n                 kernel_regularizer=regularizers.l2(weight_decay)),\n    layers.BatchNormalization(),\n    layers.Dropout(0.5),\n\n    layers.Dense(NUM_CLASSES, activation='softmax')\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T06:46:42.105774Z","iopub.execute_input":"2025-12-17T06:46:42.106003Z","iopub.status.idle":"2025-12-17T06:46:42.226964Z","shell.execute_reply.started":"2025-12-17T06:46:42.105987Z","shell.execute_reply":"2025-12-17T06:46:42.226144Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T06:46:42.227917Z","iopub.execute_input":"2025-12-17T06:46:42.228193Z","iopub.status.idle":"2025-12-17T06:46:42.255214Z","shell.execute_reply.started":"2025-12-17T06:46:42.228176Z","shell.execute_reply":"2025-12-17T06:46:42.254644Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚           \u001b[38;5;34m896\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_5           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚           \u001b[38;5;34m128\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚        \u001b[38;5;34m18,496\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_6           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚           \u001b[38;5;34m256\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    â”‚        \u001b[38;5;34m73,856\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_7           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    â”‚           \u001b[38;5;34m512\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling2d_6 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)      â”‚       \u001b[38;5;34m295,168\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_8           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)      â”‚         \u001b[38;5;34m1,024\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling2d_7 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m256\u001b[0m)      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m256\u001b[0m)      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â”‚       \u001b[38;5;34m262,400\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_9           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â”‚         \u001b[38;5;34m1,024\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m)             â”‚         \u001b[38;5;34m7,453\u001b[0m â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_5           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_6           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_7           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_8           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,400</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_9           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,453</span> â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m661,213\u001b[0m (2.52 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">661,213</span> (2.52 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m659,741\u001b[0m (2.52 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">659,741</span> (2.52 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,472\u001b[0m (5.75 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,472</span> (5.75 KB)\n</pre>\n"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"callbacks = [\n    EarlyStopping(\n        monitor='val_loss',\n        patience=5,\n        restore_best_weights=True\n    ),\n    ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.5,\n        patience=3,\n        min_lr=1e-6\n    )\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T06:46:42.256686Z","iopub.execute_input":"2025-12-17T06:46:42.257123Z","iopub.status.idle":"2025-12-17T06:46:42.260595Z","shell.execute_reply.started":"2025-12-17T06:46:42.257106Z","shell.execute_reply":"2025-12-17T06:46:42.259862Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    train_generator,\n    epochs=EPOCHS,\n    validation_data=test_generator,\n    callbacks=callbacks\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T06:46:42.261431Z","iopub.execute_input":"2025-12-17T06:46:42.261755Z","iopub.status.idle":"2025-12-17T06:48:35.236673Z","shell.execute_reply.started":"2025-12-17T06:46:42.261733Z","shell.execute_reply":"2025-12-17T06:48:35.235573Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/30\n\u001b[1m 382/6972\u001b[0m \u001b[32mâ”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m28:45\u001b[0m 262ms/step - accuracy: 0.0544 - loss: 4.3454","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_38/3050153361.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":31},{"cell_type":"code","source":"model.save(\"asl_cnn_model_5.h5\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TRAIN_PATH=\"/kaggle/input/asl-updated-data/archive(1)/archive/ASL_Alphabet_Dataset/asl_alphabet_train/\"\nTEST_PATH=\"/kaggle/input/asl-updated-data/archive(1)/archive/ASL_Alphabet_Dataset/asl_alphabet_test/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T06:56:51.622799Z","iopub.execute_input":"2025-12-17T06:56:51.623314Z","iopub.status.idle":"2025-12-17T06:56:51.626882Z","shell.execute_reply.started":"2025-12-17T06:56:51.623294Z","shell.execute_reply":"2025-12-17T06:56:51.625967Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"len(os.listdir(TEST_PATH))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T06:56:52.501037Z","iopub.execute_input":"2025-12-17T06:56:52.501288Z","iopub.status.idle":"2025-12-17T06:56:52.507517Z","shell.execute_reply.started":"2025-12-17T06:56:52.501271Z","shell.execute_reply":"2025-12-17T06:56:52.506815Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"28"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.optimizers import Adam\nimport os\n\n# --- Configuration ---\n# DATASET_DIR = \"/kaggle/input/asl-alphabet/asl_alphabet_train\"\nIMG_SIZE = (64, 64)\nBATCH_SIZE = 32\nEPOCHS = 20\nNUM_CLASSES = 29\nLEARNING_RATE = 0.001\n\n# --- Data Preparation ---\n\n# Data Augmentation Configuration\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    validation_split=0.2,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    brightness_range=[0.8, 1.2],\n    fill_mode='nearest'\n)\n\ntest_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n\nprint(f\"Loading data from {DATASET_DIR}...\")\n\ntrain_generator = train_datagen.flow_from_directory(\n    TRAIN_DIR,\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    subset='training',\n    shuffle=True\n)\n\n# validation_generator = test_datagen.flow_from_directory(\n#     TEST_DIR,\n#     target_size=IMG_SIZE,\n#     batch_size=BATCH_SIZE,\n#     class_mode='categorical',\n#     subset='validation',\n#     shuffle=False\n# )\n\n# --- Model Architecture ---\ndef build_model():\n    model = Sequential()\n\n    # First Block\n    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', \n                     input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3),\n                     kernel_regularizer=regularizers.l2(0.001)))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.2)) # Light dropout after conv block\n\n    # Second Block\n    model.add(Conv2D(64, (3, 3), activation='relu', padding='same',\n                     kernel_regularizer=regularizers.l2(0.001)))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.2))\n\n    # Third Block\n    model.add(Conv2D(128, (3, 3), activation='relu', padding='same',\n                     kernel_regularizer=regularizers.l2(0.001)))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.3))\n    \n    # Fourth Block\n    model.add(Conv2D(256, (3, 3), activation='relu', padding='same',\n                     kernel_regularizer=regularizers.l2(0.001)))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.3))\n\n    # Dense Layers\n    model.add(Flatten())\n    \n    model.add(Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5)) # Configurable high dropout\n\n    model.add(Dense(NUM_CLASSES, activation='softmax'))\n\n    return model\n\nmodel = build_model()\n\n# --- Compilation ---\noptimizer = Adam(learning_rate=LEARNING_RATE)\n\nmodel.compile(optimizer=optimizer,\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.summary()\n\n# --- Training ---\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n    validation_data=validation_generator,\n    validation_steps=validation_generator.samples // BATCH_SIZE,\n    epochs=EPOCHS\n)\n\n# --- Save Model ---\nmodel.save('asl_model_5.h5')\nprint(\"Model training complete and saved as asl_model.h5\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T07:33:09.759253Z","iopub.execute_input":"2025-12-17T07:33:09.759562Z","iopub.status.idle":"2025-12-17T07:33:09.771982Z","shell.execute_reply.started":"2025-12-17T07:33:09.759542Z","shell.execute_reply":"2025-12-17T07:33:09.771020Z"}},"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_38/2994836406.py\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    train_datagen = ImageDataGenerator(\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '(' was never closed\n"],"ename":"SyntaxError","evalue":"'(' was never closed (2994836406.py, line 20)","output_type":"error"}],"execution_count":45},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.optimizers import Adam\nimport os\nimport sys\n\n# --- Configuration ---\nDATASET_DIR = \"/kaggle/input/asl-alphabet/asl_alphabet_train\"\nIMG_SIZE = (64, 64)\nBATCH_SIZE = 32\nEPOCHS = 20\nNUM_CLASSES = 29\nLEARNING_RATE = 0.001\n\n# --- Data Preparation ---\nif not os.path.exists(DATASET_DIR):\n    print(f\"ERROR: Dataset directory not found at {DATASET_DIR}\")\n    print(\"Please check your dataset path.\")\n    sys.exit(1)\n\n\n# Data Augmentation Configuration\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    validation_split=0.2,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    brightness_range=[0.8, 1.2],\n    fill_mode='nearest'\n)\n\ntest_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n\nprint(f\"Loading data from {DATASET_DIR}...\")\n\ntrain_generator = train_datagen.flow_from_directory(\n    TRAIN_DIR,\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    subset='training',\n    shuffle=True\n)\n\nvalidation_generator = test_datagen.flow_from_directory(\n    TRAIN_DIR,\n    target_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    subset='validation',\n    shuffle=False\n)\n\n# Safety check for empty generators\nif train_generator.samples == 0:\n    print(\"ERROR: Training set is empty. Check dataset structure.\")\n    sys.exit(1)\nif validation_generator.samples == 0:\n    print(\"ERROR: Validation set is empty. Check dataset structure or validation_split.\")\n    sys.exit(1)\n\n# --- Model Architecture ---\ndef build_model():\n    model = Sequential()\n\n    # First Block\n    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', \n                     input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3),\n                     kernel_regularizer=regularizers.l2(0.001)))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.2)) # Light dropout after conv block\n\n    # Second Block\n    model.add(Conv2D(64, (3, 3), activation='relu', padding='same',\n                     kernel_regularizer=regularizers.l2(0.001)))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.2))\n\n    # Third Block\n    model.add(Conv2D(128, (3, 3), activation='relu', padding='same',\n                     kernel_regularizer=regularizers.l2(0.001)))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.3))\n    \n    # Fourth Block\n    model.add(Conv2D(256, (3, 3), activation='relu', padding='same',\n                     kernel_regularizer=regularizers.l2(0.001)))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.3))\n\n    # Dense Layers\n    model.add(Flatten())\n    \n    model.add(Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5)) # Configurable high dropout\n\n    model.add(Dense(NUM_CLASSES, activation='softmax'))\n\n    return model\n\nmodel = build_model()\n\n# --- Compilation ---\noptimizer = Adam(learning_rate=LEARNING_RATE)\n\nmodel.compile(optimizer=optimizer,\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.summary()\n\n# --- Training ---\n# Using len(generator) automatically calculates the correct number of steps (samples // batch_size + 1 if remainder)\n# This prevents crashes when samples < batch_size or when integer division results in 0\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=len(train_generator),\n    validation_data=validation_generator,\n    validation_steps=len(validation_generator),\n    epochs=EPOCHS\n)\n\n# --- Save Model ---\nmodel.save('asl_model_5.h5')\nprint(\"Model training complete and saved as asl_model.h5\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T07:36:57.542082Z","iopub.execute_input":"2025-12-17T07:36:57.542607Z"}},"outputs":[{"name":"stdout","text":"Loading data from /kaggle/input/asl-alphabet/asl_alphabet_train...\nFound 178472 images belonging to 29 classes.\nFound 44602 images belonging to 29 classes.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_6\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ conv2d_24 (\u001b[38;5;33mConv2D\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚           \u001b[38;5;34m896\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_30          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚           \u001b[38;5;34m128\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling2d_24 (\u001b[38;5;33mMaxPooling2D\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_30 (\u001b[38;5;33mDropout\u001b[0m)            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_25 (\u001b[38;5;33mConv2D\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚        \u001b[38;5;34m18,496\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_31          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚           \u001b[38;5;34m256\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling2d_25 (\u001b[38;5;33mMaxPooling2D\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_31 (\u001b[38;5;33mDropout\u001b[0m)            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_26 (\u001b[38;5;33mConv2D\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    â”‚        \u001b[38;5;34m73,856\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_32          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    â”‚           \u001b[38;5;34m512\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling2d_26 (\u001b[38;5;33mMaxPooling2D\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_32 (\u001b[38;5;33mDropout\u001b[0m)            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_27 (\u001b[38;5;33mConv2D\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)      â”‚       \u001b[38;5;34m295,168\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_33          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)      â”‚         \u001b[38;5;34m1,024\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling2d_27 (\u001b[38;5;33mMaxPooling2D\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_33 (\u001b[38;5;33mDropout\u001b[0m)            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ flatten_6 (\u001b[38;5;33mFlatten\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_12 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            â”‚     \u001b[38;5;34m2,097,664\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_34          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            â”‚         \u001b[38;5;34m2,048\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_34 (\u001b[38;5;33mDropout\u001b[0m)            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_13 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m)             â”‚        \u001b[38;5;34m14,877\u001b[0m â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ conv2d_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_30          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling2d_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_31          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling2d_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_32          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling2d_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_33          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling2d_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ flatten_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,097,664</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_34          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">14,877</span> â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,504,925\u001b[0m (9.56 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,504,925</span> (9.56 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,502,941\u001b[0m (9.55 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,502,941</span> (9.55 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,984\u001b[0m (7.75 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,984</span> (7.75 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/20\n\u001b[1m   5/5578\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m7:45\u001b[0m 84ms/step - accuracy: 0.0012 - loss: 6.5650     ","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}