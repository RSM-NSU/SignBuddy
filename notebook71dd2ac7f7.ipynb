{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install mediapipe\n",
    "!pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join('/kaggle/input/mpdata/MP_DATA')\n",
    "\n",
    "actions = np.array(['a','b','c','d','e','f','g',\n",
    "                    'h','i','j','k','l','m','n',\n",
    "                    'o','p','q','r','s','t','u',\n",
    "                    'v','w','x','y','z'])\n",
    "\n",
    "#no of videos\n",
    "no_sequnces = 30\n",
    "\n",
    "#number of frames\n",
    "sequnce_length = 30 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join('/kaggle/input/wlpsl/WLPSL')\n",
    "action = []\n",
    "for subdir,dirs,files in os.walk(DATA_PATH +\"/Videos\"):\n",
    "    for dirc in dirs:\n",
    "        action.append(dirc)\n",
    "\n",
    "actions = np.array(action)\n",
    "\n",
    "#no of videos\n",
    "no_sequnces = 30\n",
    "\n",
    "#number of frames\n",
    "sequnce_length = 30 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "label_map = {label:num for num,label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sequences,labels = [],[]\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequnces):\n",
    "        window = []\n",
    "        for frame_num in range(sequnce_length):\n",
    "            res = np.load(os.path.join(DATA_PATH,action,str(sequence),\"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "x = np.array(sequences)\n",
    "y = to_categorical(labels).astype(int)\n",
    "X_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(x.shape)   # should be (num_samples, 30, 1662) or similar\n",
    "print(y.shape)   # should be (num_samples,)\n",
    "print(np.unique(np.argmax(y, axis=1)))  # should show 26 unique numbers (0–25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n",
    "X_train_scaled = scaler.fit_transform(X_train_reshaped)\n",
    "X_train = X_train_scaled.reshape(X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X_test_reshaped = x_test.reshape(-1, x_test.shape[-1])\n",
    "X_test_scaled = scaler.transform(X_test_reshaped)\n",
    "x_test = X_test_scaled.reshape(x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(700)\n",
    "X_train_reduced = pca.fit_transform(X_train.reshape(-1, X_train.shape[-1]))\n",
    "X_train_reduced = X_train_reduced.reshape(X_train.shape[0], X_train.shape[1], 700)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_test_reduced = pca.transform(x_test.reshape(-1, x_test.shape[-1]))\n",
    "X_test_reduced = X_test_reduced.reshape(x_test.shape[0], x_test.shape[1], 700)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(scaler,\"scalerV2.pkl\")\n",
    "joblib.dump(pca,\"pcaV2_700.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def augment_sequence(sequence, noise_std=0.001, dropout_prob=0.1):\n",
    "    \"\"\"\n",
    "    sequence: np.array of shape (30, 1662)\n",
    "    \"\"\"\n",
    "    seq = sequence.copy()\n",
    "\n",
    "    # 1. Add small Gaussian noise\n",
    "    seq += np.random.normal(0, noise_std, seq.shape)\n",
    "\n",
    "    # 2. Random feature scaling\n",
    "    seq *= np.random.uniform(0.95, 1.02)\n",
    "\n",
    "    # 3. Random frame dropout\n",
    "    if random.random() < dropout_prob:\n",
    "        drop_idx = np.random.choice(seq.shape[0], size=1, replace=False)\n",
    "        seq = np.delete(seq, drop_idx, axis=0)\n",
    "        # Pad back to 30 frames (duplicate last frame)\n",
    "        seq = np.pad(seq, ((0, 1), (0, 0)), mode='edge')\n",
    "\n",
    "    return seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_augmented = []\n",
    "y_augmented = []\n",
    "\n",
    "for x, y in zip(X_train_reduced, y_train):\n",
    "    X_augmented.append(x)\n",
    "    y_augmented.append(y)\n",
    "\n",
    "    # Add one augmented version of each sequence\n",
    "    X_augmented.append(augment_sequence(x))\n",
    "    y_augmented.append(y)\n",
    "\n",
    "X_augmented = np.array(X_augmented)\n",
    "y_augmented = np.array(y_augmented)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_augmented.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_augmented.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense,Dropout,BatchNormalization,Bidirectional\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "log_dir = os.path.join('logs')\n",
    "\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=40,              # stop if val_loss doesn't improve for 20 epochs\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'best_model.keras',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=15,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Bidirectional(LSTM(64,return_sequences=True,activation=\"tanh\",input_shape=(30,700))))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Bidirectional(LSTM(64,return_sequences=False,activation=\"tanh\")))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Dense(128,activation='relu',kernel_regularizer=l2(5e-6)))\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "model.add(Dense(64,activation='relu',))\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "model.add(Dense(32,activation='relu',))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(actions.shape[0],activation='softmax'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"Adam\",loss=\"categorical_crossentropy\",metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_augmented,y_augmented,validation_split=0.2,epochs=2000,callbacks=[tb_callback,early_stop,checkpoint,reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "loss,acc = model.evaluate(X_test_reduced,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save(\"model_12.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = load_model(\"model_11.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2,os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join('/kaggle/input/asl-dataset/asl_dataset/asl_dataset')\n",
    "output_path = \"/kaggle/working/outputs\"\n",
    "FRAMES_PER_VIDEO = 30\n",
    "FRAME_SIZE = (224,224)\n",
    "OUTPUT_PATH = \"/kaggle/working/outputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.listdir(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2, os\n",
    "\n",
    "def extract_frames(video_path, output_folder, frame_count=60):  \n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    if total_frames == 0:\n",
    "        print(f\"Skipping (no frames): {video_path}\")\n",
    "        return \n",
    "    \n",
    "    # FIXED evenly spaced frames\n",
    "    frame_indices = [int(i * total_frames / frame_count) for i in range(frame_count)]\n",
    "    frame_indices = sorted(set(frame_indices))\n",
    "\n",
    "    saved = 0\n",
    "    count = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if count in frame_indices:\n",
    "            frame = cv2.resize(frame, FRAME_SIZE)\n",
    "            frame_name = f\"frame_{saved}.jpg\"\n",
    "            cv2.imwrite(os.path.join(output_folder, frame_name), frame)\n",
    "            saved += 1\n",
    "        \n",
    "        count += 1\n",
    "\n",
    "        if saved >= len(frame_indices):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"Extracted {saved} frames from {video_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_all_frames():\n",
    "    for class_name in os.listdir(DATA_PATH):\n",
    "        class_path = os.path.join(DATA_PATH, class_name)\n",
    "\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nProcessing Class: {class_name}\")\n",
    "\n",
    "        output_dir = os.path.join(output_path, class_name)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        for video_file in os.listdir(class_path):\n",
    "            if video_file.lower().endswith((\".mp4\", \".avi\", \".mov\", \".mkv\")):\n",
    "\n",
    "                video_path = os.path.join(class_path, video_file)\n",
    "\n",
    "                video_output_folder = os.path.join(\n",
    "                    output_dir,\n",
    "                    video_file.split('.')[0]\n",
    "                )\n",
    "                os.makedirs(video_output_folder, exist_ok=True)\n",
    "\n",
    "                print(f\"Extracting ALL frames from: {video_file}\")\n",
    "                extract_frames(video_path, video_output_folder)\n",
    "\n",
    "    print(\"\\nAll Frames Extracted Successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "extract_all_frames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_frame(frame_path,dataset_dir,frame):\n",
    "    os.makedirs(dataset_dir,exist_ok=True)\n",
    "    \n",
    "    img = cv2.imread(frame_path)\n",
    "    resized_img = cv2.resize(img,(224,224),interpolation=cv2.INTER_AREA)\n",
    "    rgb_img = cv2.cvtColor(resized_img,cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    norm_img = cv2.normalize(\n",
    "    rgb_img, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
    "     \n",
    "    final_img = norm_img\n",
    "    bgr_final = cv2.cvtColor(final_img, cv2.COLOR_RGB2BGR)\n",
    "    folder_path = os.path.join(dataset_dir)\n",
    "    cv2.imwrite(os.path.join(folder_path,frame),bgr_final)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "preprocess_output_path = \"/kaggle/working/new_dataset_asl\"\n",
    "alphabetinput = '/kaggle/input/dataset-pakistani-sign-language/dataset'\n",
    "os.makedirs(preprocess_output_path,exist_ok=True)\n",
    "\n",
    "def process_all_frames():\n",
    "    for class_name in os.listdir(DATA_PATH):\n",
    "        class_path = os.path.join(DATA_PATH,class_name)\n",
    "        \n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "    \n",
    "        for video_num in os.listdir(class_path):\n",
    "            frame_dir = os.path.join(class_path,video_num)\n",
    "    \n",
    "            for frame in os.listdir(frame_dir):\n",
    "                \n",
    "                frame_path = os.path.join(frame_dir,frame)\n",
    "                \n",
    "                dataset_path = os.path.join(preprocess_output_path,class_name)\n",
    "                dataset_dir = os.path.join(dataset_path,video_num)\n",
    "                print(dataset_dir)\n",
    "                print(f\"Preprocessing {frame} of {class_name}\")\n",
    "                preprocess_frame(frame_path,dataset_dir,frame)\n",
    "                \n",
    "                \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#for alphabets\n",
    "def process_all_frames():\n",
    "    for class_name in os.listdir(DATA_PATH):\n",
    "        class_path = os.path.join(DATA_PATH, class_name)\n",
    "\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "\n",
    "        # output folder for this class\n",
    "        output_class_dir = os.path.join(preprocess_output_path, class_name)\n",
    "        os.makedirs(output_class_dir, exist_ok=True)\n",
    "\n",
    "        # iterate all images\n",
    "        for frame in os.listdir(class_path):\n",
    "            frame_path = os.path.join(class_path, frame)\n",
    "\n",
    "            if not frame.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                continue  # skip non-images\n",
    "            \n",
    "            print(f\"Preprocessing {frame} of {class_name}\")\n",
    "            preprocess_frame(frame_path, output_class_dir, frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "process_all_frames()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T05:46:20.212884Z",
     "iopub.status.busy": "2025-12-01T05:46:20.212576Z",
     "iopub.status.idle": "2025-12-01T05:46:20.218378Z",
     "shell.execute_reply": "2025-12-01T05:46:20.216798Z",
     "shell.execute_reply.started": "2025-12-01T05:46:20.212860Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = \"/kaggle/working/new_dataset_asl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_size= (224,224)\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    DATASET_PATH,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=42,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    DATASET_PATH,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=42,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BIGGER_ASL_DATASET = \"/kaggle/input/american-sign-language/ASL_Dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = os.path.join(BIGGER_ASL_DATASET,\"Train\")\n",
    "test = os.path.join(BIGGER_ASL_DATASET,\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "batch_size = 32\n",
    "img_size= (224,224)\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=42,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=42,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    test,\n",
    "    seed=42,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)\n",
    "\n",
    "for images,labels in train_ds.take(1):\n",
    "    print(images.shape)\n",
    "    print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "\n",
    "\n",
    "train_ds = train_ds.prefetch(AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(AUTOTUNE)\n",
    "test_ds = test_ds.prefetch(AUTOTUNE)\n",
    "\n",
    "for images, labels in train_ds.take(1):\n",
    "    print(f\"After normalization - Min: {images.numpy().min():.3f}, Max: {images.numpy().max():.3f}, Mean: {images.numpy().mean():.3f}\")\n",
    "    # Should now print: Min: 0.0, Max: 1.0 (approximately)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout,Rescaling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_classes=len(class_names)\n",
    "\n",
    "model = Sequential([\n",
    "    \n",
    "    Conv2D(32,(3,3),activation='relu',input_shape=(224,224,3)),\n",
    "    MaxPooling2D(2,2),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Conv2D(64,(3,3),activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Conv2D(128,(3,3),activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(128,activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(64,activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes,activation='softmax')\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',           # Monitor validation loss\n",
    "    patience=3,                    # Stop if no improvement for 5 epochs\n",
    "    restore_best_weights=True,     # Restore weights from best epoch\n",
    "    verbose=1                      # Print messages\n",
    ")\n",
    "\n",
    "# Optional: Save the best model during training\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    'best_model.keras',           # or 'best_model.h5'\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy\n",
    "ax[0].plot(history.history['accuracy'], label='Train Acc')\n",
    "ax[0].plot(history.history['val_accuracy'], label='Val Acc')\n",
    "ax[0].set_title(\"Accuracy\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Accuracy\")\n",
    "ax[0].legend()\n",
    "ax[0].grid(True)\n",
    "\n",
    "# Loss\n",
    "ax[1].plot(history.history['loss'], label='Train Loss')\n",
    "ax[1].plot(history.history['val_loss'], label='Val Loss')\n",
    "ax[1].set_title(\"Loss\")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"Loss\")\n",
    "ax[1].legend()\n",
    "ax[1].grid(True)\n",
    "\n",
    "plt.show()\n",
    "plt.savefig(\"signbuddy_cnn.png\",dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for batch_x, batch_y in test_ds:\n",
    "    # predict on this batch\n",
    "    batch_preds = model.predict(batch_x, verbose=0)\n",
    "    batch_pred_classes = np.argmax(batch_preds, axis=1)\n",
    "\n",
    "    # store\n",
    "    y_true.extend(batch_y.numpy())\n",
    "    y_pred.extend(batch_pred_classes)\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(cm)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_ds)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get one batch for visualization\n",
    "for images, labels in test_ds.take(1):\n",
    "    preds = model.predict(images)\n",
    "    pred_classes = np.argmax(preds, axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i in range(min(9, len(images))):\n",
    "        plt.subplot(3, 3, i+1)\n",
    "        plt.imshow(images[i].numpy())\n",
    "        plt.title(f\"True: {class_names[labels[i]]}\\nPred: {class_names[pred_classes[i]]}\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save(\"signbuddy_cnn_asl_model_4.h5\")\n",
    "model.save(\"signbuddy_cnn_asl_model_4.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T05:45:21.690648Z",
     "iopub.status.busy": "2025-12-01T05:45:21.690043Z",
     "iopub.status.idle": "2025-12-01T05:45:40.194920Z",
     "shell.execute_reply": "2025-12-01T05:45:40.193601Z",
     "shell.execute_reply.started": "2025-12-01T05:45:21.690615Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 05:45:23.576441: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764567923.819990      38 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764567923.889228      38 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T05:45:40.197271Z",
     "iopub.status.busy": "2025-12-01T05:45:40.196657Z",
     "iopub.status.idle": "2025-12-01T05:45:41.030965Z",
     "shell.execute_reply": "2025-12-01T05:45:41.029793Z",
     "shell.execute_reply.started": "2025-12-01T05:45:40.197234Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 05:45:40.214198: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 12 variables whereas the saved optimizer has 22 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "model = load_model('/kaggle/working/signbuddy_cnn_asl_model_4.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T05:49:35.199512Z",
     "iopub.status.busy": "2025-12-01T05:49:35.199221Z",
     "iopub.status.idle": "2025-12-01T05:49:44.123556Z",
     "shell.execute_reply": "2025-12-01T05:49:44.122174Z",
     "shell.execute_reply.started": "2025-12-01T05:49:35.199491Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1764568175.824789      89 service.cc:148] XLA service 0x7c89c4008870 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1764568175.825651      89 service.cc:156]   StreamExecutor device (0): Host, Default Version\n",
      "I0000 00:00:1764568176.317569      89 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 755ms/step - accuracy: 1.0000 - loss: 0.0035\n",
      "Test Accuracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       1.00      1.00      1.00         4\n",
      "           B       1.00      1.00      1.00         4\n",
      "           C       1.00      1.00      1.00         4\n",
      "           D       1.00      1.00      1.00         4\n",
      "           E       1.00      1.00      1.00         4\n",
      "           F       1.00      1.00      1.00         4\n",
      "           G       1.00      1.00      1.00         4\n",
      "           H       1.00      1.00      1.00         4\n",
      "           I       1.00      1.00      1.00         4\n",
      "           J       1.00      1.00      1.00         4\n",
      "           K       1.00      1.00      1.00         4\n",
      "           L       1.00      1.00      1.00         4\n",
      "           M       1.00      1.00      1.00         4\n",
      "           N       1.00      1.00      1.00         4\n",
      "     Nothing       1.00      1.00      1.00         4\n",
      "           O       1.00      1.00      1.00         4\n",
      "           P       1.00      1.00      1.00         4\n",
      "           Q       1.00      1.00      1.00         4\n",
      "           R       1.00      1.00      1.00         4\n",
      "           S       1.00      1.00      1.00         4\n",
      "       Space       1.00      1.00      1.00         4\n",
      "           T       1.00      1.00      1.00         4\n",
      "           U       1.00      1.00      1.00         4\n",
      "           V       1.00      1.00      1.00         4\n",
      "           W       1.00      1.00      1.00         4\n",
      "           X       1.00      1.00      1.00         4\n",
      "           Y       1.00      1.00      1.00         4\n",
      "           Z       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           1.00       112\n",
      "   macro avg       1.00      1.00      1.00       112\n",
      "weighted avg       1.00      1.00      1.00       112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the SAME test set that gave you 100% accuracy\n",
    "test_loss, test_acc = model.evaluate(test_ds)\n",
    "print(f\"Test Accuracy: {test_acc}\")\n",
    "\n",
    "# Get predictions for the entire test set\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for images, labels in test_ds:\n",
    "    preds = model.predict(images, verbose=0)\n",
    "    predictions.extend(preds.argmax(axis=1))\n",
    "    true_labels.extend(labels.numpy())\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(true_labels, predictions, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T05:50:15.164085Z",
     "iopub.status.busy": "2025-12-01T05:50:15.163789Z",
     "iopub.status.idle": "2025-12-01T05:50:19.307760Z",
     "shell.execute_reply": "2025-12-01T05:50:19.306602Z",
     "shell.execute_reply.started": "2025-12-01T05:50:15.164065Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmptqv7gko8'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_layer_15')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 28), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  136932680043984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136932680046288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136932680045712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136932680044752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136932680048400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136932680049168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136932680049360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136932680050896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136932680051088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  136932680052816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1764568217.049429      38 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1764568217.049472      38 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(\"asl_alphabert_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 177084,
     "sourceId": 399170,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1311225,
     "sourceId": 2184214,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3400451,
     "sourceId": 6094993,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7793547,
     "sourceId": 12361288,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8575690,
     "sourceId": 13506823,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
