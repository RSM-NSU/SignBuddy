{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c0b13c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Can not perform a '--user' install. User site-packages are not visible in this virtualenv.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ipykernel -U --user --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4186e9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./.venv/lib/python3.10/site-packages (25.3)\n",
      "Collecting setuptools\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting wheel\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Installing collected packages: wheel, setuptools\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [setuptools]2\u001b[0m [setuptools]\n",
      "\u001b[1A\u001b[2KSuccessfully installed setuptools-80.9.0 wheel-0.45.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy==1.23.5 in ./.venv/lib/python3.10/site-packages (1.23.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tflite-runtime==2.14.0 in ./.venv/lib/python3.10/site-packages (2.14.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in ./.venv/lib/python3.10/site-packages (from tflite-runtime==2.14.0) (1.23.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting opencv-python==4.7.0.72\n",
      "  Downloading opencv_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in ./.venv/lib/python3.10/site-packages (from opencv-python==4.7.0.72) (1.23.5)\n",
      "Downloading opencv_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 MB\u001b[0m \u001b[31m73.3 kB/s\u001b[0m  \u001b[33m0:06:11\u001b[0mm0:00:03\u001b[0mm00:13\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: opencv-python\n",
      "  Attempting uninstall: opencv-python\n",
      "    Found existing installation: opencv-python 4.12.0.88\n",
      "    Uninstalling opencv-python-4.12.0.88:\n",
      "      Successfully uninstalled opencv-python-4.12.0.88\n",
      "Successfully installed opencv-python-4.7.0.72\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip setuptools wheel\n",
    "%pip install numpy==1.23.5\n",
    "%pip install tflite-runtime==2.14.0\n",
    "%pip install opencv-python==4.7.0.72\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43448ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Open the default camera (0 usuammlly refers to the first camera)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "    \n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Camera', frame)\n",
    "    \n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e7192a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'Nothing', 'O', 'P', 'Q', 'R', 'S', 'Space', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae3c21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model input: [  1 224 224   3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1764845767.269510   17221 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1764845767.271477   28099 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.2.7), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "import tflite_runtime.interpreter as tflite\n",
    "\n",
    "class_names = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'Nothing', 'O', 'P', 'Q', 'R', 'S', 'Space', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "TARGET_SIZE = 224\n",
    "MODEL_PATH = \"/home/rafayahmadraza/SignBuddy/Models/asl_alphabert_model.tflite\"   # your trained model file\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Resize the cropped hand to 224×224 while keeping aspect\n",
    "# -------------------------------------------------------\n",
    "def resize_with_padding(img, target=TARGET_SIZE):\n",
    "    h, w = img.shape[:2]\n",
    "    aspect = w / h\n",
    "\n",
    "    if aspect > 1:  # wide\n",
    "        new_w = target\n",
    "        new_h = int(target / aspect)\n",
    "    else:  # tall\n",
    "        new_h = target\n",
    "        new_w = int(target * aspect)\n",
    "\n",
    "    resized = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Add black padding\n",
    "    top = (target - new_h) // 2\n",
    "    bottom = target - new_h - top\n",
    "    left = (target - new_w) // 2\n",
    "    right = target - new_w - left\n",
    "\n",
    "    padded = cv2.copyMakeBorder(\n",
    "        resized, top, bottom, left, right,\n",
    "        cv2.BORDER_CONSTANT, value=(0, 0, 0)\n",
    "    )\n",
    "    return padded\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Load TFLite Model\n",
    "# -------------------------------------------------------\n",
    "interpreter = tflite.Interpreter(model_path=MODEL_PATH)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"Model input:\", input_details[0][\"shape\"])\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Initialize MediaPipe Hands\n",
    "# -------------------------------------------------------\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.6,\n",
    "    min_tracking_confidence=0.6\n",
    ")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Main loop\n",
    "# -------------------------------------------------------\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(rgb)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "\n",
    "            # Extract bounding box\n",
    "            xs = [lm.x for lm in hand_landmarks.landmark]\n",
    "            ys = [lm.y for lm in hand_landmarks.landmark]\n",
    "\n",
    "            xmin = int(min(xs) * w)\n",
    "            xmax = int(max(xs) * w)\n",
    "            ymin = int(min(ys) * h)\n",
    "            ymax = int(max(ys) * h)\n",
    "\n",
    "            pad = 30\n",
    "            xmin = max(0, xmin - pad)\n",
    "            ymin = max(0, ymin - pad)\n",
    "            xmax = min(w, xmax + pad)\n",
    "            ymax = min(h, ymax + pad)\n",
    "\n",
    "            # Draw box\n",
    "            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "\n",
    "            # Crop\n",
    "            crop = frame[ymin:ymax, xmin:xmax]\n",
    "\n",
    "            if crop.size > 0:\n",
    "                crop_224 = resize_with_padding(crop)\n",
    "\n",
    "                # Show resized hand\n",
    "                cv2.imshow(\"Hand 224x224\", crop_224)\n",
    "\n",
    "                # Prepare input for model\n",
    "                inp = cv2.cvtColor(crop_224, cv2.COLOR_BGR2RGB)\n",
    "                inp = inp.astype(np.float32) / 255.0\n",
    "                inp = np.expand_dims(inp, axis=0)\n",
    "\n",
    "                # Run inference\n",
    "                interpreter.set_tensor(input_details[0][\"index\"], inp)\n",
    "                interpreter.invoke()\n",
    "                prediction = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "\n",
    "                # Get predicted class\n",
    "                cls = np.argmax(prediction)\n",
    "                conf = np.max(prediction)\n",
    "\n",
    "                # Display prediction\n",
    "                text = f\"Class: {class_names[np.argmax(prediction)]}  ({conf:.2f})\"\n",
    "                cv2.putText(frame, text, (xmin, ymin - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "                # Draw landmarks\n",
    "                mp_draw.draw_landmarks(\n",
    "                    frame, hand_landmarks, mp_hands.HAND_CONNECTIONS\n",
    "                )\n",
    "\n",
    "    cv2.imshow(\"Live\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f05cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
