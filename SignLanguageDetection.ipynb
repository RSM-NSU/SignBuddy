{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "452e0057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import scipy.stats\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display, Image\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83c9df39",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"D:/SignBuddy\"\n",
    "\n",
    "test_videos_path = os.path.join(base_dir,'test_videos')\n",
    "\n",
    "DATA_Path = os.path.join(base_dir,'MP_Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "850c92b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Holistic model from MediaPipe for full-body pose detection.\n",
    "# This model can detect landmarks for the face, hands, and body.\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "# Initialize the drawing utilities from MediaPipe to draw the landmarks on images or videos.\n",
    "# These utilities help visualize the pose detection results by drawing the keypoints and connections.\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# This function processes an image using a MediaPipe model to detect landmarks/poses and returns the processed image and the results.\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results\n",
    "\n",
    "# This function draws the detected pose, left hand, and right hand landmarks with their connections on the given image.\n",
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections\n",
    "\n",
    "# This function draws the pose, left hand, and right hand landmarks with custom styling (color, thickness, and circle radius)\n",
    "# on the given image, using the MediaPipe Holistic model's results.\n",
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    # Draw right hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db578fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change the name of the test video to ensure the code is running\n",
    "video_name = \"signsletters.MP4\"\n",
    "video_path = os.path.join(test_videos_path, video_name)\n",
    "\n",
    "# Try to open the video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if the video opened successfully\n",
    "if not cap.isOpened():\n",
    "    raise FileNotFoundError(f\"Error: Cannot open video file '{video_path}'. Please check if the file exists and the path is correct.\")\n",
    "\n",
    "# Set MediaPipe model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        frame = cv2.flip(frame, 0)\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "        # # Draw landmarks on the frame\n",
    "        # draw_styled_landmarks(image, results)\n",
    "\n",
    "        # # Convert frame to RGB (to display in Colab)\n",
    "        # image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # # Display the frame\n",
    "        # plt.figure(figsize=(10, 10))\n",
    "        # plt.imshow(image_rgb)\n",
    "        # plt.axis('off')\n",
    "        # display(plt.gcf())\n",
    "        # plt.close()\n",
    "\n",
    "        # Pause for a short moment to simulate frame rate\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7739bf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store pose landmark data\n",
    "pose = []\n",
    "\n",
    "# Loop through each landmark in the detected pose landmarks\n",
    "for res in results.pose_landmarks.landmark:\n",
    "    # Create a numpy array with the x, y, z coordinates and visibility of each landmark\n",
    "    test = np.array([res.x, res.y, res.z, res.visibility])\n",
    "\n",
    "    # Append the numpy array for the current landmark to the 'pose' list\n",
    "    pose.append(test)\n",
    "\n",
    "# Create a numpy array for pose landmarks, flattening the x, y, z coordinates and visibility for each landmark.\n",
    "# If pose landmarks are not detected, return a zero array of length 132 (assuming 33 landmarks, each with 4 values: x, y, z, visibility).\n",
    "pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
    "\n",
    "# Create a numpy array for face landmarks, flattening the x, y, z coordinates for each landmark.\n",
    "# If face landmarks are not detected, return a zero array of length 1404 (assuming 468 landmarks, each with 3 values: x, y, z).\n",
    "face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n",
    "\n",
    "# Create a numpy array for left hand landmarks, flattening the x, y, z coordinates for each landmark.\n",
    "# If left hand landmarks are not detected, return a zero array of length 63 (21 landmarks, each with 3 values: x, y, z).\n",
    "lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "\n",
    "# Create a numpy array for right hand landmarks, flattening the x, y, z coordinates for each landmark.\n",
    "# If right hand landmarks are not detected, return a zero array of length 63 (21 landmarks, each with 3 values: x, y, z).\n",
    "rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "\n",
    "# This function extracts keypoints (pose, face, left hand, right hand) from the results of MediaPipe landmarks,\n",
    "# flattens the data into 1D arrays, and concatenates them into a single array for further processing.\n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38a6ccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the set of actions (sign language gestures) to detect\n",
    "actions = np.array(['a', 'b', 'c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z'])\n",
    "\n",
    "# Path for exported data (numpy arrays will be saved here)\n",
    "DATA_PATH = os.path.join(base_dir, 'MP_Data')\n",
    "VIDEO_PATH = os.path.join(base_dir, 'videos')\n",
    "\n",
    "# Create the data directory if it doesn't exist\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    os.makedirs(DATA_PATH)\n",
    "\n",
    "# Number of sequences (videos) to record per action\n",
    "no_sequences = 120\n",
    "\n",
    "# Number of frames per video sequence\n",
    "sequence_length = 30\n",
    "\n",
    "# Starting index for naming folders that store each sequence\n",
    "start_folder = 30\n",
    "\n",
    "# Create folder structure for each action and each sequence\n",
    "for action in actions:\n",
    "    # Path for this action's data\n",
    "    action_path = os.path.join(DATA_PATH, action)\n",
    "    video_path = os.path.join(VIDEO_PATH, action)\n",
    "\n",
    "    # Create action folder if it doesn't exist\n",
    "    if not os.path.exists(action_path):\n",
    "        os.makedirs(action_path)\n",
    "\n",
    "    # Create video folder if it doesn't exist\n",
    "    if not os.path.exists(video_path):\n",
    "        os.makedirs(video_path)\n",
    "\n",
    "    # Find existing numbered folders (sequences) for this action\n",
    "    existing_dirs = [d for d in os.listdir(action_path) if d.isdigit()]\n",
    "\n",
    "    # Determine the current max sequence number to avoid overwriting\n",
    "    if existing_dirs:\n",
    "        dirmax = np.max(np.array(existing_dirs).astype(int))\n",
    "    else:\n",
    "        dirmax = 0\n",
    "\n",
    "    # Create subfolders for each new sequence\n",
    "    for sequence in range(0, no_sequences):\n",
    "        seq_path = os.path.join(action_path, str(dirmax + sequence))\n",
    "        if not os.path.exists(seq_path):\n",
    "            os.makedirs(seq_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd76a48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c62b24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b47e0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0df5e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
